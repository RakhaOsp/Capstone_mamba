{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62de866c-109e-468c-a9b1-5da1f5c059c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smartview/miniconda3/envs/mamba_working/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 8, 22, 1000]             512\n",
      "       BatchNorm2d-2          [-1, 8, 22, 1000]              16\n",
      "            Conv2d-3          [-1, 16, 1, 1000]             352\n",
      "       BatchNorm2d-4          [-1, 16, 1, 1000]              32\n",
      "               ELU-5          [-1, 16, 1, 1000]               0\n",
      "         AvgPool2d-6           [-1, 16, 1, 125]               0\n",
      "           Dropout-7           [-1, 16, 1, 125]               0\n",
      "            Conv2d-8           [-1, 16, 1, 125]           4,096\n",
      "       BatchNorm2d-9           [-1, 16, 1, 125]              32\n",
      "              ELU-10           [-1, 16, 1, 125]               0\n",
      "        AvgPool2d-11            [-1, 16, 1, 15]               0\n",
      "          Dropout-12            [-1, 16, 1, 15]               0\n",
      "        Rearrange-13               [-1, 15, 16]               0\n",
      "PatchEmbeddingCNN-14               [-1, 15, 16]               0\n",
      "        LayerNorm-15               [-1, 15, 16]              32\n",
      "           Linear-16               [-1, 15, 64]           1,024\n",
      "           Conv1d-17               [-1, 32, 18]             160\n",
      "           Linear-18               [-1, 15, 65]           2,080\n",
      "           Linear-19               [-1, 15, 32]              64\n",
      "           Linear-20               [-1, 15, 16]             512\n",
      "       MambaBlock-21               [-1, 15, 16]               0\n",
      "          Dropout-22               [-1, 15, 16]               0\n",
      "        LayerNorm-23               [-1, 15, 16]              32\n",
      "           Linear-24               [-1, 15, 48]             768\n",
      "           Linear-25               [-1, 15, 16]             272\n",
      "          Dropout-26               [-1, 15, 16]               0\n",
      "  LinearAttention-27               [-1, 15, 16]               0\n",
      "          Dropout-28               [-1, 15, 16]               0\n",
      "        LayerNorm-29               [-1, 15, 16]              32\n",
      "           Linear-30               [-1, 15, 64]           1,088\n",
      "             GELU-31               [-1, 15, 64]               0\n",
      "          Dropout-32               [-1, 15, 64]               0\n",
      "           Linear-33               [-1, 15, 16]           1,040\n",
      "          Dropout-34               [-1, 15, 16]               0\n",
      "        LayerNorm-35               [-1, 15, 16]              32\n",
      "           Linear-36               [-1, 15, 64]           1,024\n",
      "           Conv1d-37               [-1, 32, 18]             160\n",
      "           Linear-38               [-1, 15, 65]           2,080\n",
      "           Linear-39               [-1, 15, 32]              64\n",
      "           Linear-40               [-1, 15, 16]             512\n",
      "       MambaBlock-41               [-1, 15, 16]               0\n",
      "          Dropout-42               [-1, 15, 16]               0\n",
      "        LayerNorm-43               [-1, 15, 16]              32\n",
      "           Linear-44               [-1, 15, 48]             768\n",
      "           Linear-45               [-1, 15, 16]             272\n",
      "          Dropout-46               [-1, 15, 16]               0\n",
      "  LinearAttention-47               [-1, 15, 16]               0\n",
      "          Dropout-48               [-1, 15, 16]               0\n",
      "        LayerNorm-49               [-1, 15, 16]              32\n",
      "           Linear-50               [-1, 15, 64]           1,088\n",
      "             GELU-51               [-1, 15, 64]               0\n",
      "          Dropout-52               [-1, 15, 64]               0\n",
      "           Linear-53               [-1, 15, 16]           1,040\n",
      "          Dropout-54               [-1, 15, 16]               0\n",
      "        LayerNorm-55               [-1, 15, 16]              32\n",
      "           Linear-56               [-1, 15, 64]           1,024\n",
      "           Conv1d-57               [-1, 32, 18]             160\n",
      "           Linear-58               [-1, 15, 65]           2,080\n",
      "           Linear-59               [-1, 15, 32]              64\n",
      "           Linear-60               [-1, 15, 16]             512\n",
      "       MambaBlock-61               [-1, 15, 16]               0\n",
      "          Dropout-62               [-1, 15, 16]               0\n",
      "        LayerNorm-63               [-1, 15, 16]              32\n",
      "           Linear-64               [-1, 15, 48]             768\n",
      "           Linear-65               [-1, 15, 16]             272\n",
      "          Dropout-66               [-1, 15, 16]               0\n",
      "  LinearAttention-67               [-1, 15, 16]               0\n",
      "          Dropout-68               [-1, 15, 16]               0\n",
      "        LayerNorm-69               [-1, 15, 16]              32\n",
      "           Linear-70               [-1, 15, 64]           1,088\n",
      "             GELU-71               [-1, 15, 64]               0\n",
      "          Dropout-72               [-1, 15, 64]               0\n",
      "           Linear-73               [-1, 15, 16]           1,040\n",
      "          Dropout-74               [-1, 15, 16]               0\n",
      "        LayerNorm-75               [-1, 15, 16]              32\n",
      "MambaTransformerblock-76               [-1, 15, 16]               0\n",
      "          Flatten-77                  [-1, 240]               0\n",
      "          Dropout-78                  [-1, 240]               0\n",
      "           Linear-79                    [-1, 4]             964\n",
      "================================================================\n",
      "Total params: 27,348\n",
      "Trainable params: 27,348\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.08\n",
      "Forward/backward pass size (MB): 3.36\n",
      "Params size (MB): 0.10\n",
      "Estimated Total Size (MB): 3.55\n",
      "----------------------------------------------------------------\n",
      "Tue Apr 29 11:04:36 2025\n",
      "seed is 324\n",
      "Subject 1\n",
      "Epoch 1/1000 | Train Loss: 1.5384 Acc: 0.2659 | Val Loss: 1.4430 Acc: 0.3721 | Mem: 793.23MB | Speed: 89.67 samples/s\n",
      "Epoch 2/1000 | Train Loss: 1.5067 Acc: 0.2976 | Val Loss: 1.4322 Acc: 0.3488 | Mem: 792.88MB | Speed: 70.82 samples/s\n",
      "Epoch 3/1000 | Train Loss: 1.3882 Acc: 0.3576 | Val Loss: 1.3880 Acc: 0.3605 | Mem: 792.88MB | Speed: 104.23 samples/s\n",
      "Epoch 4/1000 | Train Loss: 1.4043 Acc: 0.3400 | Val Loss: 1.3265 Acc: 0.3605 | Mem: 792.88MB | Speed: 90.46 samples/s\n",
      "Epoch 5/1000 | Train Loss: 1.3585 Acc: 0.3635 | Val Loss: 1.2809 Acc: 0.3837 | Mem: 792.88MB | Speed: 84.56 samples/s\n",
      "Epoch 6/1000 | Train Loss: 1.3374 Acc: 0.3718 | Val Loss: 1.2554 Acc: 0.3721 | Mem: 792.88MB | Speed: 176.04 samples/s\n",
      "Epoch 7/1000 | Train Loss: 1.2902 Acc: 0.3776 | Val Loss: 1.2383 Acc: 0.3953 | Mem: 792.88MB | Speed: 84.66 samples/s\n",
      "Epoch 8/1000 | Train Loss: 1.2925 Acc: 0.3918 | Val Loss: 1.2326 Acc: 0.4186 | Mem: 792.88MB | Speed: 122.56 samples/s\n",
      "Epoch 9/1000 | Train Loss: 1.2516 Acc: 0.4153 | Val Loss: 1.2246 Acc: 0.4070 | Mem: 792.88MB | Speed: 78.85 samples/s\n",
      "Epoch 10/1000 | Train Loss: 1.2478 Acc: 0.4424 | Val Loss: 1.2083 Acc: 0.4419 | Mem: 792.88MB | Speed: 132.97 samples/s\n",
      "Epoch 11/1000 | Train Loss: 1.2108 Acc: 0.4400 | Val Loss: 1.1930 Acc: 0.4419 | Mem: 792.88MB | Speed: 120.22 samples/s\n",
      "Epoch 12/1000 | Train Loss: 1.1814 Acc: 0.4906 | Val Loss: 1.1723 Acc: 0.4767 | Mem: 792.88MB | Speed: 66.05 samples/s\n",
      "Epoch 13/1000 | Train Loss: 1.1778 Acc: 0.4682 | Val Loss: 1.1473 Acc: 0.4884 | Mem: 792.88MB | Speed: 67.95 samples/s\n",
      "Epoch 14/1000 | Train Loss: 1.1690 Acc: 0.4753 | Val Loss: 1.1237 Acc: 0.5116 | Mem: 792.88MB | Speed: 66.26 samples/s\n",
      "Epoch 15/1000 | Train Loss: 1.1140 Acc: 0.5059 | Val Loss: 1.0993 Acc: 0.5116 | Mem: 792.88MB | Speed: 89.46 samples/s\n",
      "Epoch 16/1000 | Train Loss: 1.1368 Acc: 0.5059 | Val Loss: 1.0716 Acc: 0.5349 | Mem: 792.88MB | Speed: 91.44 samples/s\n",
      "Epoch 17/1000 | Train Loss: 1.1138 Acc: 0.5259 | Val Loss: 1.0453 Acc: 0.5465 | Mem: 792.88MB | Speed: 84.27 samples/s\n",
      "Epoch 18/1000 | Train Loss: 1.1033 Acc: 0.5224 | Val Loss: 1.0202 Acc: 0.5349 | Mem: 792.88MB | Speed: 158.75 samples/s\n",
      "Epoch 19/1000 | Train Loss: 1.0433 Acc: 0.5659 | Val Loss: 0.9966 Acc: 0.5349 | Mem: 792.88MB | Speed: 89.32 samples/s\n",
      "Epoch 20/1000 | Train Loss: 1.0377 Acc: 0.5412 | Val Loss: 0.9701 Acc: 0.5814 | Mem: 792.88MB | Speed: 71.12 samples/s\n",
      "Epoch 21/1000 | Train Loss: 1.0071 Acc: 0.5671 | Val Loss: 0.9435 Acc: 0.5581 | Mem: 792.88MB | Speed: 82.04 samples/s\n",
      "Epoch 22/1000 | Train Loss: 1.0410 Acc: 0.5412 | Val Loss: 0.9097 Acc: 0.6047 | Mem: 792.88MB | Speed: 243.78 samples/s\n",
      "Epoch 23/1000 | Train Loss: 0.9651 Acc: 0.5918 | Val Loss: 0.8919 Acc: 0.6279 | Mem: 792.88MB | Speed: 186.61 samples/s\n",
      "Epoch 24/1000 | Train Loss: 0.9864 Acc: 0.5906 | Val Loss: 0.8687 Acc: 0.6512 | Mem: 792.88MB | Speed: 179.80 samples/s\n",
      "Epoch 25/1000 | Train Loss: 0.9547 Acc: 0.5765 | Val Loss: 0.8362 Acc: 0.6860 | Mem: 792.88MB | Speed: 226.06 samples/s\n",
      "Epoch 26/1000 | Train Loss: 0.8792 Acc: 0.6188 | Val Loss: 0.8105 Acc: 0.7209 | Mem: 792.88MB | Speed: 65.71 samples/s\n",
      "Epoch 27/1000 | Train Loss: 0.8924 Acc: 0.6176 | Val Loss: 0.7850 Acc: 0.7326 | Mem: 792.88MB | Speed: 121.08 samples/s\n",
      "Epoch 28/1000 | Train Loss: 0.8379 Acc: 0.6459 | Val Loss: 0.7686 Acc: 0.7326 | Mem: 792.88MB | Speed: 131.37 samples/s\n",
      "Epoch 29/1000 | Train Loss: 0.8027 Acc: 0.6871 | Val Loss: 0.7702 Acc: 0.7326 | Mem: 792.88MB | Speed: 517.72 samples/s\n",
      "Epoch 30/1000 | Train Loss: 0.8004 Acc: 0.6400 | Val Loss: 0.7841 Acc: 0.6860 | Mem: 792.88MB | Speed: 519.44 samples/s\n",
      "Epoch 31/1000 | Train Loss: 0.7611 Acc: 0.6929 | Val Loss: 0.7948 Acc: 0.7209 | Mem: 792.88MB | Speed: 593.36 samples/s\n",
      "Epoch 32/1000 | Train Loss: 0.7592 Acc: 0.6706 | Val Loss: 0.8020 Acc: 0.7093 | Mem: 792.88MB | Speed: 512.77 samples/s\n",
      "Epoch 33/1000 | Train Loss: 0.7355 Acc: 0.6859 | Val Loss: 0.7884 Acc: 0.6860 | Mem: 792.88MB | Speed: 530.43 samples/s\n",
      "Epoch 34/1000 | Train Loss: 0.7261 Acc: 0.6965 | Val Loss: 0.7844 Acc: 0.6977 | Mem: 792.88MB | Speed: 485.37 samples/s\n",
      "Epoch 35/1000 | Train Loss: 0.7228 Acc: 0.6753 | Val Loss: 0.7826 Acc: 0.6628 | Mem: 792.88MB | Speed: 514.42 samples/s\n",
      "Epoch 36/1000 | Train Loss: 0.7081 Acc: 0.6835 | Val Loss: 0.7815 Acc: 0.7093 | Mem: 792.88MB | Speed: 537.82 samples/s\n",
      "Epoch 37/1000 | Train Loss: 0.6735 Acc: 0.6894 | Val Loss: 0.7676 Acc: 0.7209 | Mem: 792.88MB | Speed: 484.05 samples/s\n",
      "Epoch 38/1000 | Train Loss: 0.6797 Acc: 0.7035 | Val Loss: 0.7311 Acc: 0.7093 | Mem: 792.88MB | Speed: 533.65 samples/s\n",
      "Epoch 39/1000 | Train Loss: 0.7035 Acc: 0.6788 | Val Loss: 0.6948 Acc: 0.7442 | Mem: 792.88MB | Speed: 473.69 samples/s\n",
      "Epoch 40/1000 | Train Loss: 0.6051 Acc: 0.7353 | Val Loss: 0.6765 Acc: 0.7442 | Mem: 792.88MB | Speed: 467.95 samples/s\n",
      "Epoch 41/1000 | Train Loss: 0.6622 Acc: 0.7129 | Val Loss: 0.6812 Acc: 0.7442 | Mem: 792.88MB | Speed: 523.24 samples/s\n",
      "Epoch 42/1000 | Train Loss: 0.6688 Acc: 0.7082 | Val Loss: 0.7099 Acc: 0.7326 | Mem: 792.88MB | Speed: 501.25 samples/s\n",
      "Epoch 43/1000 | Train Loss: 0.6545 Acc: 0.7047 | Val Loss: 0.7030 Acc: 0.7209 | Mem: 792.88MB | Speed: 499.73 samples/s\n",
      "Epoch 44/1000 | Train Loss: 0.6338 Acc: 0.7306 | Val Loss: 0.6760 Acc: 0.6977 | Mem: 792.88MB | Speed: 522.08 samples/s\n",
      "Epoch 45/1000 | Train Loss: 0.6369 Acc: 0.7176 | Val Loss: 0.6450 Acc: 0.7209 | Mem: 792.88MB | Speed: 504.80 samples/s\n",
      "Epoch 46/1000 | Train Loss: 0.6040 Acc: 0.7306 | Val Loss: 0.6237 Acc: 0.7093 | Mem: 792.88MB | Speed: 506.21 samples/s\n",
      "Epoch 47/1000 | Train Loss: 0.6156 Acc: 0.7388 | Val Loss: 0.6251 Acc: 0.7326 | Mem: 792.88MB | Speed: 498.53 samples/s\n",
      "Epoch 48/1000 | Train Loss: 0.5704 Acc: 0.7518 | Val Loss: 0.6328 Acc: 0.7093 | Mem: 792.88MB | Speed: 486.06 samples/s\n",
      "Epoch 49/1000 | Train Loss: 0.5838 Acc: 0.7482 | Val Loss: 0.6496 Acc: 0.7093 | Mem: 792.88MB | Speed: 490.77 samples/s\n",
      "Epoch 50/1000 | Train Loss: 0.5851 Acc: 0.7388 | Val Loss: 0.6514 Acc: 0.7209 | Mem: 792.88MB | Speed: 553.09 samples/s\n",
      "Epoch 51/1000 | Train Loss: 0.5759 Acc: 0.7576 | Val Loss: 0.6259 Acc: 0.7326 | Mem: 792.88MB | Speed: 476.23 samples/s\n",
      "Epoch 52/1000 | Train Loss: 0.5652 Acc: 0.7459 | Val Loss: 0.6108 Acc: 0.7442 | Mem: 792.88MB | Speed: 539.12 samples/s\n",
      "Epoch 53/1000 | Train Loss: 0.5477 Acc: 0.7647 | Val Loss: 0.5997 Acc: 0.7326 | Mem: 792.88MB | Speed: 596.51 samples/s\n",
      "Epoch 54/1000 | Train Loss: 0.5534 Acc: 0.7635 | Val Loss: 0.6008 Acc: 0.7326 | Mem: 792.88MB | Speed: 611.11 samples/s\n",
      "Epoch 55/1000 | Train Loss: 0.5373 Acc: 0.7788 | Val Loss: 0.5804 Acc: 0.7442 | Mem: 792.88MB | Speed: 548.85 samples/s\n",
      "Epoch 56/1000 | Train Loss: 0.5320 Acc: 0.7729 | Val Loss: 0.5697 Acc: 0.7442 | Mem: 792.88MB | Speed: 485.10 samples/s\n",
      "Epoch 57/1000 | Train Loss: 0.5406 Acc: 0.7729 | Val Loss: 0.5627 Acc: 0.7442 | Mem: 792.88MB | Speed: 469.94 samples/s\n",
      "Epoch 58/1000 | Train Loss: 0.5069 Acc: 0.7847 | Val Loss: 0.5925 Acc: 0.7442 | Mem: 792.88MB | Speed: 601.57 samples/s\n",
      "Epoch 59/1000 | Train Loss: 0.5268 Acc: 0.7741 | Val Loss: 0.6076 Acc: 0.7442 | Mem: 792.88MB | Speed: 626.44 samples/s\n",
      "Epoch 60/1000 | Train Loss: 0.5327 Acc: 0.7765 | Val Loss: 0.5877 Acc: 0.7326 | Mem: 792.88MB | Speed: 539.25 samples/s\n",
      "Epoch 61/1000 | Train Loss: 0.4863 Acc: 0.8012 | Val Loss: 0.5581 Acc: 0.7326 | Mem: 792.88MB | Speed: 514.36 samples/s\n",
      "Epoch 62/1000 | Train Loss: 0.5169 Acc: 0.7882 | Val Loss: 0.5408 Acc: 0.7326 | Mem: 792.88MB | Speed: 494.90 samples/s\n",
      "Epoch 63/1000 | Train Loss: 0.5272 Acc: 0.7800 | Val Loss: 0.5485 Acc: 0.7558 | Mem: 792.88MB | Speed: 542.35 samples/s\n",
      "Epoch 64/1000 | Train Loss: 0.5026 Acc: 0.7882 | Val Loss: 0.5822 Acc: 0.7442 | Mem: 792.88MB | Speed: 471.75 samples/s\n",
      "Epoch 65/1000 | Train Loss: 0.5137 Acc: 0.7647 | Val Loss: 0.6247 Acc: 0.7326 | Mem: 792.88MB | Speed: 632.58 samples/s\n",
      "Epoch 66/1000 | Train Loss: 0.5006 Acc: 0.7812 | Val Loss: 0.6155 Acc: 0.7326 | Mem: 792.88MB | Speed: 632.56 samples/s\n",
      "Epoch 67/1000 | Train Loss: 0.4726 Acc: 0.8059 | Val Loss: 0.5881 Acc: 0.7326 | Mem: 792.88MB | Speed: 468.31 samples/s\n",
      "Epoch 68/1000 | Train Loss: 0.5320 Acc: 0.7682 | Val Loss: 0.5545 Acc: 0.7442 | Mem: 792.88MB | Speed: 480.92 samples/s\n",
      "Epoch 69/1000 | Train Loss: 0.4811 Acc: 0.7871 | Val Loss: 0.5538 Acc: 0.7442 | Mem: 792.88MB | Speed: 497.80 samples/s\n",
      "Epoch 70/1000 | Train Loss: 0.4748 Acc: 0.8047 | Val Loss: 0.5749 Acc: 0.7326 | Mem: 792.88MB | Speed: 514.50 samples/s\n",
      "Epoch 71/1000 | Train Loss: 0.5056 Acc: 0.7918 | Val Loss: 0.5963 Acc: 0.7442 | Mem: 792.88MB | Speed: 420.54 samples/s\n",
      "Epoch 72/1000 | Train Loss: 0.4683 Acc: 0.8106 | Val Loss: 0.5950 Acc: 0.7442 | Mem: 792.88MB | Speed: 507.65 samples/s\n",
      "Epoch 73/1000 | Train Loss: 0.4809 Acc: 0.7965 | Val Loss: 0.5935 Acc: 0.7442 | Mem: 792.88MB | Speed: 510.99 samples/s\n",
      "Epoch 74/1000 | Train Loss: 0.4745 Acc: 0.8106 | Val Loss: 0.5518 Acc: 0.7442 | Mem: 792.88MB | Speed: 493.07 samples/s\n",
      "Epoch 75/1000 | Train Loss: 0.4576 Acc: 0.8035 | Val Loss: 0.5440 Acc: 0.7442 | Mem: 792.88MB | Speed: 420.71 samples/s\n",
      "Epoch 76/1000 | Train Loss: 0.4571 Acc: 0.8200 | Val Loss: 0.5503 Acc: 0.7558 | Mem: 792.88MB | Speed: 522.76 samples/s\n",
      "Epoch 77/1000 | Train Loss: 0.4726 Acc: 0.7871 | Val Loss: 0.5442 Acc: 0.7326 | Mem: 792.88MB | Speed: 512.95 samples/s\n",
      "Epoch 78/1000 | Train Loss: 0.4523 Acc: 0.7918 | Val Loss: 0.5303 Acc: 0.7442 | Mem: 792.88MB | Speed: 538.84 samples/s\n",
      "Epoch 79/1000 | Train Loss: 0.4510 Acc: 0.8071 | Val Loss: 0.4937 Acc: 0.7674 | Mem: 792.88MB | Speed: 560.93 samples/s\n",
      "Epoch 80/1000 | Train Loss: 0.4612 Acc: 0.8106 | Val Loss: 0.4650 Acc: 0.7674 | Mem: 792.88MB | Speed: 502.83 samples/s\n",
      "Epoch 81/1000 | Train Loss: 0.4550 Acc: 0.8106 | Val Loss: 0.4966 Acc: 0.7674 | Mem: 792.88MB | Speed: 480.47 samples/s\n",
      "Epoch 82/1000 | Train Loss: 0.4602 Acc: 0.8141 | Val Loss: 0.5529 Acc: 0.7442 | Mem: 792.88MB | Speed: 527.48 samples/s\n",
      "Epoch 83/1000 | Train Loss: 0.4624 Acc: 0.8106 | Val Loss: 0.5520 Acc: 0.7442 | Mem: 792.88MB | Speed: 530.32 samples/s\n",
      "Epoch 84/1000 | Train Loss: 0.4403 Acc: 0.8000 | Val Loss: 0.4842 Acc: 0.7674 | Mem: 792.88MB | Speed: 602.83 samples/s\n",
      "Epoch 85/1000 | Train Loss: 0.4427 Acc: 0.8071 | Val Loss: 0.4516 Acc: 0.7791 | Mem: 792.88MB | Speed: 515.59 samples/s\n",
      "Epoch 86/1000 | Train Loss: 0.4569 Acc: 0.8035 | Val Loss: 0.4598 Acc: 0.7442 | Mem: 792.88MB | Speed: 494.04 samples/s\n",
      "Epoch 87/1000 | Train Loss: 0.4441 Acc: 0.8024 | Val Loss: 0.4797 Acc: 0.7558 | Mem: 792.88MB | Speed: 611.64 samples/s\n",
      "Epoch 88/1000 | Train Loss: 0.3963 Acc: 0.8576 | Val Loss: 0.4646 Acc: 0.7674 | Mem: 792.88MB | Speed: 537.96 samples/s\n",
      "Epoch 89/1000 | Train Loss: 0.4112 Acc: 0.8271 | Val Loss: 0.4561 Acc: 0.7442 | Mem: 792.88MB | Speed: 525.46 samples/s\n",
      "Epoch 90/1000 | Train Loss: 0.4344 Acc: 0.8153 | Val Loss: 0.4691 Acc: 0.7326 | Mem: 792.88MB | Speed: 488.79 samples/s\n",
      "Epoch 91/1000 | Train Loss: 0.4251 Acc: 0.8165 | Val Loss: 0.4592 Acc: 0.7558 | Mem: 792.88MB | Speed: 548.08 samples/s\n",
      "Epoch 92/1000 | Train Loss: 0.4512 Acc: 0.8165 | Val Loss: 0.4306 Acc: 0.7674 | Mem: 792.88MB | Speed: 525.92 samples/s\n",
      "Epoch 93/1000 | Train Loss: 0.4185 Acc: 0.8412 | Val Loss: 0.4354 Acc: 0.7674 | Mem: 792.88MB | Speed: 397.91 samples/s\n",
      "Epoch 94/1000 | Train Loss: 0.3780 Acc: 0.8388 | Val Loss: 0.4423 Acc: 0.7558 | Mem: 792.88MB | Speed: 544.58 samples/s\n",
      "Epoch 95/1000 | Train Loss: 0.4017 Acc: 0.8353 | Val Loss: 0.4349 Acc: 0.7558 | Mem: 792.88MB | Speed: 530.16 samples/s\n",
      "Epoch 96/1000 | Train Loss: 0.4141 Acc: 0.8329 | Val Loss: 0.4582 Acc: 0.7558 | Mem: 792.88MB | Speed: 505.97 samples/s\n",
      "Epoch 97/1000 | Train Loss: 0.4374 Acc: 0.8200 | Val Loss: 0.4605 Acc: 0.7558 | Mem: 792.88MB | Speed: 522.01 samples/s\n",
      "Epoch 98/1000 | Train Loss: 0.4292 Acc: 0.8059 | Val Loss: 0.4335 Acc: 0.7674 | Mem: 792.88MB | Speed: 555.83 samples/s\n",
      "Epoch 99/1000 | Train Loss: 0.4246 Acc: 0.8176 | Val Loss: 0.4201 Acc: 0.7674 | Mem: 792.88MB | Speed: 503.61 samples/s\n",
      "Epoch 100/1000 | Train Loss: 0.3904 Acc: 0.8424 | Val Loss: 0.4313 Acc: 0.8023 | Mem: 792.88MB | Speed: 606.76 samples/s\n",
      "Epoch 101/1000 | Train Loss: 0.3911 Acc: 0.8447 | Val Loss: 0.4310 Acc: 0.8023 | Mem: 792.88MB | Speed: 413.08 samples/s\n",
      "Epoch 102/1000 | Train Loss: 0.4253 Acc: 0.8200 | Val Loss: 0.4010 Acc: 0.7907 | Mem: 792.88MB | Speed: 486.04 samples/s\n",
      "Epoch 103/1000 | Train Loss: 0.3721 Acc: 0.8565 | Val Loss: 0.4216 Acc: 0.7674 | Mem: 792.88MB | Speed: 504.15 samples/s\n",
      "Epoch 104/1000 | Train Loss: 0.3872 Acc: 0.8471 | Val Loss: 0.4423 Acc: 0.7907 | Mem: 792.88MB | Speed: 590.86 samples/s\n",
      "Epoch 105/1000 | Train Loss: 0.3860 Acc: 0.8506 | Val Loss: 0.4105 Acc: 0.7791 | Mem: 792.88MB | Speed: 602.06 samples/s\n",
      "Epoch 106/1000 | Train Loss: 0.3751 Acc: 0.8600 | Val Loss: 0.3723 Acc: 0.7791 | Mem: 792.88MB | Speed: 596.38 samples/s\n",
      "Epoch 107/1000 | Train Loss: 0.3983 Acc: 0.8412 | Val Loss: 0.3838 Acc: 0.7674 | Mem: 792.88MB | Speed: 527.98 samples/s\n",
      "Epoch 108/1000 | Train Loss: 0.3605 Acc: 0.8506 | Val Loss: 0.4102 Acc: 0.7791 | Mem: 792.88MB | Speed: 620.79 samples/s\n",
      "Epoch 109/1000 | Train Loss: 0.3677 Acc: 0.8518 | Val Loss: 0.4160 Acc: 0.7907 | Mem: 792.88MB | Speed: 541.34 samples/s\n",
      "Epoch 110/1000 | Train Loss: 0.3723 Acc: 0.8553 | Val Loss: 0.3923 Acc: 0.7791 | Mem: 792.88MB | Speed: 715.87 samples/s\n",
      "Epoch 111/1000 | Train Loss: 0.3487 Acc: 0.8671 | Val Loss: 0.3601 Acc: 0.8023 | Mem: 792.88MB | Speed: 514.19 samples/s\n",
      "Epoch 112/1000 | Train Loss: 0.3823 Acc: 0.8506 | Val Loss: 0.3335 Acc: 0.7907 | Mem: 792.88MB | Speed: 503.82 samples/s\n",
      "Epoch 113/1000 | Train Loss: 0.3385 Acc: 0.8765 | Val Loss: 0.3559 Acc: 0.7907 | Mem: 792.88MB | Speed: 523.28 samples/s\n",
      "Epoch 114/1000 | Train Loss: 0.3692 Acc: 0.8482 | Val Loss: 0.4211 Acc: 0.8023 | Mem: 792.88MB | Speed: 506.21 samples/s\n",
      "Epoch 115/1000 | Train Loss: 0.3759 Acc: 0.8424 | Val Loss: 0.4202 Acc: 0.7674 | Mem: 792.88MB | Speed: 491.24 samples/s\n",
      "Epoch 116/1000 | Train Loss: 0.3820 Acc: 0.8553 | Val Loss: 0.3637 Acc: 0.7791 | Mem: 792.88MB | Speed: 508.82 samples/s\n",
      "Epoch 117/1000 | Train Loss: 0.3749 Acc: 0.8482 | Val Loss: 0.3518 Acc: 0.7674 | Mem: 792.88MB | Speed: 540.88 samples/s\n",
      "Epoch 118/1000 | Train Loss: 0.3619 Acc: 0.8459 | Val Loss: 0.3841 Acc: 0.7674 | Mem: 792.88MB | Speed: 553.12 samples/s\n",
      "Epoch 119/1000 | Train Loss: 0.3612 Acc: 0.8506 | Val Loss: 0.4180 Acc: 0.7674 | Mem: 792.88MB | Speed: 569.09 samples/s\n",
      "Epoch 120/1000 | Train Loss: 0.3687 Acc: 0.8565 | Val Loss: 0.4017 Acc: 0.7791 | Mem: 792.88MB | Speed: 564.51 samples/s\n",
      "Epoch 121/1000 | Train Loss: 0.3311 Acc: 0.8776 | Val Loss: 0.3517 Acc: 0.7907 | Mem: 792.88MB | Speed: 545.73 samples/s\n",
      "Epoch 122/1000 | Train Loss: 0.3718 Acc: 0.8518 | Val Loss: 0.3386 Acc: 0.8140 | Mem: 792.88MB | Speed: 507.32 samples/s\n",
      "Epoch 123/1000 | Train Loss: 0.3577 Acc: 0.8553 | Val Loss: 0.3633 Acc: 0.7907 | Mem: 792.88MB | Speed: 481.76 samples/s\n",
      "Epoch 124/1000 | Train Loss: 0.3270 Acc: 0.8671 | Val Loss: 0.4056 Acc: 0.7907 | Mem: 792.88MB | Speed: 545.70 samples/s\n",
      "Epoch 125/1000 | Train Loss: 0.3611 Acc: 0.8541 | Val Loss: 0.4118 Acc: 0.8140 | Mem: 792.88MB | Speed: 626.59 samples/s\n",
      "Epoch 126/1000 | Train Loss: 0.3656 Acc: 0.8624 | Val Loss: 0.3951 Acc: 0.7791 | Mem: 792.88MB | Speed: 520.75 samples/s\n",
      "Epoch 127/1000 | Train Loss: 0.3465 Acc: 0.8565 | Val Loss: 0.4222 Acc: 0.7791 | Mem: 792.88MB | Speed: 572.29 samples/s\n",
      "Epoch 128/1000 | Train Loss: 0.3307 Acc: 0.8659 | Val Loss: 0.4132 Acc: 0.7558 | Mem: 792.88MB | Speed: 512.34 samples/s\n",
      "Epoch 129/1000 | Train Loss: 0.3484 Acc: 0.8529 | Val Loss: 0.3728 Acc: 0.7674 | Mem: 792.88MB | Speed: 497.76 samples/s\n",
      "Epoch 130/1000 | Train Loss: 0.3391 Acc: 0.8635 | Val Loss: 0.3496 Acc: 0.7674 | Mem: 792.88MB | Speed: 494.50 samples/s\n",
      "Epoch 131/1000 | Train Loss: 0.3308 Acc: 0.8682 | Val Loss: 0.3437 Acc: 0.7791 | Mem: 792.88MB | Speed: 501.53 samples/s\n",
      "Epoch 132/1000 | Train Loss: 0.3527 Acc: 0.8576 | Val Loss: 0.3729 Acc: 0.7791 | Mem: 792.88MB | Speed: 512.38 samples/s\n",
      "Epoch 133/1000 | Train Loss: 0.3137 Acc: 0.8729 | Val Loss: 0.4020 Acc: 0.7907 | Mem: 792.88MB | Speed: 492.60 samples/s\n",
      "Epoch 134/1000 | Train Loss: 0.2828 Acc: 0.8871 | Val Loss: 0.4263 Acc: 0.8023 | Mem: 792.88MB | Speed: 489.68 samples/s\n",
      "Epoch 135/1000 | Train Loss: 0.2923 Acc: 0.8800 | Val Loss: 0.4259 Acc: 0.7791 | Mem: 792.88MB | Speed: 537.09 samples/s\n",
      "Epoch 136/1000 | Train Loss: 0.3292 Acc: 0.8541 | Val Loss: 0.4241 Acc: 0.7791 | Mem: 792.88MB | Speed: 595.27 samples/s\n",
      "Epoch 137/1000 | Train Loss: 0.3248 Acc: 0.8800 | Val Loss: 0.3874 Acc: 0.7907 | Mem: 792.88MB | Speed: 587.89 samples/s\n",
      "Epoch 138/1000 | Train Loss: 0.3411 Acc: 0.8635 | Val Loss: 0.3638 Acc: 0.8256 | Mem: 792.88MB | Speed: 508.93 samples/s\n",
      "Epoch 139/1000 | Train Loss: 0.3538 Acc: 0.8718 | Val Loss: 0.3653 Acc: 0.8140 | Mem: 792.88MB | Speed: 486.50 samples/s\n",
      "Epoch 140/1000 | Train Loss: 0.3348 Acc: 0.8718 | Val Loss: 0.3722 Acc: 0.7907 | Mem: 792.88MB | Speed: 528.36 samples/s\n",
      "Epoch 141/1000 | Train Loss: 0.3198 Acc: 0.8694 | Val Loss: 0.3863 Acc: 0.7791 | Mem: 792.88MB | Speed: 546.85 samples/s\n",
      "Epoch 142/1000 | Train Loss: 0.3275 Acc: 0.8612 | Val Loss: 0.4105 Acc: 0.7674 | Mem: 792.88MB | Speed: 550.78 samples/s\n",
      "Epoch 143/1000 | Train Loss: 0.2985 Acc: 0.8941 | Val Loss: 0.4157 Acc: 0.7674 | Mem: 792.88MB | Speed: 538.90 samples/s\n",
      "Epoch 144/1000 | Train Loss: 0.3044 Acc: 0.8729 | Val Loss: 0.3754 Acc: 0.7791 | Mem: 792.88MB | Speed: 532.62 samples/s\n",
      "Epoch 145/1000 | Train Loss: 0.2801 Acc: 0.8894 | Val Loss: 0.3459 Acc: 0.7791 | Mem: 792.88MB | Speed: 603.33 samples/s\n",
      "Epoch 146/1000 | Train Loss: 0.3001 Acc: 0.8812 | Val Loss: 0.3309 Acc: 0.7907 | Mem: 792.88MB | Speed: 604.44 samples/s\n",
      "Epoch 147/1000 | Train Loss: 0.2926 Acc: 0.8847 | Val Loss: 0.3339 Acc: 0.8023 | Mem: 792.88MB | Speed: 477.18 samples/s\n",
      "Epoch 148/1000 | Train Loss: 0.3054 Acc: 0.8765 | Val Loss: 0.3487 Acc: 0.8140 | Mem: 792.88MB | Speed: 517.63 samples/s\n",
      "Epoch 149/1000 | Train Loss: 0.3218 Acc: 0.8741 | Val Loss: 0.3371 Acc: 0.8140 | Mem: 792.88MB | Speed: 636.52 samples/s\n",
      "Epoch 150/1000 | Train Loss: 0.3416 Acc: 0.8682 | Val Loss: 0.3109 Acc: 0.7907 | Mem: 792.88MB | Speed: 676.92 samples/s\n",
      "Epoch 151/1000 | Train Loss: 0.3118 Acc: 0.8812 | Val Loss: 0.3283 Acc: 0.8256 | Mem: 792.88MB | Speed: 668.35 samples/s\n",
      "Epoch 152/1000 | Train Loss: 0.3395 Acc: 0.8694 | Val Loss: 0.3651 Acc: 0.8140 | Mem: 792.88MB | Speed: 678.37 samples/s\n",
      "Epoch 153/1000 | Train Loss: 0.2707 Acc: 0.8965 | Val Loss: 0.3505 Acc: 0.8140 | Mem: 792.88MB | Speed: 715.54 samples/s\n",
      "Epoch 154/1000 | Train Loss: 0.3323 Acc: 0.8682 | Val Loss: 0.3301 Acc: 0.8023 | Mem: 792.88MB | Speed: 673.62 samples/s\n",
      "Epoch 155/1000 | Train Loss: 0.3198 Acc: 0.8706 | Val Loss: 0.3226 Acc: 0.8023 | Mem: 792.88MB | Speed: 627.04 samples/s\n",
      "Epoch 156/1000 | Train Loss: 0.2726 Acc: 0.9024 | Val Loss: 0.3213 Acc: 0.8023 | Mem: 792.88MB | Speed: 634.96 samples/s\n",
      "Epoch 157/1000 | Train Loss: 0.2892 Acc: 0.8894 | Val Loss: 0.3394 Acc: 0.8140 | Mem: 792.88MB | Speed: 589.95 samples/s\n",
      "Epoch 158/1000 | Train Loss: 0.2876 Acc: 0.8859 | Val Loss: 0.3793 Acc: 0.8256 | Mem: 792.88MB | Speed: 662.00 samples/s\n",
      "Epoch 159/1000 | Train Loss: 0.2996 Acc: 0.8753 | Val Loss: 0.3638 Acc: 0.8140 | Mem: 792.88MB | Speed: 667.71 samples/s\n",
      "Epoch 160/1000 | Train Loss: 0.2699 Acc: 0.8824 | Val Loss: 0.3100 Acc: 0.8023 | Mem: 792.88MB | Speed: 703.80 samples/s\n",
      "Epoch 161/1000 | Train Loss: 0.2865 Acc: 0.8800 | Val Loss: 0.3059 Acc: 0.8023 | Mem: 792.88MB | Speed: 709.13 samples/s\n",
      "Epoch 162/1000 | Train Loss: 0.2870 Acc: 0.8847 | Val Loss: 0.3166 Acc: 0.8140 | Mem: 792.88MB | Speed: 669.95 samples/s\n",
      "Epoch 163/1000 | Train Loss: 0.2870 Acc: 0.8788 | Val Loss: 0.3477 Acc: 0.8023 | Mem: 792.88MB | Speed: 651.05 samples/s\n",
      "Epoch 164/1000 | Train Loss: 0.3094 Acc: 0.8741 | Val Loss: 0.3750 Acc: 0.8023 | Mem: 792.88MB | Speed: 686.12 samples/s\n",
      "Epoch 165/1000 | Train Loss: 0.2951 Acc: 0.8718 | Val Loss: 0.3808 Acc: 0.8023 | Mem: 792.88MB | Speed: 709.05 samples/s\n",
      "Epoch 166/1000 | Train Loss: 0.2894 Acc: 0.8765 | Val Loss: 0.3415 Acc: 0.8140 | Mem: 792.88MB | Speed: 693.42 samples/s\n",
      "Epoch 167/1000 | Train Loss: 0.2893 Acc: 0.8929 | Val Loss: 0.3158 Acc: 0.8256 | Mem: 792.88MB | Speed: 653.25 samples/s\n",
      "Epoch 168/1000 | Train Loss: 0.3044 Acc: 0.8859 | Val Loss: 0.3344 Acc: 0.8256 | Mem: 792.88MB | Speed: 646.64 samples/s\n",
      "Epoch 169/1000 | Train Loss: 0.2735 Acc: 0.8953 | Val Loss: 0.3472 Acc: 0.8256 | Mem: 792.88MB | Speed: 702.12 samples/s\n",
      "Epoch 170/1000 | Train Loss: 0.2966 Acc: 0.8741 | Val Loss: 0.3402 Acc: 0.8256 | Mem: 792.88MB | Speed: 756.68 samples/s\n",
      "Epoch 171/1000 | Train Loss: 0.2449 Acc: 0.9106 | Val Loss: 0.3315 Acc: 0.8256 | Mem: 792.88MB | Speed: 703.06 samples/s\n",
      "Epoch 172/1000 | Train Loss: 0.3020 Acc: 0.8624 | Val Loss: 0.2966 Acc: 0.8256 | Mem: 792.88MB | Speed: 705.65 samples/s\n",
      "Epoch 173/1000 | Train Loss: 0.2782 Acc: 0.8976 | Val Loss: 0.2917 Acc: 0.8256 | Mem: 792.88MB | Speed: 705.04 samples/s\n",
      "Epoch 174/1000 | Train Loss: 0.2793 Acc: 0.8988 | Val Loss: 0.3075 Acc: 0.8605 | Mem: 792.88MB | Speed: 673.26 samples/s\n",
      "Epoch 175/1000 | Train Loss: 0.2760 Acc: 0.8894 | Val Loss: 0.2956 Acc: 0.8488 | Mem: 792.88MB | Speed: 666.42 samples/s\n",
      "Epoch 176/1000 | Train Loss: 0.2625 Acc: 0.9000 | Val Loss: 0.2786 Acc: 0.8488 | Mem: 792.88MB | Speed: 661.90 samples/s\n",
      "Epoch 177/1000 | Train Loss: 0.2769 Acc: 0.8906 | Val Loss: 0.2692 Acc: 0.8488 | Mem: 792.88MB | Speed: 668.53 samples/s\n",
      "Epoch 178/1000 | Train Loss: 0.2902 Acc: 0.8847 | Val Loss: 0.2736 Acc: 0.8372 | Mem: 792.88MB | Speed: 685.71 samples/s\n",
      "Epoch 179/1000 | Train Loss: 0.2742 Acc: 0.8847 | Val Loss: 0.2903 Acc: 0.8372 | Mem: 792.88MB | Speed: 667.47 samples/s\n",
      "Epoch 180/1000 | Train Loss: 0.2855 Acc: 0.8894 | Val Loss: 0.3026 Acc: 0.8488 | Mem: 792.88MB | Speed: 718.95 samples/s\n",
      "Epoch 181/1000 | Train Loss: 0.2710 Acc: 0.8882 | Val Loss: 0.2950 Acc: 0.8372 | Mem: 792.88MB | Speed: 752.41 samples/s\n",
      "Epoch 182/1000 | Train Loss: 0.2582 Acc: 0.9000 | Val Loss: 0.2919 Acc: 0.8488 | Mem: 792.88MB | Speed: 698.12 samples/s\n",
      "Epoch 183/1000 | Train Loss: 0.2307 Acc: 0.9071 | Val Loss: 0.2831 Acc: 0.8256 | Mem: 792.88MB | Speed: 702.90 samples/s\n",
      "Epoch 184/1000 | Train Loss: 0.2708 Acc: 0.9012 | Val Loss: 0.3028 Acc: 0.8372 | Mem: 792.88MB | Speed: 707.29 samples/s\n",
      "Epoch 185/1000 | Train Loss: 0.2615 Acc: 0.8965 | Val Loss: 0.3306 Acc: 0.8605 | Mem: 792.88MB | Speed: 680.38 samples/s\n",
      "Epoch 186/1000 | Train Loss: 0.2708 Acc: 0.8882 | Val Loss: 0.3234 Acc: 0.8372 | Mem: 792.88MB | Speed: 687.79 samples/s\n",
      "Epoch 187/1000 | Train Loss: 0.2857 Acc: 0.8788 | Val Loss: 0.2880 Acc: 0.8256 | Mem: 792.88MB | Speed: 679.70 samples/s\n",
      "Epoch 188/1000 | Train Loss: 0.2508 Acc: 0.9071 | Val Loss: 0.2854 Acc: 0.8372 | Mem: 792.88MB | Speed: 665.66 samples/s\n",
      "Epoch 189/1000 | Train Loss: 0.2686 Acc: 0.8976 | Val Loss: 0.2894 Acc: 0.8372 | Mem: 792.88MB | Speed: 646.25 samples/s\n",
      "Epoch 190/1000 | Train Loss: 0.2854 Acc: 0.8882 | Val Loss: 0.3201 Acc: 0.8488 | Mem: 792.88MB | Speed: 667.48 samples/s\n",
      "Epoch 191/1000 | Train Loss: 0.2930 Acc: 0.8824 | Val Loss: 0.3418 Acc: 0.8605 | Mem: 792.88MB | Speed: 668.20 samples/s\n",
      "Epoch 192/1000 | Train Loss: 0.2844 Acc: 0.8847 | Val Loss: 0.3159 Acc: 0.8488 | Mem: 792.88MB | Speed: 725.45 samples/s\n",
      "Epoch 193/1000 | Train Loss: 0.2386 Acc: 0.8988 | Val Loss: 0.2972 Acc: 0.8488 | Mem: 792.88MB | Speed: 699.09 samples/s\n",
      "Epoch 194/1000 | Train Loss: 0.2544 Acc: 0.8929 | Val Loss: 0.2930 Acc: 0.8372 | Mem: 792.88MB | Speed: 665.78 samples/s\n",
      "Epoch 195/1000 | Train Loss: 0.2875 Acc: 0.8859 | Val Loss: 0.2677 Acc: 0.8256 | Mem: 792.88MB | Speed: 651.18 samples/s\n",
      "Epoch 196/1000 | Train Loss: 0.2433 Acc: 0.9071 | Val Loss: 0.2539 Acc: 0.8256 | Mem: 792.88MB | Speed: 672.90 samples/s\n",
      "Epoch 197/1000 | Train Loss: 0.2451 Acc: 0.9024 | Val Loss: 0.2590 Acc: 0.8372 | Mem: 792.88MB | Speed: 677.76 samples/s\n",
      "Epoch 198/1000 | Train Loss: 0.2787 Acc: 0.8847 | Val Loss: 0.2758 Acc: 0.8605 | Mem: 792.88MB | Speed: 710.72 samples/s\n",
      "Epoch 199/1000 | Train Loss: 0.2534 Acc: 0.9071 | Val Loss: 0.2921 Acc: 0.8488 | Mem: 792.88MB | Speed: 761.37 samples/s\n",
      "Epoch 200/1000 | Train Loss: 0.2717 Acc: 0.8835 | Val Loss: 0.2987 Acc: 0.8488 | Mem: 792.88MB | Speed: 688.84 samples/s\n",
      "Epoch 201/1000 | Train Loss: 0.2671 Acc: 0.9129 | Val Loss: 0.2584 Acc: 0.8372 | Mem: 792.88MB | Speed: 700.61 samples/s\n",
      "Epoch 202/1000 | Train Loss: 0.2195 Acc: 0.9176 | Val Loss: 0.2431 Acc: 0.8488 | Mem: 792.88MB | Speed: 712.86 samples/s\n",
      "Epoch 203/1000 | Train Loss: 0.2561 Acc: 0.9000 | Val Loss: 0.2559 Acc: 0.8605 | Mem: 792.88MB | Speed: 676.40 samples/s\n",
      "Epoch 204/1000 | Train Loss: 0.2353 Acc: 0.9082 | Val Loss: 0.2894 Acc: 0.8605 | Mem: 792.88MB | Speed: 660.61 samples/s\n",
      "Epoch 205/1000 | Train Loss: 0.2675 Acc: 0.9035 | Val Loss: 0.2995 Acc: 0.8605 | Mem: 792.88MB | Speed: 666.76 samples/s\n",
      "Epoch 206/1000 | Train Loss: 0.2314 Acc: 0.9141 | Val Loss: 0.2839 Acc: 0.8488 | Mem: 792.88MB | Speed: 663.65 samples/s\n",
      "Epoch 207/1000 | Train Loss: 0.2884 Acc: 0.8918 | Val Loss: 0.2441 Acc: 0.8488 | Mem: 792.88MB | Speed: 663.02 samples/s\n",
      "Epoch 208/1000 | Train Loss: 0.2673 Acc: 0.8929 | Val Loss: 0.2464 Acc: 0.8256 | Mem: 792.88MB | Speed: 674.11 samples/s\n",
      "Epoch 209/1000 | Train Loss: 0.2847 Acc: 0.8882 | Val Loss: 0.2567 Acc: 0.8488 | Mem: 792.88MB | Speed: 719.31 samples/s\n",
      "Epoch 210/1000 | Train Loss: 0.2402 Acc: 0.8988 | Val Loss: 0.2560 Acc: 0.8256 | Mem: 792.88MB | Speed: 690.81 samples/s\n",
      "Epoch 211/1000 | Train Loss: 0.2549 Acc: 0.8988 | Val Loss: 0.2685 Acc: 0.8372 | Mem: 792.88MB | Speed: 707.79 samples/s\n",
      "Epoch 212/1000 | Train Loss: 0.1999 Acc: 0.9341 | Val Loss: 0.3031 Acc: 0.8488 | Mem: 792.88MB | Speed: 634.70 samples/s\n",
      "Epoch 213/1000 | Train Loss: 0.2568 Acc: 0.8988 | Val Loss: 0.3114 Acc: 0.8605 | Mem: 792.88MB | Speed: 661.67 samples/s\n",
      "Epoch 214/1000 | Train Loss: 0.2508 Acc: 0.9012 | Val Loss: 0.3250 Acc: 0.8605 | Mem: 792.88MB | Speed: 665.76 samples/s\n",
      "Epoch 215/1000 | Train Loss: 0.2576 Acc: 0.8965 | Val Loss: 0.3127 Acc: 0.8721 | Mem: 792.88MB | Speed: 722.66 samples/s\n",
      "Epoch 216/1000 | Train Loss: 0.2285 Acc: 0.9176 | Val Loss: 0.2662 Acc: 0.8605 | Mem: 792.88MB | Speed: 805.25 samples/s\n",
      "Epoch 217/1000 | Train Loss: 0.2357 Acc: 0.9059 | Val Loss: 0.2424 Acc: 0.8488 | Mem: 792.88MB | Speed: 645.22 samples/s\n",
      "Epoch 218/1000 | Train Loss: 0.2485 Acc: 0.9071 | Val Loss: 0.2615 Acc: 0.8256 | Mem: 792.88MB | Speed: 586.93 samples/s\n",
      "Epoch 219/1000 | Train Loss: 0.2384 Acc: 0.9059 | Val Loss: 0.3017 Acc: 0.8256 | Mem: 792.88MB | Speed: 613.81 samples/s\n",
      "Epoch 220/1000 | Train Loss: 0.2437 Acc: 0.9047 | Val Loss: 0.3292 Acc: 0.8256 | Mem: 792.88MB | Speed: 638.27 samples/s\n",
      "Epoch 221/1000 | Train Loss: 0.2549 Acc: 0.9047 | Val Loss: 0.3270 Acc: 0.8256 | Mem: 792.88MB | Speed: 649.41 samples/s\n",
      "Epoch 222/1000 | Train Loss: 0.2540 Acc: 0.8988 | Val Loss: 0.2989 Acc: 0.8488 | Mem: 792.88MB | Speed: 669.19 samples/s\n",
      "Epoch 223/1000 | Train Loss: 0.2216 Acc: 0.9106 | Val Loss: 0.2774 Acc: 0.8721 | Mem: 792.88MB | Speed: 703.02 samples/s\n",
      "Epoch 224/1000 | Train Loss: 0.2261 Acc: 0.9212 | Val Loss: 0.2580 Acc: 0.8605 | Mem: 792.88MB | Speed: 660.78 samples/s\n",
      "Epoch 225/1000 | Train Loss: 0.2236 Acc: 0.9071 | Val Loss: 0.2464 Acc: 0.8372 | Mem: 792.88MB | Speed: 649.41 samples/s\n",
      "Epoch 226/1000 | Train Loss: 0.2335 Acc: 0.9129 | Val Loss: 0.2378 Acc: 0.8488 | Mem: 792.88MB | Speed: 651.14 samples/s\n",
      "Epoch 227/1000 | Train Loss: 0.2108 Acc: 0.9212 | Val Loss: 0.2549 Acc: 0.8372 | Mem: 792.88MB | Speed: 667.31 samples/s\n",
      "Epoch 228/1000 | Train Loss: 0.2251 Acc: 0.9141 | Val Loss: 0.2657 Acc: 0.8605 | Mem: 792.88MB | Speed: 733.36 samples/s\n",
      "Epoch 229/1000 | Train Loss: 0.2307 Acc: 0.9024 | Val Loss: 0.3027 Acc: 0.8605 | Mem: 792.88MB | Speed: 661.40 samples/s\n",
      "Epoch 230/1000 | Train Loss: 0.2605 Acc: 0.9082 | Val Loss: 0.2973 Acc: 0.8721 | Mem: 792.88MB | Speed: 689.31 samples/s\n",
      "Epoch 231/1000 | Train Loss: 0.2403 Acc: 0.9024 | Val Loss: 0.2755 Acc: 0.8488 | Mem: 792.88MB | Speed: 649.65 samples/s\n",
      "Epoch 232/1000 | Train Loss: 0.2386 Acc: 0.9059 | Val Loss: 0.2603 Acc: 0.8721 | Mem: 792.88MB | Speed: 656.91 samples/s\n",
      "Epoch 233/1000 | Train Loss: 0.2813 Acc: 0.9000 | Val Loss: 0.2370 Acc: 0.8721 | Mem: 792.88MB | Speed: 694.03 samples/s\n",
      "Epoch 234/1000 | Train Loss: 0.2297 Acc: 0.9129 | Val Loss: 0.2450 Acc: 0.8721 | Mem: 792.88MB | Speed: 656.16 samples/s\n",
      "Epoch 235/1000 | Train Loss: 0.2626 Acc: 0.9012 | Val Loss: 0.2662 Acc: 0.8605 | Mem: 792.88MB | Speed: 658.71 samples/s\n",
      "Epoch 236/1000 | Train Loss: 0.2320 Acc: 0.9106 | Val Loss: 0.2727 Acc: 0.8488 | Mem: 792.88MB | Speed: 712.80 samples/s\n",
      "Epoch 237/1000 | Train Loss: 0.2324 Acc: 0.9071 | Val Loss: 0.2806 Acc: 0.8605 | Mem: 792.88MB | Speed: 691.09 samples/s\n",
      "Epoch 238/1000 | Train Loss: 0.2497 Acc: 0.8941 | Val Loss: 0.3030 Acc: 0.8488 | Mem: 792.88MB | Speed: 693.80 samples/s\n",
      "Epoch 239/1000 | Train Loss: 0.2350 Acc: 0.9176 | Val Loss: 0.3276 Acc: 0.8605 | Mem: 792.88MB | Speed: 667.95 samples/s\n",
      "Epoch 240/1000 | Train Loss: 0.2213 Acc: 0.9141 | Val Loss: 0.3163 Acc: 0.8605 | Mem: 792.88MB | Speed: 663.28 samples/s\n",
      "Epoch 241/1000 | Train Loss: 0.2448 Acc: 0.9094 | Val Loss: 0.2804 Acc: 0.8605 | Mem: 792.88MB | Speed: 652.33 samples/s\n",
      "Epoch 242/1000 | Train Loss: 0.2058 Acc: 0.9118 | Val Loss: 0.2671 Acc: 0.8488 | Mem: 792.88MB | Speed: 683.20 samples/s\n",
      "Epoch 243/1000 | Train Loss: 0.2145 Acc: 0.9188 | Val Loss: 0.2641 Acc: 0.8605 | Mem: 792.88MB | Speed: 718.01 samples/s\n",
      "Epoch 244/1000 | Train Loss: 0.2467 Acc: 0.9094 | Val Loss: 0.2615 Acc: 0.8488 | Mem: 792.88MB | Speed: 744.26 samples/s\n",
      "Epoch 245/1000 | Train Loss: 0.2265 Acc: 0.9094 | Val Loss: 0.2592 Acc: 0.8721 | Mem: 792.88MB | Speed: 700.03 samples/s\n",
      "Epoch 246/1000 | Train Loss: 0.2122 Acc: 0.9176 | Val Loss: 0.2526 Acc: 0.8605 | Mem: 792.88MB | Speed: 641.84 samples/s\n",
      "Epoch 247/1000 | Train Loss: 0.2198 Acc: 0.9141 | Val Loss: 0.2609 Acc: 0.8721 | Mem: 792.88MB | Speed: 652.29 samples/s\n",
      "Epoch 248/1000 | Train Loss: 0.2009 Acc: 0.9165 | Val Loss: 0.2757 Acc: 0.8605 | Mem: 792.88MB | Speed: 724.37 samples/s\n",
      "Epoch 249/1000 | Train Loss: 0.1820 Acc: 0.9318 | Val Loss: 0.2752 Acc: 0.8605 | Mem: 792.88MB | Speed: 673.82 samples/s\n",
      "Epoch 250/1000 | Train Loss: 0.2154 Acc: 0.9165 | Val Loss: 0.2429 Acc: 0.8721 | Mem: 792.88MB | Speed: 713.73 samples/s\n",
      "Epoch 251/1000 | Train Loss: 0.2095 Acc: 0.9165 | Val Loss: 0.2160 Acc: 0.8721 | Mem: 792.88MB | Speed: 719.10 samples/s\n",
      "Epoch 252/1000 | Train Loss: 0.2335 Acc: 0.9188 | Val Loss: 0.2228 Acc: 0.9186 | Mem: 792.88MB | Speed: 702.18 samples/s\n",
      "Epoch 253/1000 | Train Loss: 0.2365 Acc: 0.9200 | Val Loss: 0.2437 Acc: 0.9186 | Mem: 792.88MB | Speed: 654.09 samples/s\n",
      "Epoch 254/1000 | Train Loss: 0.1942 Acc: 0.9282 | Val Loss: 0.2440 Acc: 0.8837 | Mem: 792.88MB | Speed: 639.38 samples/s\n",
      "Epoch 255/1000 | Train Loss: 0.2388 Acc: 0.9024 | Val Loss: 0.2237 Acc: 0.8721 | Mem: 792.88MB | Speed: 684.60 samples/s\n",
      "Epoch 256/1000 | Train Loss: 0.2463 Acc: 0.9000 | Val Loss: 0.2067 Acc: 0.8721 | Mem: 792.88MB | Speed: 645.90 samples/s\n",
      "Epoch 257/1000 | Train Loss: 0.2209 Acc: 0.9141 | Val Loss: 0.2202 Acc: 0.8488 | Mem: 792.88MB | Speed: 644.17 samples/s\n",
      "Epoch 258/1000 | Train Loss: 0.2043 Acc: 0.9176 | Val Loss: 0.2329 Acc: 0.8721 | Mem: 792.88MB | Speed: 698.96 samples/s\n",
      "Epoch 259/1000 | Train Loss: 0.2015 Acc: 0.9118 | Val Loss: 0.2487 Acc: 0.8837 | Mem: 792.88MB | Speed: 648.88 samples/s\n",
      "Epoch 260/1000 | Train Loss: 0.2206 Acc: 0.9082 | Val Loss: 0.2559 Acc: 0.8837 | Mem: 792.88MB | Speed: 662.78 samples/s\n",
      "Epoch 261/1000 | Train Loss: 0.2208 Acc: 0.9129 | Val Loss: 0.2554 Acc: 0.8721 | Mem: 792.88MB | Speed: 700.17 samples/s\n",
      "Epoch 262/1000 | Train Loss: 0.2213 Acc: 0.9094 | Val Loss: 0.2387 Acc: 0.8721 | Mem: 792.88MB | Speed: 733.35 samples/s\n",
      "Epoch 263/1000 | Train Loss: 0.1907 Acc: 0.9224 | Val Loss: 0.2482 Acc: 0.8721 | Mem: 792.88MB | Speed: 671.82 samples/s\n",
      "Epoch 264/1000 | Train Loss: 0.2002 Acc: 0.9212 | Val Loss: 0.2551 Acc: 0.8953 | Mem: 792.88MB | Speed: 645.32 samples/s\n",
      "Epoch 265/1000 | Train Loss: 0.2036 Acc: 0.9224 | Val Loss: 0.2452 Acc: 0.8953 | Mem: 792.88MB | Speed: 657.55 samples/s\n",
      "Epoch 266/1000 | Train Loss: 0.1861 Acc: 0.9329 | Val Loss: 0.2279 Acc: 0.8837 | Mem: 792.88MB | Speed: 732.69 samples/s\n",
      "Epoch 267/1000 | Train Loss: 0.2200 Acc: 0.9129 | Val Loss: 0.2215 Acc: 0.8837 | Mem: 792.88MB | Speed: 687.88 samples/s\n",
      "Epoch 268/1000 | Train Loss: 0.2015 Acc: 0.9141 | Val Loss: 0.2262 Acc: 0.8721 | Mem: 792.88MB | Speed: 715.12 samples/s\n",
      "Epoch 269/1000 | Train Loss: 0.2232 Acc: 0.9059 | Val Loss: 0.2276 Acc: 0.8605 | Mem: 792.88MB | Speed: 734.02 samples/s\n",
      "Epoch 270/1000 | Train Loss: 0.1998 Acc: 0.9282 | Val Loss: 0.2185 Acc: 0.8837 | Mem: 792.88MB | Speed: 708.19 samples/s\n",
      "Epoch 271/1000 | Train Loss: 0.2178 Acc: 0.9165 | Val Loss: 0.2056 Acc: 0.8953 | Mem: 792.88MB | Speed: 674.46 samples/s\n",
      "Epoch 272/1000 | Train Loss: 0.2043 Acc: 0.9294 | Val Loss: 0.1963 Acc: 0.8953 | Mem: 792.88MB | Speed: 655.26 samples/s\n",
      "Epoch 273/1000 | Train Loss: 0.2200 Acc: 0.9224 | Val Loss: 0.2158 Acc: 0.8837 | Mem: 792.88MB | Speed: 665.71 samples/s\n",
      "Epoch 274/1000 | Train Loss: 0.2044 Acc: 0.9247 | Val Loss: 0.2404 Acc: 0.8721 | Mem: 792.88MB | Speed: 639.01 samples/s\n",
      "Epoch 275/1000 | Train Loss: 0.1856 Acc: 0.9282 | Val Loss: 0.2353 Acc: 0.8837 | Mem: 792.88MB | Speed: 705.60 samples/s\n",
      "Epoch 276/1000 | Train Loss: 0.2401 Acc: 0.9059 | Val Loss: 0.2118 Acc: 0.8605 | Mem: 792.88MB | Speed: 686.40 samples/s\n",
      "Epoch 277/1000 | Train Loss: 0.1744 Acc: 0.9388 | Val Loss: 0.2058 Acc: 0.8721 | Mem: 792.88MB | Speed: 728.15 samples/s\n",
      "Epoch 278/1000 | Train Loss: 0.2078 Acc: 0.9129 | Val Loss: 0.2051 Acc: 0.8953 | Mem: 792.88MB | Speed: 673.76 samples/s\n",
      "Epoch 279/1000 | Train Loss: 0.1998 Acc: 0.9235 | Val Loss: 0.2146 Acc: 0.9070 | Mem: 792.88MB | Speed: 685.74 samples/s\n",
      "Epoch 280/1000 | Train Loss: 0.2207 Acc: 0.9094 | Val Loss: 0.2402 Acc: 0.8837 | Mem: 792.88MB | Speed: 656.28 samples/s\n",
      "Epoch 281/1000 | Train Loss: 0.2016 Acc: 0.9259 | Val Loss: 0.2411 Acc: 0.8953 | Mem: 792.88MB | Speed: 647.47 samples/s\n",
      "Epoch 282/1000 | Train Loss: 0.1891 Acc: 0.9341 | Val Loss: 0.2236 Acc: 0.8837 | Mem: 792.88MB | Speed: 715.63 samples/s\n",
      "Epoch 283/1000 | Train Loss: 0.1920 Acc: 0.9341 | Val Loss: 0.2030 Acc: 0.8837 | Mem: 792.88MB | Speed: 692.12 samples/s\n",
      "Epoch 284/1000 | Train Loss: 0.2104 Acc: 0.9059 | Val Loss: 0.2045 Acc: 0.8837 | Mem: 792.88MB | Speed: 605.92 samples/s\n",
      "Epoch 285/1000 | Train Loss: 0.2159 Acc: 0.9082 | Val Loss: 0.2434 Acc: 0.8953 | Mem: 792.88MB | Speed: 663.12 samples/s\n",
      "Epoch 286/1000 | Train Loss: 0.2205 Acc: 0.9129 | Val Loss: 0.2854 Acc: 0.8953 | Mem: 792.88MB | Speed: 668.94 samples/s\n",
      "Epoch 287/1000 | Train Loss: 0.1977 Acc: 0.9212 | Val Loss: 0.3027 Acc: 0.8488 | Mem: 792.88MB | Speed: 675.57 samples/s\n",
      "Epoch 288/1000 | Train Loss: 0.2109 Acc: 0.9271 | Val Loss: 0.2615 Acc: 0.8721 | Mem: 792.88MB | Speed: 588.58 samples/s\n",
      "Epoch 289/1000 | Train Loss: 0.2102 Acc: 0.9259 | Val Loss: 0.2435 Acc: 0.8837 | Mem: 792.88MB | Speed: 642.17 samples/s\n",
      "Epoch 290/1000 | Train Loss: 0.2092 Acc: 0.9165 | Val Loss: 0.2355 Acc: 0.8721 | Mem: 792.88MB | Speed: 580.58 samples/s\n",
      "Epoch 291/1000 | Train Loss: 0.2178 Acc: 0.9129 | Val Loss: 0.2447 Acc: 0.8837 | Mem: 792.88MB | Speed: 677.95 samples/s\n",
      "Epoch 292/1000 | Train Loss: 0.1880 Acc: 0.9282 | Val Loss: 0.2835 Acc: 0.9070 | Mem: 792.88MB | Speed: 654.75 samples/s\n",
      "Epoch 293/1000 | Train Loss: 0.2106 Acc: 0.9188 | Val Loss: 0.2775 Acc: 0.8953 | Mem: 792.88MB | Speed: 664.39 samples/s\n",
      "Epoch 294/1000 | Train Loss: 0.2006 Acc: 0.9318 | Val Loss: 0.2654 Acc: 0.8837 | Mem: 792.88MB | Speed: 696.44 samples/s\n",
      "Epoch 295/1000 | Train Loss: 0.1941 Acc: 0.9247 | Val Loss: 0.2452 Acc: 0.8953 | Mem: 792.88MB | Speed: 668.23 samples/s\n",
      "Epoch 296/1000 | Train Loss: 0.2010 Acc: 0.9153 | Val Loss: 0.2213 Acc: 0.8953 | Mem: 792.88MB | Speed: 754.89 samples/s\n",
      "Epoch 297/1000 | Train Loss: 0.1868 Acc: 0.9376 | Val Loss: 0.2095 Acc: 0.8953 | Mem: 792.88MB | Speed: 673.15 samples/s\n",
      "Epoch 298/1000 | Train Loss: 0.2102 Acc: 0.9247 | Val Loss: 0.2150 Acc: 0.8837 | Mem: 792.88MB | Speed: 726.35 samples/s\n",
      "Epoch 299/1000 | Train Loss: 0.1882 Acc: 0.9271 | Val Loss: 0.2476 Acc: 0.8837 | Mem: 792.88MB | Speed: 680.54 samples/s\n",
      "Epoch 300/1000 | Train Loss: 0.2072 Acc: 0.9224 | Val Loss: 0.2493 Acc: 0.9070 | Mem: 792.88MB | Speed: 692.14 samples/s\n",
      "Epoch 301/1000 | Train Loss: 0.2034 Acc: 0.9188 | Val Loss: 0.2322 Acc: 0.9070 | Mem: 792.88MB | Speed: 757.23 samples/s\n",
      "Epoch 302/1000 | Train Loss: 0.2256 Acc: 0.9129 | Val Loss: 0.2100 Acc: 0.9070 | Mem: 792.88MB | Speed: 735.52 samples/s\n",
      "Epoch 303/1000 | Train Loss: 0.2213 Acc: 0.9176 | Val Loss: 0.1941 Acc: 0.9186 | Mem: 792.88MB | Speed: 682.94 samples/s\n",
      "Epoch 304/1000 | Train Loss: 0.2174 Acc: 0.9071 | Val Loss: 0.2012 Acc: 0.8953 | Mem: 792.88MB | Speed: 746.66 samples/s\n",
      "Epoch 305/1000 | Train Loss: 0.2076 Acc: 0.9035 | Val Loss: 0.2248 Acc: 0.9070 | Mem: 792.88MB | Speed: 684.70 samples/s\n",
      "Epoch 306/1000 | Train Loss: 0.1905 Acc: 0.9294 | Val Loss: 0.2620 Acc: 0.8953 | Mem: 792.88MB | Speed: 706.88 samples/s\n",
      "Epoch 307/1000 | Train Loss: 0.1873 Acc: 0.9224 | Val Loss: 0.2292 Acc: 0.8953 | Mem: 792.88MB | Speed: 675.49 samples/s\n",
      "Epoch 308/1000 | Train Loss: 0.1943 Acc: 0.9200 | Val Loss: 0.2103 Acc: 0.9070 | Mem: 792.88MB | Speed: 678.68 samples/s\n",
      "Epoch 309/1000 | Train Loss: 0.2183 Acc: 0.9129 | Val Loss: 0.2197 Acc: 0.9070 | Mem: 792.88MB | Speed: 646.41 samples/s\n",
      "Epoch 310/1000 | Train Loss: 0.2065 Acc: 0.9271 | Val Loss: 0.2028 Acc: 0.8953 | Mem: 792.88MB | Speed: 644.12 samples/s\n",
      "Epoch 311/1000 | Train Loss: 0.2187 Acc: 0.9141 | Val Loss: 0.1905 Acc: 0.9070 | Mem: 792.88MB | Speed: 694.01 samples/s\n",
      "Epoch 312/1000 | Train Loss: 0.1773 Acc: 0.9259 | Val Loss: 0.2036 Acc: 0.8837 | Mem: 792.88MB | Speed: 649.28 samples/s\n",
      "Epoch 313/1000 | Train Loss: 0.1979 Acc: 0.9200 | Val Loss: 0.2077 Acc: 0.8953 | Mem: 792.88MB | Speed: 656.89 samples/s\n",
      "Epoch 314/1000 | Train Loss: 0.1915 Acc: 0.9294 | Val Loss: 0.1974 Acc: 0.8953 | Mem: 792.88MB | Speed: 728.95 samples/s\n",
      "Epoch 315/1000 | Train Loss: 0.1781 Acc: 0.9294 | Val Loss: 0.1761 Acc: 0.9070 | Mem: 792.88MB | Speed: 655.36 samples/s\n",
      "Epoch 316/1000 | Train Loss: 0.2191 Acc: 0.9129 | Val Loss: 0.1657 Acc: 0.9186 | Mem: 792.88MB | Speed: 704.09 samples/s\n",
      "Epoch 317/1000 | Train Loss: 0.1852 Acc: 0.9306 | Val Loss: 0.1766 Acc: 0.9186 | Mem: 792.88MB | Speed: 703.55 samples/s\n",
      "Epoch 318/1000 | Train Loss: 0.2269 Acc: 0.9141 | Val Loss: 0.2210 Acc: 0.8953 | Mem: 792.88MB | Speed: 704.66 samples/s\n",
      "Epoch 319/1000 | Train Loss: 0.1860 Acc: 0.9259 | Val Loss: 0.2648 Acc: 0.8953 | Mem: 792.88MB | Speed: 723.20 samples/s\n",
      "Epoch 320/1000 | Train Loss: 0.1687 Acc: 0.9341 | Val Loss: 0.2483 Acc: 0.8837 | Mem: 792.88MB | Speed: 706.68 samples/s\n",
      "Epoch 321/1000 | Train Loss: 0.1800 Acc: 0.9294 | Val Loss: 0.2031 Acc: 0.9070 | Mem: 792.88MB | Speed: 718.05 samples/s\n",
      "Epoch 322/1000 | Train Loss: 0.2039 Acc: 0.9224 | Val Loss: 0.1778 Acc: 0.8837 | Mem: 792.88MB | Speed: 706.65 samples/s\n",
      "Epoch 323/1000 | Train Loss: 0.2032 Acc: 0.9282 | Val Loss: 0.1623 Acc: 0.9186 | Mem: 792.88MB | Speed: 697.31 samples/s\n",
      "Epoch 324/1000 | Train Loss: 0.1932 Acc: 0.9247 | Val Loss: 0.1806 Acc: 0.8953 | Mem: 792.88MB | Speed: 711.76 samples/s\n",
      "Epoch 325/1000 | Train Loss: 0.1934 Acc: 0.9318 | Val Loss: 0.2264 Acc: 0.8721 | Mem: 792.88MB | Speed: 711.43 samples/s\n",
      "Epoch 326/1000 | Train Loss: 0.2339 Acc: 0.9106 | Val Loss: 0.1608 Acc: 0.8953 | Mem: 792.88MB | Speed: 766.79 samples/s\n",
      "Epoch 327/1000 | Train Loss: 0.1700 Acc: 0.9271 | Val Loss: 0.1479 Acc: 0.9070 | Mem: 792.88MB | Speed: 680.62 samples/s\n",
      "Epoch 328/1000 | Train Loss: 0.1855 Acc: 0.9294 | Val Loss: 0.1765 Acc: 0.8837 | Mem: 792.88MB | Speed: 753.41 samples/s\n",
      "Epoch 329/1000 | Train Loss: 0.1916 Acc: 0.9212 | Val Loss: 0.1897 Acc: 0.8721 | Mem: 792.88MB | Speed: 674.08 samples/s\n",
      "Epoch 330/1000 | Train Loss: 0.1825 Acc: 0.9400 | Val Loss: 0.2051 Acc: 0.8721 | Mem: 792.88MB | Speed: 735.84 samples/s\n",
      "Epoch 331/1000 | Train Loss: 0.1678 Acc: 0.9329 | Val Loss: 0.2069 Acc: 0.8837 | Mem: 792.88MB | Speed: 700.54 samples/s\n",
      "Epoch 332/1000 | Train Loss: 0.1856 Acc: 0.9318 | Val Loss: 0.1998 Acc: 0.8837 | Mem: 792.88MB | Speed: 676.22 samples/s\n",
      "Epoch 333/1000 | Train Loss: 0.2200 Acc: 0.9212 | Val Loss: 0.1624 Acc: 0.8953 | Mem: 792.88MB | Speed: 667.37 samples/s\n",
      "Epoch 334/1000 | Train Loss: 0.1962 Acc: 0.9341 | Val Loss: 0.1763 Acc: 0.8953 | Mem: 792.88MB | Speed: 665.37 samples/s\n",
      "Epoch 335/1000 | Train Loss: 0.1769 Acc: 0.9329 | Val Loss: 0.1896 Acc: 0.8837 | Mem: 792.88MB | Speed: 662.05 samples/s\n",
      "Epoch 336/1000 | Train Loss: 0.1802 Acc: 0.9224 | Val Loss: 0.1863 Acc: 0.8837 | Mem: 792.88MB | Speed: 713.66 samples/s\n",
      "Epoch 337/1000 | Train Loss: 0.1729 Acc: 0.9306 | Val Loss: 0.1699 Acc: 0.9070 | Mem: 792.88MB | Speed: 699.02 samples/s\n",
      "Epoch 338/1000 | Train Loss: 0.1635 Acc: 0.9376 | Val Loss: 0.1807 Acc: 0.9186 | Mem: 792.88MB | Speed: 707.73 samples/s\n",
      "Epoch 339/1000 | Train Loss: 0.1920 Acc: 0.9282 | Val Loss: 0.1847 Acc: 0.9186 | Mem: 792.88MB | Speed: 679.31 samples/s\n",
      "Epoch 340/1000 | Train Loss: 0.2071 Acc: 0.9153 | Val Loss: 0.1743 Acc: 0.9186 | Mem: 792.88MB | Speed: 658.25 samples/s\n",
      "Epoch 341/1000 | Train Loss: 0.1783 Acc: 0.9271 | Val Loss: 0.1754 Acc: 0.9070 | Mem: 792.88MB | Speed: 655.49 samples/s\n",
      "Epoch 342/1000 | Train Loss: 0.1903 Acc: 0.9306 | Val Loss: 0.1909 Acc: 0.9070 | Mem: 792.88MB | Speed: 656.49 samples/s\n",
      "Epoch 343/1000 | Train Loss: 0.1791 Acc: 0.9329 | Val Loss: 0.2379 Acc: 0.8953 | Mem: 792.88MB | Speed: 735.64 samples/s\n",
      "Epoch 344/1000 | Train Loss: 0.2016 Acc: 0.9282 | Val Loss: 0.2642 Acc: 0.8953 | Mem: 792.88MB | Speed: 678.11 samples/s\n",
      "Epoch 345/1000 | Train Loss: 0.1678 Acc: 0.9294 | Val Loss: 0.2110 Acc: 0.9070 | Mem: 792.88MB | Speed: 716.22 samples/s\n",
      "Epoch 346/1000 | Train Loss: 0.1912 Acc: 0.9235 | Val Loss: 0.2007 Acc: 0.8837 | Mem: 792.88MB | Speed: 744.68 samples/s\n",
      "Epoch 347/1000 | Train Loss: 0.1847 Acc: 0.9329 | Val Loss: 0.2124 Acc: 0.8721 | Mem: 792.88MB | Speed: 702.87 samples/s\n",
      "Epoch 348/1000 | Train Loss: 0.1711 Acc: 0.9376 | Val Loss: 0.2165 Acc: 0.8721 | Mem: 792.88MB | Speed: 673.59 samples/s\n",
      "Epoch 349/1000 | Train Loss: 0.1815 Acc: 0.9412 | Val Loss: 0.1882 Acc: 0.8953 | Mem: 792.88MB | Speed: 659.87 samples/s\n",
      "Epoch 350/1000 | Train Loss: 0.1824 Acc: 0.9341 | Val Loss: 0.1825 Acc: 0.8837 | Mem: 792.88MB | Speed: 667.31 samples/s\n",
      "Epoch 351/1000 | Train Loss: 0.1771 Acc: 0.9306 | Val Loss: 0.2158 Acc: 0.8837 | Mem: 792.88MB | Speed: 654.21 samples/s\n",
      "Epoch 352/1000 | Train Loss: 0.1660 Acc: 0.9424 | Val Loss: 0.2281 Acc: 0.8953 | Mem: 792.88MB | Speed: 717.70 samples/s\n",
      "Epoch 353/1000 | Train Loss: 0.1982 Acc: 0.9200 | Val Loss: 0.2139 Acc: 0.8953 | Mem: 792.88MB | Speed: 734.10 samples/s\n",
      "Epoch 354/1000 | Train Loss: 0.1884 Acc: 0.9235 | Val Loss: 0.2043 Acc: 0.8953 | Mem: 792.88MB | Speed: 716.31 samples/s\n",
      "Epoch 355/1000 | Train Loss: 0.1746 Acc: 0.9306 | Val Loss: 0.1895 Acc: 0.9186 | Mem: 792.88MB | Speed: 749.38 samples/s\n",
      "Epoch 356/1000 | Train Loss: 0.1308 Acc: 0.9565 | Val Loss: 0.1781 Acc: 0.9186 | Mem: 792.88MB | Speed: 699.29 samples/s\n",
      "Epoch 357/1000 | Train Loss: 0.1908 Acc: 0.9294 | Val Loss: 0.1698 Acc: 0.9186 | Mem: 792.88MB | Speed: 675.11 samples/s\n",
      "Epoch 358/1000 | Train Loss: 0.1729 Acc: 0.9341 | Val Loss: 0.1791 Acc: 0.9070 | Mem: 792.88MB | Speed: 661.81 samples/s\n",
      "Epoch 359/1000 | Train Loss: 0.1483 Acc: 0.9471 | Val Loss: 0.1812 Acc: 0.8953 | Mem: 792.88MB | Speed: 652.77 samples/s\n",
      "Epoch 360/1000 | Train Loss: 0.2237 Acc: 0.9247 | Val Loss: 0.1773 Acc: 0.8953 | Mem: 792.88MB | Speed: 661.51 samples/s\n",
      "Epoch 361/1000 | Train Loss: 0.1545 Acc: 0.9412 | Val Loss: 0.1852 Acc: 0.8837 | Mem: 792.88MB | Speed: 679.10 samples/s\n",
      "Epoch 362/1000 | Train Loss: 0.1817 Acc: 0.9306 | Val Loss: 0.1906 Acc: 0.8837 | Mem: 792.88MB | Speed: 660.05 samples/s\n",
      "Epoch 363/1000 | Train Loss: 0.1686 Acc: 0.9376 | Val Loss: 0.2302 Acc: 0.8837 | Mem: 792.88MB | Speed: 701.21 samples/s\n",
      "Epoch 364/1000 | Train Loss: 0.1728 Acc: 0.9318 | Val Loss: 0.2093 Acc: 0.8953 | Mem: 792.88MB | Speed: 736.62 samples/s\n",
      "Epoch 365/1000 | Train Loss: 0.1803 Acc: 0.9376 | Val Loss: 0.1908 Acc: 0.9070 | Mem: 792.88MB | Speed: 742.51 samples/s\n",
      "Epoch 366/1000 | Train Loss: 0.1768 Acc: 0.9282 | Val Loss: 0.1971 Acc: 0.8953 | Mem: 792.88MB | Speed: 652.06 samples/s\n",
      "Epoch 367/1000 | Train Loss: 0.1523 Acc: 0.9424 | Val Loss: 0.1989 Acc: 0.8953 | Mem: 792.88MB | Speed: 701.14 samples/s\n",
      "Epoch 368/1000 | Train Loss: 0.1557 Acc: 0.9471 | Val Loss: 0.2135 Acc: 0.8837 | Mem: 792.88MB | Speed: 697.52 samples/s\n",
      "Epoch 369/1000 | Train Loss: 0.1739 Acc: 0.9247 | Val Loss: 0.2305 Acc: 0.8721 | Mem: 792.88MB | Speed: 712.73 samples/s\n",
      "Epoch 370/1000 | Train Loss: 0.1715 Acc: 0.9400 | Val Loss: 0.2293 Acc: 0.8837 | Mem: 792.88MB | Speed: 728.00 samples/s\n",
      "Epoch 371/1000 | Train Loss: 0.1554 Acc: 0.9482 | Val Loss: 0.2009 Acc: 0.8953 | Mem: 792.88MB | Speed: 704.68 samples/s\n",
      "Epoch 372/1000 | Train Loss: 0.1822 Acc: 0.9447 | Val Loss: 0.1825 Acc: 0.9070 | Mem: 792.88MB | Speed: 708.24 samples/s\n",
      "Epoch 373/1000 | Train Loss: 0.1585 Acc: 0.9400 | Val Loss: 0.1763 Acc: 0.9070 | Mem: 792.88MB | Speed: 700.30 samples/s\n",
      "Epoch 374/1000 | Train Loss: 0.1832 Acc: 0.9247 | Val Loss: 0.1874 Acc: 0.9070 | Mem: 792.88MB | Speed: 712.77 samples/s\n",
      "Epoch 375/1000 | Train Loss: 0.1851 Acc: 0.9341 | Val Loss: 0.1704 Acc: 0.9186 | Mem: 792.88MB | Speed: 704.09 samples/s\n",
      "Epoch 376/1000 | Train Loss: 0.1816 Acc: 0.9282 | Val Loss: 0.1625 Acc: 0.9070 | Mem: 792.88MB | Speed: 739.27 samples/s\n",
      "Epoch 377/1000 | Train Loss: 0.1628 Acc: 0.9294 | Val Loss: 0.1608 Acc: 0.9186 | Mem: 792.88MB | Speed: 742.05 samples/s\n",
      "Epoch 378/1000 | Train Loss: 0.2221 Acc: 0.9176 | Val Loss: 0.1808 Acc: 0.8837 | Mem: 792.88MB | Speed: 704.74 samples/s\n",
      "Epoch 379/1000 | Train Loss: 0.1540 Acc: 0.9388 | Val Loss: 0.2271 Acc: 0.8837 | Mem: 792.88MB | Speed: 668.27 samples/s\n",
      "Epoch 380/1000 | Train Loss: 0.1679 Acc: 0.9435 | Val Loss: 0.2582 Acc: 0.8721 | Mem: 792.88MB | Speed: 667.14 samples/s\n",
      "Epoch 381/1000 | Train Loss: 0.1815 Acc: 0.9318 | Val Loss: 0.2443 Acc: 0.8837 | Mem: 792.88MB | Speed: 686.72 samples/s\n",
      "Epoch 382/1000 | Train Loss: 0.1976 Acc: 0.9353 | Val Loss: 0.2102 Acc: 0.8953 | Mem: 792.88MB | Speed: 693.65 samples/s\n",
      "Epoch 383/1000 | Train Loss: 0.1663 Acc: 0.9435 | Val Loss: 0.1781 Acc: 0.9070 | Mem: 792.88MB | Speed: 714.82 samples/s\n",
      "Epoch 384/1000 | Train Loss: 0.1832 Acc: 0.9247 | Val Loss: 0.1698 Acc: 0.9070 | Mem: 792.88MB | Speed: 751.16 samples/s\n",
      "Epoch 385/1000 | Train Loss: 0.1910 Acc: 0.9294 | Val Loss: 0.1927 Acc: 0.8953 | Mem: 792.88MB | Speed: 692.52 samples/s\n",
      "Epoch 386/1000 | Train Loss: 0.1499 Acc: 0.9388 | Val Loss: 0.1925 Acc: 0.8721 | Mem: 792.88MB | Speed: 654.31 samples/s\n",
      "Epoch 387/1000 | Train Loss: 0.1684 Acc: 0.9482 | Val Loss: 0.1970 Acc: 0.8605 | Mem: 792.88MB | Speed: 657.74 samples/s\n",
      "Epoch 388/1000 | Train Loss: 0.1571 Acc: 0.9376 | Val Loss: 0.1977 Acc: 0.8837 | Mem: 792.88MB | Speed: 752.12 samples/s\n",
      "Epoch 389/1000 | Train Loss: 0.1498 Acc: 0.9329 | Val Loss: 0.2006 Acc: 0.8605 | Mem: 792.88MB | Speed: 656.19 samples/s\n",
      "Epoch 390/1000 | Train Loss: 0.1846 Acc: 0.9271 | Val Loss: 0.1761 Acc: 0.9070 | Mem: 792.88MB | Speed: 751.44 samples/s\n",
      "Epoch 391/1000 | Train Loss: 0.1819 Acc: 0.9388 | Val Loss: 0.1595 Acc: 0.8953 | Mem: 792.88MB | Speed: 665.10 samples/s\n",
      "Epoch 392/1000 | Train Loss: 0.1793 Acc: 0.9271 | Val Loss: 0.1603 Acc: 0.9070 | Mem: 792.88MB | Speed: 717.65 samples/s\n",
      "Epoch 393/1000 | Train Loss: 0.1868 Acc: 0.9259 | Val Loss: 0.1906 Acc: 0.8721 | Mem: 792.88MB | Speed: 731.67 samples/s\n",
      "Epoch 394/1000 | Train Loss: 0.1926 Acc: 0.9212 | Val Loss: 0.2182 Acc: 0.8837 | Mem: 792.88MB | Speed: 708.51 samples/s\n",
      "Epoch 395/1000 | Train Loss: 0.1719 Acc: 0.9365 | Val Loss: 0.2156 Acc: 0.8837 | Mem: 792.88MB | Speed: 740.31 samples/s\n",
      "Epoch 396/1000 | Train Loss: 0.1737 Acc: 0.9424 | Val Loss: 0.2073 Acc: 0.8837 | Mem: 792.88MB | Speed: 754.81 samples/s\n",
      "Epoch 397/1000 | Train Loss: 0.1937 Acc: 0.9235 | Val Loss: 0.1918 Acc: 0.8837 | Mem: 792.88MB | Speed: 675.27 samples/s\n",
      "Epoch 398/1000 | Train Loss: 0.1613 Acc: 0.9412 | Val Loss: 0.1854 Acc: 0.8953 | Mem: 792.88MB | Speed: 717.97 samples/s\n",
      "Epoch 399/1000 | Train Loss: 0.1490 Acc: 0.9376 | Val Loss: 0.1705 Acc: 0.8953 | Mem: 792.88MB | Speed: 752.44 samples/s\n",
      "Epoch 400/1000 | Train Loss: 0.1805 Acc: 0.9353 | Val Loss: 0.1710 Acc: 0.8953 | Mem: 792.88MB | Speed: 696.61 samples/s\n",
      "Epoch 401/1000 | Train Loss: 0.1848 Acc: 0.9341 | Val Loss: 0.1944 Acc: 0.8953 | Mem: 792.88MB | Speed: 707.76 samples/s\n",
      "Epoch 402/1000 | Train Loss: 0.1466 Acc: 0.9412 | Val Loss: 0.1954 Acc: 0.9070 | Mem: 792.88MB | Speed: 701.42 samples/s\n",
      "Epoch 403/1000 | Train Loss: 0.1667 Acc: 0.9365 | Val Loss: 0.1761 Acc: 0.9070 | Mem: 792.88MB | Speed: 709.48 samples/s\n",
      "Epoch 404/1000 | Train Loss: 0.1728 Acc: 0.9376 | Val Loss: 0.1617 Acc: 0.9070 | Mem: 792.88MB | Speed: 716.17 samples/s\n",
      "Epoch 405/1000 | Train Loss: 0.1683 Acc: 0.9365 | Val Loss: 0.1712 Acc: 0.9070 | Mem: 792.88MB | Speed: 717.52 samples/s\n",
      "Epoch 406/1000 | Train Loss: 0.1461 Acc: 0.9459 | Val Loss: 0.1797 Acc: 0.9186 | Mem: 792.88MB | Speed: 670.66 samples/s\n",
      "Epoch 407/1000 | Train Loss: 0.1500 Acc: 0.9400 | Val Loss: 0.1721 Acc: 0.9070 | Mem: 792.88MB | Speed: 732.94 samples/s\n",
      "Epoch 408/1000 | Train Loss: 0.1384 Acc: 0.9494 | Val Loss: 0.1702 Acc: 0.9070 | Mem: 792.88MB | Speed: 687.23 samples/s\n",
      "Epoch 409/1000 | Train Loss: 0.1570 Acc: 0.9365 | Val Loss: 0.1614 Acc: 0.8953 | Mem: 792.88MB | Speed: 646.70 samples/s\n",
      "Epoch 410/1000 | Train Loss: 0.1607 Acc: 0.9412 | Val Loss: 0.2063 Acc: 0.8837 | Mem: 792.88MB | Speed: 702.52 samples/s\n",
      "Epoch 411/1000 | Train Loss: 0.2048 Acc: 0.9212 | Val Loss: 0.2006 Acc: 0.8837 | Mem: 792.88MB | Speed: 778.24 samples/s\n",
      "Epoch 412/1000 | Train Loss: 0.1638 Acc: 0.9318 | Val Loss: 0.1808 Acc: 0.8953 | Mem: 792.88MB | Speed: 782.62 samples/s\n",
      "Epoch 413/1000 | Train Loss: 0.1550 Acc: 0.9400 | Val Loss: 0.1837 Acc: 0.8953 | Mem: 792.88MB | Speed: 742.39 samples/s\n",
      "Epoch 414/1000 | Train Loss: 0.1317 Acc: 0.9471 | Val Loss: 0.1865 Acc: 0.8953 | Mem: 792.88MB | Speed: 794.33 samples/s\n",
      "Epoch 415/1000 | Train Loss: 0.1676 Acc: 0.9306 | Val Loss: 0.2012 Acc: 0.8953 | Mem: 792.88MB | Speed: 773.93 samples/s\n",
      "Epoch 416/1000 | Train Loss: 0.1787 Acc: 0.9282 | Val Loss: 0.1937 Acc: 0.8953 | Mem: 792.88MB | Speed: 675.15 samples/s\n",
      "Epoch 417/1000 | Train Loss: 0.1906 Acc: 0.9200 | Val Loss: 0.1785 Acc: 0.9186 | Mem: 792.88MB | Speed: 655.47 samples/s\n",
      "Epoch 418/1000 | Train Loss: 0.1420 Acc: 0.9459 | Val Loss: 0.1782 Acc: 0.9070 | Mem: 792.88MB | Speed: 657.76 samples/s\n",
      "Epoch 419/1000 | Train Loss: 0.1529 Acc: 0.9294 | Val Loss: 0.1828 Acc: 0.8721 | Mem: 792.88MB | Speed: 716.58 samples/s\n",
      "Epoch 420/1000 | Train Loss: 0.1581 Acc: 0.9412 | Val Loss: 0.1998 Acc: 0.8953 | Mem: 792.88MB | Speed: 749.20 samples/s\n",
      "Epoch 421/1000 | Train Loss: 0.1527 Acc: 0.9341 | Val Loss: 0.2141 Acc: 0.8953 | Mem: 792.88MB | Speed: 697.73 samples/s\n",
      "Epoch 422/1000 | Train Loss: 0.1843 Acc: 0.9306 | Val Loss: 0.1966 Acc: 0.8837 | Mem: 792.88MB | Speed: 763.29 samples/s\n",
      "Epoch 423/1000 | Train Loss: 0.1296 Acc: 0.9529 | Val Loss: 0.1881 Acc: 0.8837 | Mem: 792.88MB | Speed: 703.37 samples/s\n",
      "Epoch 424/1000 | Train Loss: 0.1617 Acc: 0.9435 | Val Loss: 0.1806 Acc: 0.9070 | Mem: 792.88MB | Speed: 739.00 samples/s\n",
      "Epoch 425/1000 | Train Loss: 0.1815 Acc: 0.9282 | Val Loss: 0.1460 Acc: 0.9070 | Mem: 792.88MB | Speed: 701.38 samples/s\n",
      "Epoch 426/1000 | Train Loss: 0.1528 Acc: 0.9424 | Val Loss: 0.1373 Acc: 0.8953 | Mem: 792.88MB | Speed: 653.59 samples/s\n",
      "Epoch 427/1000 | Train Loss: 0.1656 Acc: 0.9365 | Val Loss: 0.1377 Acc: 0.8953 | Mem: 792.88MB | Speed: 654.42 samples/s\n",
      "Epoch 428/1000 | Train Loss: 0.1383 Acc: 0.9518 | Val Loss: 0.1407 Acc: 0.8953 | Mem: 792.88MB | Speed: 681.04 samples/s\n",
      "Epoch 429/1000 | Train Loss: 0.1502 Acc: 0.9353 | Val Loss: 0.1547 Acc: 0.9070 | Mem: 792.88MB | Speed: 670.64 samples/s\n",
      "Epoch 430/1000 | Train Loss: 0.1662 Acc: 0.9329 | Val Loss: 0.1467 Acc: 0.9186 | Mem: 792.88MB | Speed: 649.76 samples/s\n",
      "Epoch 431/1000 | Train Loss: 0.1548 Acc: 0.9447 | Val Loss: 0.1523 Acc: 0.9186 | Mem: 792.88MB | Speed: 655.68 samples/s\n",
      "Epoch 432/1000 | Train Loss: 0.1717 Acc: 0.9365 | Val Loss: 0.1821 Acc: 0.9070 | Mem: 792.88MB | Speed: 671.49 samples/s\n",
      "Epoch 433/1000 | Train Loss: 0.1489 Acc: 0.9400 | Val Loss: 0.2056 Acc: 0.8953 | Mem: 792.88MB | Speed: 672.80 samples/s\n",
      "Epoch 434/1000 | Train Loss: 0.1572 Acc: 0.9376 | Val Loss: 0.2095 Acc: 0.8837 | Mem: 792.88MB | Speed: 662.53 samples/s\n",
      "Epoch 435/1000 | Train Loss: 0.2015 Acc: 0.9188 | Val Loss: 0.1804 Acc: 0.8953 | Mem: 792.88MB | Speed: 725.49 samples/s\n",
      "Epoch 436/1000 | Train Loss: 0.1486 Acc: 0.9388 | Val Loss: 0.1645 Acc: 0.8953 | Mem: 792.88MB | Speed: 659.45 samples/s\n",
      "Epoch 437/1000 | Train Loss: 0.1579 Acc: 0.9471 | Val Loss: 0.1531 Acc: 0.9070 | Mem: 792.88MB | Speed: 693.65 samples/s\n",
      "Epoch 438/1000 | Train Loss: 0.1625 Acc: 0.9435 | Val Loss: 0.1670 Acc: 0.9186 | Mem: 792.88MB | Speed: 651.38 samples/s\n",
      "Epoch 439/1000 | Train Loss: 0.1621 Acc: 0.9329 | Val Loss: 0.1845 Acc: 0.9070 | Mem: 792.88MB | Speed: 659.04 samples/s\n",
      "Epoch 440/1000 | Train Loss: 0.1405 Acc: 0.9471 | Val Loss: 0.2212 Acc: 0.8721 | Mem: 792.88MB | Speed: 714.71 samples/s\n",
      "Epoch 441/1000 | Train Loss: 0.1547 Acc: 0.9400 | Val Loss: 0.2204 Acc: 0.8721 | Mem: 792.88MB | Speed: 751.74 samples/s\n",
      "Epoch 442/1000 | Train Loss: 0.1805 Acc: 0.9306 | Val Loss: 0.1878 Acc: 0.8953 | Mem: 792.88MB | Speed: 693.82 samples/s\n",
      "Epoch 443/1000 | Train Loss: 0.2040 Acc: 0.9259 | Val Loss: 0.1886 Acc: 0.9070 | Mem: 792.88MB | Speed: 652.32 samples/s\n",
      "Epoch 444/1000 | Train Loss: 0.1683 Acc: 0.9400 | Val Loss: 0.2013 Acc: 0.9070 | Mem: 792.88MB | Speed: 668.30 samples/s\n",
      "Epoch 445/1000 | Train Loss: 0.1434 Acc: 0.9471 | Val Loss: 0.2254 Acc: 0.8837 | Mem: 792.88MB | Speed: 713.55 samples/s\n",
      "Epoch 446/1000 | Train Loss: 0.1629 Acc: 0.9412 | Val Loss: 0.2129 Acc: 0.8837 | Mem: 792.88MB | Speed: 668.04 samples/s\n",
      "Epoch 447/1000 | Train Loss: 0.1910 Acc: 0.9235 | Val Loss: 0.1998 Acc: 0.8721 | Mem: 792.88MB | Speed: 699.91 samples/s\n",
      "Epoch 448/1000 | Train Loss: 0.1650 Acc: 0.9294 | Val Loss: 0.2116 Acc: 0.8837 | Mem: 792.88MB | Speed: 671.91 samples/s\n",
      "Epoch 449/1000 | Train Loss: 0.1422 Acc: 0.9447 | Val Loss: 0.2195 Acc: 0.8721 | Mem: 792.88MB | Speed: 651.75 samples/s\n",
      "Epoch 450/1000 | Train Loss: 0.1395 Acc: 0.9482 | Val Loss: 0.1899 Acc: 0.8953 | Mem: 792.88MB | Speed: 691.42 samples/s\n",
      "Epoch 451/1000 | Train Loss: 0.1200 Acc: 0.9565 | Val Loss: 0.1623 Acc: 0.9186 | Mem: 792.88MB | Speed: 672.70 samples/s\n",
      "Epoch 452/1000 | Train Loss: 0.1485 Acc: 0.9494 | Val Loss: 0.1441 Acc: 0.9186 | Mem: 792.88MB | Speed: 695.20 samples/s\n",
      "Epoch 453/1000 | Train Loss: 0.1820 Acc: 0.9294 | Val Loss: 0.1439 Acc: 0.9070 | Mem: 792.88MB | Speed: 660.86 samples/s\n",
      "Epoch 454/1000 | Train Loss: 0.1477 Acc: 0.9412 | Val Loss: 0.1664 Acc: 0.9070 | Mem: 792.88MB | Speed: 673.07 samples/s\n",
      "Epoch 455/1000 | Train Loss: 0.1391 Acc: 0.9471 | Val Loss: 0.1862 Acc: 0.9070 | Mem: 792.88MB | Speed: 662.98 samples/s\n",
      "Epoch 456/1000 | Train Loss: 0.1345 Acc: 0.9482 | Val Loss: 0.1680 Acc: 0.9186 | Mem: 792.88MB | Speed: 645.77 samples/s\n",
      "Epoch 457/1000 | Train Loss: 0.1630 Acc: 0.9412 | Val Loss: 0.1457 Acc: 0.9070 | Mem: 792.88MB | Speed: 651.85 samples/s\n",
      "Epoch 458/1000 | Train Loss: 0.1627 Acc: 0.9365 | Val Loss: 0.1459 Acc: 0.9186 | Mem: 792.88MB | Speed: 714.33 samples/s\n",
      "Epoch 459/1000 | Train Loss: 0.1468 Acc: 0.9412 | Val Loss: 0.1491 Acc: 0.9186 | Mem: 792.88MB | Speed: 700.37 samples/s\n",
      "Epoch 460/1000 | Train Loss: 0.1762 Acc: 0.9318 | Val Loss: 0.1500 Acc: 0.8953 | Mem: 792.88MB | Speed: 699.63 samples/s\n",
      "Epoch 461/1000 | Train Loss: 0.1613 Acc: 0.9376 | Val Loss: 0.1505 Acc: 0.9070 | Mem: 792.88MB | Speed: 707.08 samples/s\n",
      "Epoch 462/1000 | Train Loss: 0.1827 Acc: 0.9329 | Val Loss: 0.1399 Acc: 0.9302 | Mem: 792.88MB | Speed: 662.12 samples/s\n",
      "Epoch 463/1000 | Train Loss: 0.1675 Acc: 0.9341 | Val Loss: 0.1388 Acc: 0.9186 | Mem: 792.88MB | Speed: 646.48 samples/s\n",
      "Epoch 464/1000 | Train Loss: 0.1357 Acc: 0.9471 | Val Loss: 0.1414 Acc: 0.9070 | Mem: 792.88MB | Speed: 678.50 samples/s\n",
      "Epoch 465/1000 | Train Loss: 0.1927 Acc: 0.9224 | Val Loss: 0.1241 Acc: 0.9186 | Mem: 792.88MB | Speed: 672.27 samples/s\n",
      "Epoch 466/1000 | Train Loss: 0.1603 Acc: 0.9424 | Val Loss: 0.1491 Acc: 0.9186 | Mem: 792.88MB | Speed: 708.58 samples/s\n",
      "Epoch 467/1000 | Train Loss: 0.1593 Acc: 0.9400 | Val Loss: 0.1553 Acc: 0.9070 | Mem: 792.88MB | Speed: 735.82 samples/s\n",
      "Epoch 468/1000 | Train Loss: 0.1417 Acc: 0.9424 | Val Loss: 0.1692 Acc: 0.9070 | Mem: 792.88MB | Speed: 705.48 samples/s\n",
      "Epoch 469/1000 | Train Loss: 0.1507 Acc: 0.9424 | Val Loss: 0.1697 Acc: 0.9070 | Mem: 792.88MB | Speed: 651.41 samples/s\n",
      "Epoch 470/1000 | Train Loss: 0.1538 Acc: 0.9518 | Val Loss: 0.1572 Acc: 0.8953 | Mem: 792.88MB | Speed: 655.02 samples/s\n",
      "Epoch 471/1000 | Train Loss: 0.1530 Acc: 0.9471 | Val Loss: 0.1326 Acc: 0.9070 | Mem: 792.88MB | Speed: 692.00 samples/s\n",
      "Epoch 472/1000 | Train Loss: 0.1465 Acc: 0.9388 | Val Loss: 0.1310 Acc: 0.9186 | Mem: 792.88MB | Speed: 672.81 samples/s\n",
      "Epoch 473/1000 | Train Loss: 0.1696 Acc: 0.9259 | Val Loss: 0.1389 Acc: 0.9070 | Mem: 792.88MB | Speed: 716.20 samples/s\n",
      "Epoch 474/1000 | Train Loss: 0.1891 Acc: 0.9318 | Val Loss: 0.1420 Acc: 0.9070 | Mem: 792.88MB | Speed: 636.61 samples/s\n",
      "Epoch 475/1000 | Train Loss: 0.1463 Acc: 0.9459 | Val Loss: 0.1421 Acc: 0.9070 | Mem: 792.88MB | Speed: 707.87 samples/s\n",
      "Epoch 476/1000 | Train Loss: 0.1545 Acc: 0.9435 | Val Loss: 0.1394 Acc: 0.9186 | Mem: 792.88MB | Speed: 737.61 samples/s\n",
      "Epoch 477/1000 | Train Loss: 0.1565 Acc: 0.9376 | Val Loss: 0.1328 Acc: 0.9302 | Mem: 792.88MB | Speed: 1154.23 samples/s\n",
      "Epoch 478/1000 | Train Loss: 0.1676 Acc: 0.9388 | Val Loss: 0.1215 Acc: 0.9419 | Mem: 792.88MB | Speed: 1164.47 samples/s\n",
      "Epoch 479/1000 | Train Loss: 0.1663 Acc: 0.9282 | Val Loss: 0.1262 Acc: 0.9186 | Mem: 792.88MB | Speed: 1181.13 samples/s\n",
      "Epoch 480/1000 | Train Loss: 0.1671 Acc: 0.9376 | Val Loss: 0.1367 Acc: 0.9070 | Mem: 792.88MB | Speed: 1187.88 samples/s\n",
      "Epoch 481/1000 | Train Loss: 0.1420 Acc: 0.9447 | Val Loss: 0.1621 Acc: 0.8953 | Mem: 792.88MB | Speed: 1170.83 samples/s\n",
      "Epoch 482/1000 | Train Loss: 0.1436 Acc: 0.9435 | Val Loss: 0.1703 Acc: 0.8953 | Mem: 792.88MB | Speed: 1180.64 samples/s\n",
      "Epoch 483/1000 | Train Loss: 0.1486 Acc: 0.9447 | Val Loss: 0.1621 Acc: 0.9186 | Mem: 792.88MB | Speed: 1179.66 samples/s\n",
      "Epoch 484/1000 | Train Loss: 0.1314 Acc: 0.9529 | Val Loss: 0.1571 Acc: 0.9186 | Mem: 792.88MB | Speed: 1159.47 samples/s\n",
      "Epoch 485/1000 | Train Loss: 0.1530 Acc: 0.9435 | Val Loss: 0.1630 Acc: 0.9186 | Mem: 792.88MB | Speed: 1174.78 samples/s\n",
      "Epoch 486/1000 | Train Loss: 0.1468 Acc: 0.9424 | Val Loss: 0.1635 Acc: 0.9070 | Mem: 792.88MB | Speed: 1173.70 samples/s\n",
      "Epoch 487/1000 | Train Loss: 0.1470 Acc: 0.9424 | Val Loss: 0.1478 Acc: 0.9070 | Mem: 792.88MB | Speed: 1175.65 samples/s\n",
      "Epoch 488/1000 | Train Loss: 0.1892 Acc: 0.9329 | Val Loss: 0.1260 Acc: 0.9186 | Mem: 792.88MB | Speed: 1159.35 samples/s\n",
      "Epoch 489/1000 | Train Loss: 0.1575 Acc: 0.9424 | Val Loss: 0.1132 Acc: 0.9302 | Mem: 792.88MB | Speed: 1165.29 samples/s\n",
      "Epoch 490/1000 | Train Loss: 0.1450 Acc: 0.9494 | Val Loss: 0.1092 Acc: 0.9419 | Mem: 792.88MB | Speed: 1173.34 samples/s\n",
      "Epoch 491/1000 | Train Loss: 0.1859 Acc: 0.9271 | Val Loss: 0.1168 Acc: 0.9302 | Mem: 792.88MB | Speed: 1123.95 samples/s\n",
      "Epoch 492/1000 | Train Loss: 0.1516 Acc: 0.9376 | Val Loss: 0.1224 Acc: 0.9186 | Mem: 792.88MB | Speed: 1140.33 samples/s\n",
      "Epoch 493/1000 | Train Loss: 0.1429 Acc: 0.9435 | Val Loss: 0.1297 Acc: 0.9186 | Mem: 792.88MB | Speed: 1169.19 samples/s\n",
      "Epoch 494/1000 | Train Loss: 0.1532 Acc: 0.9400 | Val Loss: 0.1383 Acc: 0.9186 | Mem: 792.88MB | Speed: 1167.85 samples/s\n",
      "Epoch 495/1000 | Train Loss: 0.1274 Acc: 0.9576 | Val Loss: 0.1372 Acc: 0.9070 | Mem: 792.88MB | Speed: 1210.03 samples/s\n",
      "Epoch 496/1000 | Train Loss: 0.1146 Acc: 0.9600 | Val Loss: 0.1376 Acc: 0.9070 | Mem: 792.88MB | Speed: 1184.04 samples/s\n",
      "Epoch 497/1000 | Train Loss: 0.1461 Acc: 0.9424 | Val Loss: 0.1332 Acc: 0.9070 | Mem: 792.88MB | Speed: 1174.79 samples/s\n",
      "Epoch 498/1000 | Train Loss: 0.1615 Acc: 0.9329 | Val Loss: 0.1163 Acc: 0.9302 | Mem: 792.88MB | Speed: 1177.35 samples/s\n",
      "Epoch 499/1000 | Train Loss: 0.1636 Acc: 0.9388 | Val Loss: 0.1259 Acc: 0.9302 | Mem: 792.88MB | Speed: 1162.82 samples/s\n",
      "Epoch 500/1000 | Train Loss: 0.1406 Acc: 0.9424 | Val Loss: 0.1347 Acc: 0.9186 | Mem: 792.88MB | Speed: 1186.36 samples/s\n",
      "Epoch 501/1000 | Train Loss: 0.1236 Acc: 0.9518 | Val Loss: 0.1565 Acc: 0.9186 | Mem: 792.88MB | Speed: 1170.84 samples/s\n",
      "Epoch 502/1000 | Train Loss: 0.1679 Acc: 0.9294 | Val Loss: 0.1782 Acc: 0.9070 | Mem: 792.88MB | Speed: 1164.50 samples/s\n",
      "Epoch 503/1000 | Train Loss: 0.1738 Acc: 0.9329 | Val Loss: 0.1704 Acc: 0.9070 | Mem: 792.88MB | Speed: 1171.85 samples/s\n",
      "Epoch 504/1000 | Train Loss: 0.1424 Acc: 0.9459 | Val Loss: 0.1479 Acc: 0.9070 | Mem: 792.88MB | Speed: 1165.47 samples/s\n",
      "Epoch 505/1000 | Train Loss: 0.1197 Acc: 0.9624 | Val Loss: 0.1196 Acc: 0.9186 | Mem: 792.88MB | Speed: 1177.23 samples/s\n",
      "Epoch 506/1000 | Train Loss: 0.1273 Acc: 0.9471 | Val Loss: 0.1141 Acc: 0.9302 | Mem: 792.88MB | Speed: 1188.87 samples/s\n",
      "Epoch 507/1000 | Train Loss: 0.1548 Acc: 0.9424 | Val Loss: 0.1313 Acc: 0.9186 | Mem: 792.88MB | Speed: 1168.82 samples/s\n",
      "Epoch 508/1000 | Train Loss: 0.1140 Acc: 0.9624 | Val Loss: 0.1608 Acc: 0.9070 | Mem: 792.88MB | Speed: 1166.26 samples/s\n",
      "Epoch 509/1000 | Train Loss: 0.1352 Acc: 0.9553 | Val Loss: 0.1557 Acc: 0.9070 | Mem: 792.88MB | Speed: 1161.40 samples/s\n",
      "Epoch 510/1000 | Train Loss: 0.1538 Acc: 0.9459 | Val Loss: 0.1245 Acc: 0.9186 | Mem: 792.88MB | Speed: 1168.71 samples/s\n",
      "Epoch 511/1000 | Train Loss: 0.1673 Acc: 0.9318 | Val Loss: 0.1147 Acc: 0.9186 | Mem: 792.88MB | Speed: 1185.29 samples/s\n",
      "Epoch 512/1000 | Train Loss: 0.1364 Acc: 0.9506 | Val Loss: 0.1297 Acc: 0.9186 | Mem: 792.88MB | Speed: 1163.98 samples/s\n",
      "Epoch 513/1000 | Train Loss: 0.1322 Acc: 0.9553 | Val Loss: 0.1471 Acc: 0.8953 | Mem: 792.88MB | Speed: 1192.60 samples/s\n",
      "Epoch 514/1000 | Train Loss: 0.1746 Acc: 0.9353 | Val Loss: 0.1303 Acc: 0.9186 | Mem: 792.88MB | Speed: 1179.99 samples/s\n",
      "Epoch 515/1000 | Train Loss: 0.1340 Acc: 0.9494 | Val Loss: 0.1168 Acc: 0.9070 | Mem: 792.88MB | Speed: 1181.08 samples/s\n",
      "Epoch 516/1000 | Train Loss: 0.1356 Acc: 0.9553 | Val Loss: 0.1215 Acc: 0.8953 | Mem: 792.88MB | Speed: 1171.27 samples/s\n",
      "Epoch 517/1000 | Train Loss: 0.1437 Acc: 0.9471 | Val Loss: 0.1346 Acc: 0.9070 | Mem: 792.88MB | Speed: 1176.06 samples/s\n",
      "Epoch 518/1000 | Train Loss: 0.1442 Acc: 0.9494 | Val Loss: 0.1548 Acc: 0.9186 | Mem: 792.88MB | Speed: 1205.60 samples/s\n",
      "Epoch 519/1000 | Train Loss: 0.1485 Acc: 0.9471 | Val Loss: 0.1687 Acc: 0.8837 | Mem: 792.88MB | Speed: 1166.48 samples/s\n",
      "Epoch 520/1000 | Train Loss: 0.1858 Acc: 0.9318 | Val Loss: 0.1365 Acc: 0.9070 | Mem: 792.88MB | Speed: 1193.78 samples/s\n",
      "Epoch 521/1000 | Train Loss: 0.1372 Acc: 0.9518 | Val Loss: 0.1160 Acc: 0.9302 | Mem: 792.88MB | Speed: 1190.88 samples/s\n",
      "Epoch 522/1000 | Train Loss: 0.1313 Acc: 0.9447 | Val Loss: 0.1274 Acc: 0.9186 | Mem: 792.88MB | Speed: 1206.55 samples/s\n",
      "Epoch 523/1000 | Train Loss: 0.2227 Acc: 0.9200 | Val Loss: 0.1676 Acc: 0.8953 | Mem: 792.88MB | Speed: 1177.30 samples/s\n",
      "Epoch 524/1000 | Train Loss: 0.1659 Acc: 0.9400 | Val Loss: 0.1557 Acc: 0.8837 | Mem: 792.88MB | Speed: 1187.01 samples/s\n",
      "Epoch 525/1000 | Train Loss: 0.1206 Acc: 0.9553 | Val Loss: 0.1269 Acc: 0.9070 | Mem: 792.88MB | Speed: 1164.16 samples/s\n",
      "Epoch 526/1000 | Train Loss: 0.1425 Acc: 0.9459 | Val Loss: 0.1454 Acc: 0.9070 | Mem: 792.88MB | Speed: 1174.02 samples/s\n",
      "Epoch 527/1000 | Train Loss: 0.1489 Acc: 0.9365 | Val Loss: 0.1853 Acc: 0.8721 | Mem: 792.88MB | Speed: 1172.07 samples/s\n",
      "Epoch 528/1000 | Train Loss: 0.1666 Acc: 0.9365 | Val Loss: 0.2100 Acc: 0.8488 | Mem: 792.88MB | Speed: 1178.00 samples/s\n",
      "Epoch 529/1000 | Train Loss: 0.1625 Acc: 0.9388 | Val Loss: 0.2174 Acc: 0.8488 | Mem: 792.88MB | Speed: 1175.70 samples/s\n",
      "Epoch 530/1000 | Train Loss: 0.1643 Acc: 0.9376 | Val Loss: 0.1802 Acc: 0.8953 | Mem: 792.88MB | Speed: 1167.14 samples/s\n",
      "Epoch 531/1000 | Train Loss: 0.1606 Acc: 0.9482 | Val Loss: 0.1570 Acc: 0.8953 | Mem: 792.88MB | Speed: 1184.00 samples/s\n",
      "Epoch 532/1000 | Train Loss: 0.1328 Acc: 0.9494 | Val Loss: 0.1458 Acc: 0.9186 | Mem: 792.88MB | Speed: 1161.34 samples/s\n",
      "Epoch 533/1000 | Train Loss: 0.1126 Acc: 0.9600 | Val Loss: 0.1479 Acc: 0.9186 | Mem: 792.88MB | Speed: 1164.22 samples/s\n",
      "Epoch 534/1000 | Train Loss: 0.1407 Acc: 0.9482 | Val Loss: 0.1393 Acc: 0.9186 | Mem: 792.88MB | Speed: 1160.71 samples/s\n",
      "Epoch 535/1000 | Train Loss: 0.1756 Acc: 0.9306 | Val Loss: 0.1665 Acc: 0.8953 | Mem: 792.88MB | Speed: 1143.22 samples/s\n",
      "Epoch 536/1000 | Train Loss: 0.1343 Acc: 0.9482 | Val Loss: 0.2299 Acc: 0.8837 | Mem: 792.88MB | Speed: 1167.97 samples/s\n",
      "Epoch 537/1000 | Train Loss: 0.1435 Acc: 0.9447 | Val Loss: 0.2392 Acc: 0.8837 | Mem: 792.88MB | Speed: 1166.80 samples/s\n",
      "Epoch 538/1000 | Train Loss: 0.1416 Acc: 0.9506 | Val Loss: 0.2130 Acc: 0.9070 | Mem: 792.88MB | Speed: 1160.92 samples/s\n",
      "Epoch 539/1000 | Train Loss: 0.1541 Acc: 0.9412 | Val Loss: 0.1567 Acc: 0.9070 | Mem: 792.88MB | Speed: 1165.97 samples/s\n",
      "Epoch 540/1000 | Train Loss: 0.1464 Acc: 0.9365 | Val Loss: 0.1493 Acc: 0.9186 | Mem: 792.88MB | Speed: 1178.41 samples/s\n",
      "Epoch 541/1000 | Train Loss: 0.1417 Acc: 0.9447 | Val Loss: 0.1724 Acc: 0.9070 | Mem: 792.88MB | Speed: 1183.24 samples/s\n",
      "Epoch 542/1000 | Train Loss: 0.1584 Acc: 0.9494 | Val Loss: 0.1720 Acc: 0.9070 | Mem: 792.88MB | Speed: 1197.40 samples/s\n",
      "Epoch 543/1000 | Train Loss: 0.1270 Acc: 0.9424 | Val Loss: 0.1742 Acc: 0.9186 | Mem: 792.88MB | Speed: 1178.98 samples/s\n",
      "Epoch 544/1000 | Train Loss: 0.1541 Acc: 0.9341 | Val Loss: 0.1732 Acc: 0.9186 | Mem: 792.88MB | Speed: 1188.03 samples/s\n",
      "Epoch 545/1000 | Train Loss: 0.1408 Acc: 0.9435 | Val Loss: 0.1454 Acc: 0.9070 | Mem: 792.88MB | Speed: 1169.39 samples/s\n",
      "Epoch 546/1000 | Train Loss: 0.1298 Acc: 0.9518 | Val Loss: 0.1488 Acc: 0.9070 | Mem: 792.88MB | Speed: 1190.53 samples/s\n",
      "Epoch 547/1000 | Train Loss: 0.1335 Acc: 0.9447 | Val Loss: 0.1767 Acc: 0.8953 | Mem: 792.88MB | Speed: 1165.09 samples/s\n",
      "Epoch 548/1000 | Train Loss: 0.1641 Acc: 0.9400 | Val Loss: 0.1499 Acc: 0.8953 | Mem: 792.88MB | Speed: 1202.12 samples/s\n",
      "Epoch 549/1000 | Train Loss: 0.1359 Acc: 0.9424 | Val Loss: 0.1353 Acc: 0.9070 | Mem: 792.88MB | Speed: 1159.70 samples/s\n",
      "Epoch 550/1000 | Train Loss: 0.1459 Acc: 0.9482 | Val Loss: 0.1318 Acc: 0.9186 | Mem: 792.88MB | Speed: 1191.18 samples/s\n",
      "Epoch 551/1000 | Train Loss: 0.1140 Acc: 0.9588 | Val Loss: 0.1370 Acc: 0.9186 | Mem: 792.88MB | Speed: 1190.14 samples/s\n",
      "Epoch 552/1000 | Train Loss: 0.1137 Acc: 0.9588 | Val Loss: 0.1456 Acc: 0.8953 | Mem: 792.88MB | Speed: 1184.47 samples/s\n",
      "Epoch 553/1000 | Train Loss: 0.1199 Acc: 0.9529 | Val Loss: 0.1477 Acc: 0.8953 | Mem: 792.88MB | Speed: 1186.84 samples/s\n",
      "Epoch 554/1000 | Train Loss: 0.1519 Acc: 0.9424 | Val Loss: 0.1419 Acc: 0.8953 | Mem: 792.88MB | Speed: 1158.25 samples/s\n",
      "Epoch 555/1000 | Train Loss: 0.1227 Acc: 0.9576 | Val Loss: 0.1505 Acc: 0.8953 | Mem: 792.88MB | Speed: 1165.46 samples/s\n",
      "Epoch 556/1000 | Train Loss: 0.1298 Acc: 0.9553 | Val Loss: 0.1598 Acc: 0.8953 | Mem: 792.88MB | Speed: 1191.12 samples/s\n",
      "Epoch 557/1000 | Train Loss: 0.1433 Acc: 0.9412 | Val Loss: 0.1581 Acc: 0.9070 | Mem: 792.88MB | Speed: 1185.27 samples/s\n",
      "Epoch 558/1000 | Train Loss: 0.1391 Acc: 0.9376 | Val Loss: 0.1609 Acc: 0.9070 | Mem: 792.88MB | Speed: 1164.02 samples/s\n",
      "Epoch 559/1000 | Train Loss: 0.1564 Acc: 0.9482 | Val Loss: 0.1739 Acc: 0.9070 | Mem: 792.88MB | Speed: 1168.98 samples/s\n",
      "Epoch 560/1000 | Train Loss: 0.1573 Acc: 0.9318 | Val Loss: 0.2270 Acc: 0.8837 | Mem: 792.88MB | Speed: 1165.58 samples/s\n",
      "Epoch 561/1000 | Train Loss: 0.1391 Acc: 0.9565 | Val Loss: 0.2401 Acc: 0.8837 | Mem: 792.88MB | Speed: 1177.82 samples/s\n",
      "Epoch 562/1000 | Train Loss: 0.1793 Acc: 0.9341 | Val Loss: 0.1992 Acc: 0.8953 | Mem: 792.88MB | Speed: 1166.47 samples/s\n",
      "Epoch 563/1000 | Train Loss: 0.1423 Acc: 0.9518 | Val Loss: 0.1887 Acc: 0.8953 | Mem: 792.88MB | Speed: 1176.41 samples/s\n",
      "Epoch 564/1000 | Train Loss: 0.1603 Acc: 0.9376 | Val Loss: 0.1962 Acc: 0.8837 | Mem: 792.88MB | Speed: 1165.65 samples/s\n",
      "Epoch 565/1000 | Train Loss: 0.1619 Acc: 0.9388 | Val Loss: 0.1829 Acc: 0.8953 | Mem: 792.88MB | Speed: 1191.45 samples/s\n",
      "Epoch 566/1000 | Train Loss: 0.1222 Acc: 0.9494 | Val Loss: 0.1429 Acc: 0.9186 | Mem: 792.88MB | Speed: 1172.89 samples/s\n",
      "Epoch 567/1000 | Train Loss: 0.1465 Acc: 0.9471 | Val Loss: 0.1313 Acc: 0.9186 | Mem: 792.88MB | Speed: 1152.58 samples/s\n",
      "Epoch 568/1000 | Train Loss: 0.1481 Acc: 0.9447 | Val Loss: 0.1347 Acc: 0.9302 | Mem: 792.88MB | Speed: 1162.43 samples/s\n",
      "Epoch 569/1000 | Train Loss: 0.1586 Acc: 0.9376 | Val Loss: 0.1558 Acc: 0.9070 | Mem: 792.88MB | Speed: 1170.46 samples/s\n",
      "Epoch 570/1000 | Train Loss: 0.1259 Acc: 0.9518 | Val Loss: 0.1854 Acc: 0.9070 | Mem: 792.88MB | Speed: 1170.48 samples/s\n",
      "Epoch 571/1000 | Train Loss: 0.1247 Acc: 0.9506 | Val Loss: 0.1888 Acc: 0.9070 | Mem: 792.88MB | Speed: 1173.65 samples/s\n",
      "Epoch 572/1000 | Train Loss: 0.1554 Acc: 0.9435 | Val Loss: 0.1610 Acc: 0.9070 | Mem: 792.88MB | Speed: 1198.01 samples/s\n",
      "Epoch 573/1000 | Train Loss: 0.1671 Acc: 0.9388 | Val Loss: 0.1508 Acc: 0.8953 | Mem: 792.88MB | Speed: 1164.25 samples/s\n",
      "Epoch 574/1000 | Train Loss: 0.1737 Acc: 0.9329 | Val Loss: 0.1526 Acc: 0.9070 | Mem: 792.88MB | Speed: 1155.59 samples/s\n",
      "Epoch 575/1000 | Train Loss: 0.1520 Acc: 0.9447 | Val Loss: 0.1558 Acc: 0.9070 | Mem: 792.88MB | Speed: 1164.47 samples/s\n",
      "Epoch 576/1000 | Train Loss: 0.1704 Acc: 0.9306 | Val Loss: 0.1459 Acc: 0.9302 | Mem: 792.88MB | Speed: 1152.11 samples/s\n",
      "Epoch 577/1000 | Train Loss: 0.1354 Acc: 0.9435 | Val Loss: 0.1499 Acc: 0.9186 | Mem: 792.88MB | Speed: 1170.01 samples/s\n",
      "Epoch 578/1000 | Train Loss: 0.1542 Acc: 0.9471 | Val Loss: 0.1619 Acc: 0.9070 | Mem: 792.88MB | Speed: 1177.41 samples/s\n",
      "Epoch 579/1000 | Train Loss: 0.1535 Acc: 0.9459 | Val Loss: 0.1387 Acc: 0.8953 | Mem: 792.88MB | Speed: 1178.01 samples/s\n",
      "Epoch 580/1000 | Train Loss: 0.1328 Acc: 0.9541 | Val Loss: 0.1352 Acc: 0.9186 | Mem: 792.88MB | Speed: 1167.34 samples/s\n",
      "Epoch 581/1000 | Train Loss: 0.1625 Acc: 0.9318 | Val Loss: 0.1407 Acc: 0.9186 | Mem: 792.88MB | Speed: 1157.80 samples/s\n",
      "Epoch 582/1000 | Train Loss: 0.1369 Acc: 0.9482 | Val Loss: 0.1582 Acc: 0.9070 | Mem: 792.88MB | Speed: 1189.14 samples/s\n",
      "Epoch 583/1000 | Train Loss: 0.1295 Acc: 0.9518 | Val Loss: 0.1880 Acc: 0.9070 | Mem: 792.88MB | Speed: 1172.08 samples/s\n",
      "Epoch 584/1000 | Train Loss: 0.1482 Acc: 0.9471 | Val Loss: 0.1949 Acc: 0.8953 | Mem: 792.88MB | Speed: 1156.77 samples/s\n",
      "Epoch 585/1000 | Train Loss: 0.1350 Acc: 0.9506 | Val Loss: 0.1669 Acc: 0.9070 | Mem: 792.88MB | Speed: 1169.03 samples/s\n",
      "Epoch 586/1000 | Train Loss: 0.1678 Acc: 0.9341 | Val Loss: 0.1481 Acc: 0.9186 | Mem: 792.88MB | Speed: 1157.44 samples/s\n",
      "Epoch 587/1000 | Train Loss: 0.1388 Acc: 0.9471 | Val Loss: 0.1443 Acc: 0.9070 | Mem: 792.88MB | Speed: 1191.79 samples/s\n",
      "Epoch 588/1000 | Train Loss: 0.1612 Acc: 0.9424 | Val Loss: 0.1615 Acc: 0.9070 | Mem: 792.88MB | Speed: 1144.64 samples/s\n",
      "Epoch 589/1000 | Train Loss: 0.1254 Acc: 0.9612 | Val Loss: 0.1609 Acc: 0.9070 | Mem: 792.88MB | Speed: 1160.14 samples/s\n",
      "Epoch 590/1000 | Train Loss: 0.1475 Acc: 0.9400 | Val Loss: 0.1584 Acc: 0.9186 | Mem: 792.88MB | Speed: 1183.11 samples/s\n",
      "Epoch 591/1000 | Train Loss: 0.1559 Acc: 0.9459 | Val Loss: 0.1628 Acc: 0.9186 | Mem: 792.88MB | Speed: 1187.26 samples/s\n",
      "Epoch 592/1000 | Train Loss: 0.1236 Acc: 0.9553 | Val Loss: 0.1539 Acc: 0.9186 | Mem: 792.88MB | Speed: 1171.92 samples/s\n",
      "Epoch 593/1000 | Train Loss: 0.1256 Acc: 0.9482 | Val Loss: 0.1354 Acc: 0.9186 | Mem: 792.88MB | Speed: 1170.99 samples/s\n",
      "Epoch 594/1000 | Train Loss: 0.1429 Acc: 0.9400 | Val Loss: 0.1415 Acc: 0.9186 | Mem: 792.88MB | Speed: 1156.93 samples/s\n",
      "Epoch 595/1000 | Train Loss: 0.1242 Acc: 0.9600 | Val Loss: 0.1440 Acc: 0.9186 | Mem: 792.88MB | Speed: 1194.01 samples/s\n",
      "Epoch 596/1000 | Train Loss: 0.1777 Acc: 0.9318 | Val Loss: 0.1321 Acc: 0.9186 | Mem: 792.88MB | Speed: 1183.23 samples/s\n",
      "Epoch 597/1000 | Train Loss: 0.1483 Acc: 0.9471 | Val Loss: 0.1167 Acc: 0.9302 | Mem: 792.88MB | Speed: 1181.01 samples/s\n",
      "Epoch 598/1000 | Train Loss: 0.1177 Acc: 0.9506 | Val Loss: 0.1165 Acc: 0.9419 | Mem: 792.88MB | Speed: 1172.13 samples/s\n",
      "Epoch 599/1000 | Train Loss: 0.1385 Acc: 0.9518 | Val Loss: 0.1231 Acc: 0.9419 | Mem: 792.88MB | Speed: 1184.05 samples/s\n",
      "Epoch 600/1000 | Train Loss: 0.1398 Acc: 0.9518 | Val Loss: 0.1398 Acc: 0.9070 | Mem: 792.88MB | Speed: 1164.20 samples/s\n",
      "Epoch 601/1000 | Train Loss: 0.1355 Acc: 0.9459 | Val Loss: 0.1574 Acc: 0.8953 | Mem: 792.88MB | Speed: 1168.56 samples/s\n",
      "Epoch 602/1000 | Train Loss: 0.1218 Acc: 0.9529 | Val Loss: 0.1735 Acc: 0.8953 | Mem: 792.88MB | Speed: 1161.38 samples/s\n",
      "Epoch 603/1000 | Train Loss: 0.1197 Acc: 0.9624 | Val Loss: 0.1831 Acc: 0.8837 | Mem: 792.88MB | Speed: 1168.64 samples/s\n",
      "Epoch 604/1000 | Train Loss: 0.1528 Acc: 0.9400 | Val Loss: 0.1679 Acc: 0.8837 | Mem: 792.88MB | Speed: 1187.62 samples/s\n",
      "Epoch 605/1000 | Train Loss: 0.1092 Acc: 0.9588 | Val Loss: 0.1497 Acc: 0.8953 | Mem: 792.88MB | Speed: 1163.48 samples/s\n",
      "Epoch 606/1000 | Train Loss: 0.1487 Acc: 0.9435 | Val Loss: 0.1347 Acc: 0.9302 | Mem: 792.88MB | Speed: 1153.82 samples/s\n",
      "Epoch 607/1000 | Train Loss: 0.1092 Acc: 0.9624 | Val Loss: 0.1364 Acc: 0.9302 | Mem: 792.88MB | Speed: 1152.58 samples/s\n",
      "Epoch 608/1000 | Train Loss: 0.1388 Acc: 0.9541 | Val Loss: 0.1468 Acc: 0.9302 | Mem: 792.88MB | Speed: 1157.50 samples/s\n",
      "Epoch 609/1000 | Train Loss: 0.1438 Acc: 0.9412 | Val Loss: 0.1541 Acc: 0.9186 | Mem: 792.88MB | Speed: 1162.69 samples/s\n",
      "Epoch 610/1000 | Train Loss: 0.1141 Acc: 0.9518 | Val Loss: 0.1620 Acc: 0.9070 | Mem: 792.88MB | Speed: 1161.91 samples/s\n",
      "Epoch 611/1000 | Train Loss: 0.1314 Acc: 0.9588 | Val Loss: 0.1603 Acc: 0.9070 | Mem: 792.88MB | Speed: 1153.68 samples/s\n",
      "Epoch 612/1000 | Train Loss: 0.1445 Acc: 0.9435 | Val Loss: 0.1469 Acc: 0.9186 | Mem: 792.88MB | Speed: 1162.21 samples/s\n",
      "Epoch 613/1000 | Train Loss: 0.1200 Acc: 0.9624 | Val Loss: 0.1301 Acc: 0.9302 | Mem: 792.88MB | Speed: 1152.50 samples/s\n",
      "Epoch 614/1000 | Train Loss: 0.1151 Acc: 0.9576 | Val Loss: 0.1277 Acc: 0.9186 | Mem: 792.88MB | Speed: 1165.00 samples/s\n",
      "Epoch 615/1000 | Train Loss: 0.1376 Acc: 0.9471 | Val Loss: 0.1309 Acc: 0.9186 | Mem: 792.88MB | Speed: 1155.03 samples/s\n",
      "Epoch 616/1000 | Train Loss: 0.1372 Acc: 0.9482 | Val Loss: 0.1297 Acc: 0.9186 | Mem: 792.88MB | Speed: 1139.73 samples/s\n",
      "Epoch 617/1000 | Train Loss: 0.1302 Acc: 0.9494 | Val Loss: 0.1299 Acc: 0.9186 | Mem: 792.88MB | Speed: 1155.56 samples/s\n",
      "Epoch 618/1000 | Train Loss: 0.1299 Acc: 0.9482 | Val Loss: 0.1317 Acc: 0.9070 | Mem: 792.88MB | Speed: 1171.12 samples/s\n",
      "Epoch 619/1000 | Train Loss: 0.1415 Acc: 0.9506 | Val Loss: 0.1282 Acc: 0.9186 | Mem: 792.88MB | Speed: 1192.64 samples/s\n",
      "Epoch 620/1000 | Train Loss: 0.1596 Acc: 0.9412 | Val Loss: 0.1270 Acc: 0.9302 | Mem: 792.88MB | Speed: 1180.74 samples/s\n",
      "Epoch 621/1000 | Train Loss: 0.1193 Acc: 0.9588 | Val Loss: 0.1398 Acc: 0.9302 | Mem: 792.88MB | Speed: 1178.90 samples/s\n",
      "Epoch 622/1000 | Train Loss: 0.1272 Acc: 0.9600 | Val Loss: 0.1880 Acc: 0.9070 | Mem: 792.88MB | Speed: 1190.64 samples/s\n",
      "Epoch 623/1000 | Train Loss: 0.1431 Acc: 0.9494 | Val Loss: 0.2109 Acc: 0.8953 | Mem: 792.88MB | Speed: 1167.53 samples/s\n",
      "Epoch 624/1000 | Train Loss: 0.1394 Acc: 0.9482 | Val Loss: 0.1698 Acc: 0.9070 | Mem: 792.88MB | Speed: 1151.92 samples/s\n",
      "Epoch 625/1000 | Train Loss: 0.1196 Acc: 0.9529 | Val Loss: 0.1333 Acc: 0.9302 | Mem: 792.88MB | Speed: 1154.27 samples/s\n",
      "Epoch 626/1000 | Train Loss: 0.1315 Acc: 0.9447 | Val Loss: 0.1330 Acc: 0.9186 | Mem: 792.88MB | Speed: 1162.58 samples/s\n",
      "Epoch 627/1000 | Train Loss: 0.1466 Acc: 0.9506 | Val Loss: 0.1258 Acc: 0.9302 | Mem: 792.88MB | Speed: 1166.33 samples/s\n",
      "Epoch 628/1000 | Train Loss: 0.1137 Acc: 0.9553 | Val Loss: 0.1271 Acc: 0.9302 | Mem: 792.88MB | Speed: 1173.39 samples/s\n",
      "Epoch 629/1000 | Train Loss: 0.1423 Acc: 0.9400 | Val Loss: 0.1318 Acc: 0.9070 | Mem: 792.88MB | Speed: 1187.01 samples/s\n",
      "Epoch 630/1000 | Train Loss: 0.1137 Acc: 0.9506 | Val Loss: 0.1334 Acc: 0.9186 | Mem: 792.88MB | Speed: 1181.30 samples/s\n",
      "Epoch 631/1000 | Train Loss: 0.1279 Acc: 0.9529 | Val Loss: 0.1275 Acc: 0.9070 | Mem: 792.88MB | Speed: 1170.55 samples/s\n",
      "Epoch 632/1000 | Train Loss: 0.1515 Acc: 0.9459 | Val Loss: 0.1009 Acc: 0.9186 | Mem: 792.88MB | Speed: 1173.74 samples/s\n",
      "Epoch 633/1000 | Train Loss: 0.1118 Acc: 0.9576 | Val Loss: 0.1184 Acc: 0.8953 | Mem: 792.88MB | Speed: 1154.55 samples/s\n",
      "Epoch 634/1000 | Train Loss: 0.1604 Acc: 0.9365 | Val Loss: 0.1160 Acc: 0.9070 | Mem: 792.88MB | Speed: 1184.39 samples/s\n",
      "Epoch 635/1000 | Train Loss: 0.1163 Acc: 0.9529 | Val Loss: 0.0970 Acc: 0.9302 | Mem: 792.88MB | Speed: 1171.54 samples/s\n",
      "Epoch 636/1000 | Train Loss: 0.1393 Acc: 0.9435 | Val Loss: 0.0983 Acc: 0.9535 | Mem: 792.88MB | Speed: 1185.77 samples/s\n",
      "Epoch 637/1000 | Train Loss: 0.1405 Acc: 0.9447 | Val Loss: 0.1235 Acc: 0.9302 | Mem: 792.88MB | Speed: 1163.81 samples/s\n",
      "Epoch 638/1000 | Train Loss: 0.1073 Acc: 0.9624 | Val Loss: 0.1557 Acc: 0.9070 | Mem: 792.88MB | Speed: 1154.83 samples/s\n",
      "Epoch 639/1000 | Train Loss: 0.1472 Acc: 0.9353 | Val Loss: 0.1885 Acc: 0.8953 | Mem: 792.88MB | Speed: 1142.77 samples/s\n",
      "Epoch 640/1000 | Train Loss: 0.1257 Acc: 0.9576 | Val Loss: 0.1755 Acc: 0.9070 | Mem: 792.88MB | Speed: 1187.85 samples/s\n",
      "Epoch 641/1000 | Train Loss: 0.1413 Acc: 0.9494 | Val Loss: 0.1503 Acc: 0.9070 | Mem: 792.88MB | Speed: 1180.35 samples/s\n",
      "Epoch 642/1000 | Train Loss: 0.1183 Acc: 0.9529 | Val Loss: 0.1351 Acc: 0.9186 | Mem: 792.88MB | Speed: 1167.81 samples/s\n",
      "Epoch 643/1000 | Train Loss: 0.1170 Acc: 0.9600 | Val Loss: 0.1297 Acc: 0.9070 | Mem: 792.88MB | Speed: 1160.70 samples/s\n",
      "Epoch 644/1000 | Train Loss: 0.1186 Acc: 0.9600 | Val Loss: 0.1241 Acc: 0.9186 | Mem: 792.88MB | Speed: 1167.40 samples/s\n",
      "Epoch 645/1000 | Train Loss: 0.1155 Acc: 0.9576 | Val Loss: 0.1135 Acc: 0.9302 | Mem: 792.88MB | Speed: 1161.98 samples/s\n",
      "Epoch 646/1000 | Train Loss: 0.1366 Acc: 0.9494 | Val Loss: 0.1260 Acc: 0.9070 | Mem: 792.88MB | Speed: 1141.63 samples/s\n",
      "Epoch 647/1000 | Train Loss: 0.1135 Acc: 0.9600 | Val Loss: 0.1429 Acc: 0.9070 | Mem: 792.88MB | Speed: 1165.47 samples/s\n",
      "Epoch 648/1000 | Train Loss: 0.1537 Acc: 0.9424 | Val Loss: 0.1580 Acc: 0.8953 | Mem: 792.88MB | Speed: 1146.90 samples/s\n",
      "Epoch 649/1000 | Train Loss: 0.1363 Acc: 0.9494 | Val Loss: 0.1602 Acc: 0.8953 | Mem: 792.88MB | Speed: 1173.05 samples/s\n",
      "Epoch 650/1000 | Train Loss: 0.1354 Acc: 0.9506 | Val Loss: 0.1504 Acc: 0.9070 | Mem: 792.88MB | Speed: 1153.16 samples/s\n",
      "Epoch 651/1000 | Train Loss: 0.1258 Acc: 0.9506 | Val Loss: 0.1274 Acc: 0.9302 | Mem: 792.88MB | Speed: 1170.28 samples/s\n",
      "Epoch 652/1000 | Train Loss: 0.1681 Acc: 0.9424 | Val Loss: 0.1220 Acc: 0.9186 | Mem: 792.88MB | Speed: 1178.21 samples/s\n",
      "Epoch 653/1000 | Train Loss: 0.1212 Acc: 0.9541 | Val Loss: 0.1234 Acc: 0.9070 | Mem: 792.88MB | Speed: 1167.20 samples/s\n",
      "Epoch 654/1000 | Train Loss: 0.1411 Acc: 0.9435 | Val Loss: 0.1456 Acc: 0.8953 | Mem: 792.88MB | Speed: 1177.83 samples/s\n",
      "Epoch 655/1000 | Train Loss: 0.1514 Acc: 0.9424 | Val Loss: 0.1435 Acc: 0.9070 | Mem: 792.88MB | Speed: 1177.83 samples/s\n",
      "Epoch 656/1000 | Train Loss: 0.1338 Acc: 0.9529 | Val Loss: 0.1300 Acc: 0.9186 | Mem: 792.88MB | Speed: 1160.81 samples/s\n",
      "Epoch 657/1000 | Train Loss: 0.1443 Acc: 0.9424 | Val Loss: 0.1079 Acc: 0.9302 | Mem: 792.88MB | Speed: 1179.64 samples/s\n",
      "Epoch 658/1000 | Train Loss: 0.1229 Acc: 0.9506 | Val Loss: 0.1050 Acc: 0.9419 | Mem: 792.88MB | Speed: 1184.62 samples/s\n",
      "Epoch 659/1000 | Train Loss: 0.1016 Acc: 0.9576 | Val Loss: 0.1038 Acc: 0.9419 | Mem: 792.88MB | Speed: 1176.04 samples/s\n",
      "Epoch 660/1000 | Train Loss: 0.1414 Acc: 0.9435 | Val Loss: 0.1153 Acc: 0.9419 | Mem: 792.88MB | Speed: 1186.10 samples/s\n",
      "Epoch 661/1000 | Train Loss: 0.1323 Acc: 0.9518 | Val Loss: 0.1402 Acc: 0.9302 | Mem: 792.88MB | Speed: 1203.88 samples/s\n",
      "Epoch 662/1000 | Train Loss: 0.1277 Acc: 0.9459 | Val Loss: 0.1521 Acc: 0.9186 | Mem: 792.88MB | Speed: 1185.55 samples/s\n",
      "Epoch 663/1000 | Train Loss: 0.1409 Acc: 0.9447 | Val Loss: 0.1556 Acc: 0.9070 | Mem: 792.88MB | Speed: 1163.29 samples/s\n",
      "Epoch 664/1000 | Train Loss: 0.1144 Acc: 0.9612 | Val Loss: 0.1392 Acc: 0.9186 | Mem: 792.88MB | Speed: 1169.43 samples/s\n",
      "Epoch 665/1000 | Train Loss: 0.1361 Acc: 0.9506 | Val Loss: 0.1221 Acc: 0.9070 | Mem: 792.88MB | Speed: 1209.66 samples/s\n",
      "Epoch 666/1000 | Train Loss: 0.1470 Acc: 0.9529 | Val Loss: 0.1009 Acc: 0.9302 | Mem: 792.88MB | Speed: 1175.26 samples/s\n",
      "Epoch 667/1000 | Train Loss: 0.1020 Acc: 0.9588 | Val Loss: 0.0948 Acc: 0.9302 | Mem: 792.88MB | Speed: 1186.51 samples/s\n",
      "Epoch 668/1000 | Train Loss: 0.1412 Acc: 0.9494 | Val Loss: 0.1273 Acc: 0.9070 | Mem: 792.88MB | Speed: 1189.24 samples/s\n",
      "Epoch 669/1000 | Train Loss: 0.1085 Acc: 0.9635 | Val Loss: 0.1644 Acc: 0.8953 | Mem: 792.88MB | Speed: 1188.72 samples/s\n",
      "Epoch 670/1000 | Train Loss: 0.1030 Acc: 0.9612 | Val Loss: 0.1828 Acc: 0.8837 | Mem: 792.88MB | Speed: 1178.90 samples/s\n",
      "Epoch 671/1000 | Train Loss: 0.1192 Acc: 0.9529 | Val Loss: 0.1473 Acc: 0.9070 | Mem: 792.88MB | Speed: 1170.11 samples/s\n",
      "Epoch 672/1000 | Train Loss: 0.1259 Acc: 0.9518 | Val Loss: 0.1411 Acc: 0.9186 | Mem: 792.88MB | Speed: 1164.47 samples/s\n",
      "Epoch 673/1000 | Train Loss: 0.1057 Acc: 0.9624 | Val Loss: 0.1475 Acc: 0.9070 | Mem: 792.88MB | Speed: 1176.63 samples/s\n",
      "Epoch 674/1000 | Train Loss: 0.1390 Acc: 0.9459 | Val Loss: 0.1548 Acc: 0.9070 | Mem: 792.88MB | Speed: 1173.98 samples/s\n",
      "Epoch 675/1000 | Train Loss: 0.1212 Acc: 0.9565 | Val Loss: 0.1378 Acc: 0.9070 | Mem: 792.88MB | Speed: 1173.95 samples/s\n",
      "Epoch 676/1000 | Train Loss: 0.1532 Acc: 0.9447 | Val Loss: 0.1263 Acc: 0.9186 | Mem: 792.88MB | Speed: 1150.11 samples/s\n",
      "Epoch 677/1000 | Train Loss: 0.1163 Acc: 0.9576 | Val Loss: 0.1197 Acc: 0.9302 | Mem: 792.88MB | Speed: 1158.30 samples/s\n",
      "Epoch 678/1000 | Train Loss: 0.1326 Acc: 0.9565 | Val Loss: 0.1161 Acc: 0.9419 | Mem: 792.88MB | Speed: 1171.37 samples/s\n",
      "Epoch 679/1000 | Train Loss: 0.1317 Acc: 0.9506 | Val Loss: 0.1127 Acc: 0.9302 | Mem: 792.88MB | Speed: 1164.12 samples/s\n",
      "Epoch 680/1000 | Train Loss: 0.1148 Acc: 0.9541 | Val Loss: 0.1092 Acc: 0.9302 | Mem: 792.88MB | Speed: 1155.24 samples/s\n",
      "Epoch 681/1000 | Train Loss: 0.1313 Acc: 0.9600 | Val Loss: 0.1115 Acc: 0.9419 | Mem: 792.88MB | Speed: 1163.18 samples/s\n",
      "Epoch 682/1000 | Train Loss: 0.1402 Acc: 0.9518 | Val Loss: 0.1136 Acc: 0.9302 | Mem: 792.88MB | Speed: 1161.25 samples/s\n",
      "Epoch 683/1000 | Train Loss: 0.1249 Acc: 0.9447 | Val Loss: 0.1364 Acc: 0.9070 | Mem: 792.88MB | Speed: 1138.69 samples/s\n",
      "Epoch 684/1000 | Train Loss: 0.1433 Acc: 0.9447 | Val Loss: 0.1266 Acc: 0.9186 | Mem: 792.88MB | Speed: 1165.19 samples/s\n",
      "Epoch 685/1000 | Train Loss: 0.0900 Acc: 0.9706 | Val Loss: 0.1235 Acc: 0.9070 | Mem: 792.88MB | Speed: 1162.78 samples/s\n",
      "Epoch 686/1000 | Train Loss: 0.1181 Acc: 0.9588 | Val Loss: 0.1253 Acc: 0.9186 | Mem: 792.88MB | Speed: 1180.24 samples/s\n",
      "Epoch 687/1000 | Train Loss: 0.1382 Acc: 0.9459 | Val Loss: 0.1264 Acc: 0.9302 | Mem: 792.88MB | Speed: 1155.43 samples/s\n",
      "Epoch 688/1000 | Train Loss: 0.1135 Acc: 0.9553 | Val Loss: 0.1400 Acc: 0.9070 | Mem: 792.88MB | Speed: 1163.51 samples/s\n",
      "Epoch 689/1000 | Train Loss: 0.1467 Acc: 0.9506 | Val Loss: 0.1305 Acc: 0.9070 | Mem: 792.88MB | Speed: 1153.98 samples/s\n",
      "Epoch 690/1000 | Train Loss: 0.1374 Acc: 0.9459 | Val Loss: 0.0966 Acc: 0.9070 | Mem: 792.88MB | Speed: 1148.12 samples/s\n",
      "Epoch 691/1000 | Train Loss: 0.1327 Acc: 0.9553 | Val Loss: 0.1043 Acc: 0.9302 | Mem: 792.88MB | Speed: 1166.39 samples/s\n",
      "Epoch 692/1000 | Train Loss: 0.1436 Acc: 0.9447 | Val Loss: 0.1278 Acc: 0.9302 | Mem: 792.88MB | Speed: 1154.79 samples/s\n",
      "Epoch 693/1000 | Train Loss: 0.1076 Acc: 0.9659 | Val Loss: 0.1455 Acc: 0.9186 | Mem: 792.88MB | Speed: 1167.28 samples/s\n",
      "Epoch 694/1000 | Train Loss: 0.1388 Acc: 0.9471 | Val Loss: 0.1357 Acc: 0.9186 | Mem: 792.88MB | Speed: 1160.84 samples/s\n",
      "Epoch 695/1000 | Train Loss: 0.1323 Acc: 0.9471 | Val Loss: 0.1245 Acc: 0.9070 | Mem: 792.88MB | Speed: 1157.04 samples/s\n",
      "Epoch 696/1000 | Train Loss: 0.1197 Acc: 0.9576 | Val Loss: 0.1218 Acc: 0.9186 | Mem: 792.88MB | Speed: 1156.24 samples/s\n",
      "Epoch 697/1000 | Train Loss: 0.1243 Acc: 0.9576 | Val Loss: 0.1227 Acc: 0.9070 | Mem: 792.88MB | Speed: 1173.60 samples/s\n",
      "Epoch 698/1000 | Train Loss: 0.1552 Acc: 0.9412 | Val Loss: 0.1439 Acc: 0.9186 | Mem: 792.88MB | Speed: 1190.02 samples/s\n",
      "Epoch 699/1000 | Train Loss: 0.1089 Acc: 0.9565 | Val Loss: 0.1544 Acc: 0.9070 | Mem: 792.88MB | Speed: 1169.11 samples/s\n",
      "Epoch 700/1000 | Train Loss: 0.1301 Acc: 0.9459 | Val Loss: 0.1223 Acc: 0.9302 | Mem: 792.88MB | Speed: 1179.36 samples/s\n",
      "Epoch 701/1000 | Train Loss: 0.1354 Acc: 0.9529 | Val Loss: 0.1076 Acc: 0.9535 | Mem: 792.88MB | Speed: 1165.68 samples/s\n",
      "Epoch 702/1000 | Train Loss: 0.1130 Acc: 0.9506 | Val Loss: 0.1114 Acc: 0.9419 | Mem: 792.88MB | Speed: 1146.55 samples/s\n",
      "Epoch 703/1000 | Train Loss: 0.1324 Acc: 0.9447 | Val Loss: 0.1190 Acc: 0.9419 | Mem: 792.88MB | Speed: 1154.54 samples/s\n",
      "Epoch 704/1000 | Train Loss: 0.0997 Acc: 0.9624 | Val Loss: 0.1386 Acc: 0.9186 | Mem: 792.88MB | Speed: 1145.82 samples/s\n",
      "Epoch 705/1000 | Train Loss: 0.1326 Acc: 0.9494 | Val Loss: 0.1480 Acc: 0.9302 | Mem: 792.88MB | Speed: 1168.99 samples/s\n",
      "Epoch 706/1000 | Train Loss: 0.1525 Acc: 0.9447 | Val Loss: 0.1377 Acc: 0.9302 | Mem: 792.88MB | Speed: 1174.09 samples/s\n",
      "Epoch 707/1000 | Train Loss: 0.1115 Acc: 0.9612 | Val Loss: 0.1285 Acc: 0.9302 | Mem: 792.88MB | Speed: 1178.23 samples/s\n",
      "Epoch 708/1000 | Train Loss: 0.1302 Acc: 0.9553 | Val Loss: 0.1234 Acc: 0.9419 | Mem: 792.88MB | Speed: 1165.39 samples/s\n",
      "Epoch 709/1000 | Train Loss: 0.1284 Acc: 0.9553 | Val Loss: 0.1296 Acc: 0.9186 | Mem: 792.88MB | Speed: 1154.02 samples/s\n",
      "Epoch 710/1000 | Train Loss: 0.1165 Acc: 0.9494 | Val Loss: 0.1501 Acc: 0.9186 | Mem: 792.88MB | Speed: 1178.36 samples/s\n",
      "Epoch 711/1000 | Train Loss: 0.1053 Acc: 0.9588 | Val Loss: 0.1590 Acc: 0.9186 | Mem: 792.88MB | Speed: 1161.41 samples/s\n",
      "Epoch 712/1000 | Train Loss: 0.1226 Acc: 0.9588 | Val Loss: 0.1491 Acc: 0.9302 | Mem: 792.88MB | Speed: 1189.46 samples/s\n",
      "Epoch 713/1000 | Train Loss: 0.1219 Acc: 0.9518 | Val Loss: 0.1453 Acc: 0.9186 | Mem: 792.88MB | Speed: 1184.69 samples/s\n",
      "Epoch 714/1000 | Train Loss: 0.1289 Acc: 0.9565 | Val Loss: 0.1454 Acc: 0.9070 | Mem: 792.88MB | Speed: 1178.62 samples/s\n",
      "Epoch 715/1000 | Train Loss: 0.1110 Acc: 0.9565 | Val Loss: 0.1583 Acc: 0.8953 | Mem: 792.88MB | Speed: 1164.81 samples/s\n",
      "Epoch 716/1000 | Train Loss: 0.1128 Acc: 0.9612 | Val Loss: 0.1475 Acc: 0.9070 | Mem: 792.88MB | Speed: 1172.71 samples/s\n",
      "Epoch 717/1000 | Train Loss: 0.1329 Acc: 0.9506 | Val Loss: 0.1481 Acc: 0.8953 | Mem: 792.88MB | Speed: 1175.92 samples/s\n",
      "Epoch 718/1000 | Train Loss: 0.1236 Acc: 0.9529 | Val Loss: 0.1316 Acc: 0.9070 | Mem: 792.88MB | Speed: 1163.91 samples/s\n",
      "Epoch 719/1000 | Train Loss: 0.1191 Acc: 0.9565 | Val Loss: 0.1195 Acc: 0.9419 | Mem: 792.88MB | Speed: 1187.03 samples/s\n",
      "Epoch 720/1000 | Train Loss: 0.1114 Acc: 0.9541 | Val Loss: 0.1224 Acc: 0.9535 | Mem: 792.88MB | Speed: 1171.60 samples/s\n",
      "Epoch 721/1000 | Train Loss: 0.1162 Acc: 0.9518 | Val Loss: 0.1237 Acc: 0.9186 | Mem: 792.88MB | Speed: 1179.55 samples/s\n",
      "Epoch 722/1000 | Train Loss: 0.1559 Acc: 0.9494 | Val Loss: 0.1444 Acc: 0.9186 | Mem: 792.88MB | Speed: 1167.59 samples/s\n",
      "Epoch 723/1000 | Train Loss: 0.1016 Acc: 0.9682 | Val Loss: 0.1307 Acc: 0.9302 | Mem: 792.88MB | Speed: 1164.48 samples/s\n",
      "Epoch 724/1000 | Train Loss: 0.0977 Acc: 0.9635 | Val Loss: 0.1248 Acc: 0.9302 | Mem: 792.88MB | Speed: 1172.92 samples/s\n",
      "Epoch 725/1000 | Train Loss: 0.0912 Acc: 0.9694 | Val Loss: 0.1335 Acc: 0.9302 | Mem: 792.88MB | Speed: 1175.12 samples/s\n",
      "Epoch 726/1000 | Train Loss: 0.1163 Acc: 0.9541 | Val Loss: 0.1389 Acc: 0.9186 | Mem: 792.88MB | Speed: 1172.53 samples/s\n",
      "Epoch 727/1000 | Train Loss: 0.1169 Acc: 0.9624 | Val Loss: 0.1529 Acc: 0.9186 | Mem: 792.88MB | Speed: 1172.07 samples/s\n",
      "Epoch 728/1000 | Train Loss: 0.1139 Acc: 0.9624 | Val Loss: 0.1580 Acc: 0.9186 | Mem: 792.88MB | Speed: 1179.57 samples/s\n",
      "Epoch 729/1000 | Train Loss: 0.1284 Acc: 0.9494 | Val Loss: 0.1590 Acc: 0.9186 | Mem: 792.88MB | Speed: 1175.83 samples/s\n",
      "Epoch 730/1000 | Train Loss: 0.1107 Acc: 0.9553 | Val Loss: 0.1482 Acc: 0.9302 | Mem: 792.88MB | Speed: 1155.28 samples/s\n",
      "Epoch 731/1000 | Train Loss: 0.1335 Acc: 0.9494 | Val Loss: 0.1388 Acc: 0.9302 | Mem: 792.88MB | Speed: 1158.19 samples/s\n",
      "Epoch 732/1000 | Train Loss: 0.1151 Acc: 0.9600 | Val Loss: 0.1530 Acc: 0.9070 | Mem: 792.88MB | Speed: 1145.34 samples/s\n",
      "Epoch 733/1000 | Train Loss: 0.1147 Acc: 0.9541 | Val Loss: 0.1749 Acc: 0.9070 | Mem: 792.88MB | Speed: 1163.45 samples/s\n",
      "Epoch 734/1000 | Train Loss: 0.1344 Acc: 0.9565 | Val Loss: 0.1685 Acc: 0.9070 | Mem: 792.88MB | Speed: 1165.39 samples/s\n",
      "Epoch 735/1000 | Train Loss: 0.1143 Acc: 0.9600 | Val Loss: 0.1355 Acc: 0.9186 | Mem: 792.88MB | Speed: 1157.09 samples/s\n",
      "Epoch 736/1000 | Train Loss: 0.1194 Acc: 0.9506 | Val Loss: 0.1241 Acc: 0.9186 | Mem: 792.88MB | Speed: 1169.41 samples/s\n",
      "Epoch 737/1000 | Train Loss: 0.1328 Acc: 0.9576 | Val Loss: 0.1243 Acc: 0.9186 | Mem: 792.88MB | Speed: 1170.09 samples/s\n",
      "Epoch 738/1000 | Train Loss: 0.1306 Acc: 0.9494 | Val Loss: 0.1475 Acc: 0.9186 | Mem: 792.88MB | Speed: 1174.34 samples/s\n",
      "Epoch 739/1000 | Train Loss: 0.1365 Acc: 0.9541 | Val Loss: 0.1744 Acc: 0.9070 | Mem: 792.88MB | Speed: 1169.31 samples/s\n",
      "Epoch 740/1000 | Train Loss: 0.1097 Acc: 0.9635 | Val Loss: 0.1528 Acc: 0.9186 | Mem: 792.88MB | Speed: 1175.60 samples/s\n",
      "Epoch 741/1000 | Train Loss: 0.1179 Acc: 0.9553 | Val Loss: 0.1347 Acc: 0.9302 | Mem: 792.88MB | Speed: 1157.27 samples/s\n",
      "Epoch 742/1000 | Train Loss: 0.1276 Acc: 0.9553 | Val Loss: 0.1480 Acc: 0.9070 | Mem: 792.88MB | Speed: 1151.25 samples/s\n",
      "Epoch 743/1000 | Train Loss: 0.1576 Acc: 0.9447 | Val Loss: 0.1456 Acc: 0.9186 | Mem: 792.88MB | Speed: 1136.33 samples/s\n",
      "Epoch 744/1000 | Train Loss: 0.1234 Acc: 0.9576 | Val Loss: 0.1346 Acc: 0.9186 | Mem: 792.88MB | Speed: 1111.59 samples/s\n",
      "Epoch 745/1000 | Train Loss: 0.1187 Acc: 0.9529 | Val Loss: 0.1535 Acc: 0.9186 | Mem: 792.88MB | Speed: 1180.70 samples/s\n",
      "Epoch 746/1000 | Train Loss: 0.1238 Acc: 0.9576 | Val Loss: 0.1534 Acc: 0.9070 | Mem: 792.88MB | Speed: 1216.76 samples/s\n",
      "Epoch 747/1000 | Train Loss: 0.1178 Acc: 0.9541 | Val Loss: 0.1296 Acc: 0.9070 | Mem: 792.88MB | Speed: 1173.95 samples/s\n",
      "Epoch 748/1000 | Train Loss: 0.1308 Acc: 0.9635 | Val Loss: 0.1235 Acc: 0.9186 | Mem: 792.88MB | Speed: 1154.64 samples/s\n",
      "Epoch 749/1000 | Train Loss: 0.1080 Acc: 0.9682 | Val Loss: 0.1167 Acc: 0.9186 | Mem: 792.88MB | Speed: 1159.77 samples/s\n",
      "Epoch 750/1000 | Train Loss: 0.0994 Acc: 0.9718 | Val Loss: 0.1159 Acc: 0.9302 | Mem: 792.88MB | Speed: 1147.86 samples/s\n",
      "Epoch 751/1000 | Train Loss: 0.1176 Acc: 0.9624 | Val Loss: 0.1203 Acc: 0.9302 | Mem: 792.88MB | Speed: 1155.88 samples/s\n",
      "Epoch 752/1000 | Train Loss: 0.1363 Acc: 0.9459 | Val Loss: 0.1179 Acc: 0.9302 | Mem: 792.88MB | Speed: 1158.53 samples/s\n",
      "Epoch 753/1000 | Train Loss: 0.1173 Acc: 0.9553 | Val Loss: 0.1192 Acc: 0.9302 | Mem: 792.88MB | Speed: 1136.03 samples/s\n",
      "Epoch 754/1000 | Train Loss: 0.1405 Acc: 0.9506 | Val Loss: 0.1284 Acc: 0.9302 | Mem: 792.88MB | Speed: 1146.86 samples/s\n",
      "Epoch 755/1000 | Train Loss: 0.1178 Acc: 0.9506 | Val Loss: 0.1365 Acc: 0.9186 | Mem: 792.88MB | Speed: 1173.32 samples/s\n",
      "Epoch 756/1000 | Train Loss: 0.1082 Acc: 0.9612 | Val Loss: 0.1317 Acc: 0.9186 | Mem: 792.88MB | Speed: 1168.84 samples/s\n",
      "Epoch 757/1000 | Train Loss: 0.1048 Acc: 0.9647 | Val Loss: 0.1104 Acc: 0.9302 | Mem: 792.88MB | Speed: 1177.94 samples/s\n",
      "Epoch 758/1000 | Train Loss: 0.1073 Acc: 0.9635 | Val Loss: 0.1024 Acc: 0.9302 | Mem: 792.88MB | Speed: 1170.00 samples/s\n",
      "Epoch 759/1000 | Train Loss: 0.1026 Acc: 0.9647 | Val Loss: 0.1062 Acc: 0.9302 | Mem: 792.88MB | Speed: 1176.22 samples/s\n",
      "Epoch 760/1000 | Train Loss: 0.1144 Acc: 0.9518 | Val Loss: 0.1201 Acc: 0.9302 | Mem: 792.88MB | Speed: 1168.81 samples/s\n",
      "Epoch 761/1000 | Train Loss: 0.0990 Acc: 0.9694 | Val Loss: 0.1251 Acc: 0.9070 | Mem: 792.88MB | Speed: 1149.30 samples/s\n",
      "Epoch 762/1000 | Train Loss: 0.1302 Acc: 0.9541 | Val Loss: 0.1182 Acc: 0.9186 | Mem: 792.88MB | Speed: 1161.26 samples/s\n",
      "Epoch 763/1000 | Train Loss: 0.1222 Acc: 0.9506 | Val Loss: 0.1055 Acc: 0.9302 | Mem: 792.88MB | Speed: 1153.84 samples/s\n",
      "Epoch 764/1000 | Train Loss: 0.1183 Acc: 0.9588 | Val Loss: 0.1088 Acc: 0.9302 | Mem: 792.88MB | Speed: 1151.53 samples/s\n",
      "Epoch 765/1000 | Train Loss: 0.1328 Acc: 0.9494 | Val Loss: 0.1059 Acc: 0.9186 | Mem: 792.88MB | Speed: 1149.64 samples/s\n",
      "Epoch 766/1000 | Train Loss: 0.1005 Acc: 0.9682 | Val Loss: 0.1163 Acc: 0.9186 | Mem: 792.88MB | Speed: 1153.94 samples/s\n",
      "Epoch 767/1000 | Train Loss: 0.1403 Acc: 0.9482 | Val Loss: 0.1013 Acc: 0.9535 | Mem: 792.88MB | Speed: 1152.03 samples/s\n",
      "Epoch 768/1000 | Train Loss: 0.1295 Acc: 0.9553 | Val Loss: 0.0981 Acc: 0.9651 | Mem: 792.88MB | Speed: 1170.04 samples/s\n",
      "Epoch 769/1000 | Train Loss: 0.1379 Acc: 0.9412 | Val Loss: 0.1091 Acc: 0.9419 | Mem: 792.88MB | Speed: 1160.89 samples/s\n",
      "Epoch 770/1000 | Train Loss: 0.1733 Acc: 0.9424 | Val Loss: 0.1369 Acc: 0.9186 | Mem: 792.88MB | Speed: 1155.81 samples/s\n",
      "Epoch 771/1000 | Train Loss: 0.1469 Acc: 0.9529 | Val Loss: 0.1387 Acc: 0.9302 | Mem: 792.88MB | Speed: 1160.45 samples/s\n",
      "Epoch 772/1000 | Train Loss: 0.1494 Acc: 0.9447 | Val Loss: 0.1307 Acc: 0.9302 | Mem: 792.88MB | Speed: 1152.44 samples/s\n",
      "Epoch 773/1000 | Train Loss: 0.1322 Acc: 0.9576 | Val Loss: 0.1384 Acc: 0.9186 | Mem: 792.88MB | Speed: 1167.45 samples/s\n",
      "Epoch 774/1000 | Train Loss: 0.1112 Acc: 0.9600 | Val Loss: 0.1506 Acc: 0.9070 | Mem: 792.88MB | Speed: 1180.50 samples/s\n",
      "Epoch 775/1000 | Train Loss: 0.1296 Acc: 0.9600 | Val Loss: 0.1355 Acc: 0.9186 | Mem: 792.88MB | Speed: 1172.46 samples/s\n",
      "Epoch 776/1000 | Train Loss: 0.0984 Acc: 0.9565 | Val Loss: 0.1115 Acc: 0.9302 | Mem: 792.88MB | Speed: 1172.42 samples/s\n",
      "Epoch 777/1000 | Train Loss: 0.1129 Acc: 0.9624 | Val Loss: 0.1064 Acc: 0.9302 | Mem: 792.88MB | Speed: 1169.27 samples/s\n",
      "Epoch 778/1000 | Train Loss: 0.1389 Acc: 0.9482 | Val Loss: 0.1138 Acc: 0.9302 | Mem: 792.88MB | Speed: 1176.57 samples/s\n",
      "Epoch 779/1000 | Train Loss: 0.1156 Acc: 0.9506 | Val Loss: 0.1468 Acc: 0.8953 | Mem: 792.88MB | Speed: 1160.23 samples/s\n",
      "Epoch 780/1000 | Train Loss: 0.1254 Acc: 0.9529 | Val Loss: 0.1545 Acc: 0.8953 | Mem: 792.88MB | Speed: 1166.50 samples/s\n",
      "Epoch 781/1000 | Train Loss: 0.1320 Acc: 0.9529 | Val Loss: 0.1150 Acc: 0.9419 | Mem: 792.88MB | Speed: 1153.36 samples/s\n",
      "Epoch 782/1000 | Train Loss: 0.1207 Acc: 0.9541 | Val Loss: 0.1043 Acc: 0.9535 | Mem: 792.88MB | Speed: 1141.76 samples/s\n",
      "Epoch 783/1000 | Train Loss: 0.1161 Acc: 0.9541 | Val Loss: 0.0986 Acc: 0.9419 | Mem: 792.88MB | Speed: 1157.78 samples/s\n",
      "Epoch 784/1000 | Train Loss: 0.1063 Acc: 0.9553 | Val Loss: 0.1121 Acc: 0.9070 | Mem: 792.88MB | Speed: 1140.43 samples/s\n",
      "Epoch 785/1000 | Train Loss: 0.1250 Acc: 0.9506 | Val Loss: 0.1213 Acc: 0.9070 | Mem: 792.88MB | Speed: 1172.38 samples/s\n",
      "Epoch 786/1000 | Train Loss: 0.1147 Acc: 0.9494 | Val Loss: 0.1134 Acc: 0.9302 | Mem: 792.88MB | Speed: 1242.55 samples/s\n",
      "Epoch 787/1000 | Train Loss: 0.0856 Acc: 0.9706 | Val Loss: 0.1084 Acc: 0.9419 | Mem: 792.88MB | Speed: 1163.77 samples/s\n",
      "Epoch 788/1000 | Train Loss: 0.1039 Acc: 0.9647 | Val Loss: 0.1093 Acc: 0.9535 | Mem: 792.88MB | Speed: 1147.73 samples/s\n",
      "Epoch 789/1000 | Train Loss: 0.1114 Acc: 0.9612 | Val Loss: 0.1086 Acc: 0.9419 | Mem: 792.88MB | Speed: 1172.99 samples/s\n",
      "Epoch 790/1000 | Train Loss: 0.0828 Acc: 0.9718 | Val Loss: 0.1101 Acc: 0.9419 | Mem: 792.88MB | Speed: 1230.18 samples/s\n",
      "Epoch 791/1000 | Train Loss: 0.1051 Acc: 0.9600 | Val Loss: 0.1164 Acc: 0.9419 | Mem: 792.88MB | Speed: 1187.72 samples/s\n",
      "Epoch 792/1000 | Train Loss: 0.1066 Acc: 0.9541 | Val Loss: 0.1210 Acc: 0.9302 | Mem: 792.88MB | Speed: 1183.72 samples/s\n",
      "Epoch 793/1000 | Train Loss: 0.1059 Acc: 0.9612 | Val Loss: 0.1221 Acc: 0.9302 | Mem: 792.88MB | Speed: 1176.79 samples/s\n",
      "Epoch 794/1000 | Train Loss: 0.1295 Acc: 0.9482 | Val Loss: 0.1261 Acc: 0.9302 | Mem: 792.88MB | Speed: 1243.53 samples/s\n",
      "Epoch 795/1000 | Train Loss: 0.1148 Acc: 0.9494 | Val Loss: 0.1292 Acc: 0.9186 | Mem: 792.88MB | Speed: 1175.65 samples/s\n",
      "Epoch 796/1000 | Train Loss: 0.1134 Acc: 0.9529 | Val Loss: 0.1331 Acc: 0.9186 | Mem: 792.88MB | Speed: 1224.04 samples/s\n",
      "Epoch 797/1000 | Train Loss: 0.1108 Acc: 0.9565 | Val Loss: 0.1439 Acc: 0.9186 | Mem: 792.88MB | Speed: 1176.04 samples/s\n",
      "Epoch 798/1000 | Train Loss: 0.1209 Acc: 0.9541 | Val Loss: 0.1470 Acc: 0.9186 | Mem: 792.88MB | Speed: 1171.36 samples/s\n",
      "Epoch 799/1000 | Train Loss: 0.0906 Acc: 0.9671 | Val Loss: 0.1338 Acc: 0.9186 | Mem: 792.88MB | Speed: 1181.98 samples/s\n",
      "Epoch 800/1000 | Train Loss: 0.1097 Acc: 0.9647 | Val Loss: 0.1244 Acc: 0.9186 | Mem: 792.88MB | Speed: 1165.39 samples/s\n",
      "Epoch 801/1000 | Train Loss: 0.1171 Acc: 0.9541 | Val Loss: 0.1195 Acc: 0.9419 | Mem: 792.88MB | Speed: 1176.94 samples/s\n",
      "Epoch 802/1000 | Train Loss: 0.1141 Acc: 0.9576 | Val Loss: 0.1188 Acc: 0.9302 | Mem: 792.88MB | Speed: 1164.79 samples/s\n",
      "Epoch 803/1000 | Train Loss: 0.1138 Acc: 0.9612 | Val Loss: 0.1160 Acc: 0.9419 | Mem: 792.88MB | Speed: 1147.99 samples/s\n",
      "Epoch 804/1000 | Train Loss: 0.1140 Acc: 0.9494 | Val Loss: 0.1152 Acc: 0.9302 | Mem: 792.88MB | Speed: 1164.90 samples/s\n",
      "Epoch 805/1000 | Train Loss: 0.1333 Acc: 0.9541 | Val Loss: 0.1214 Acc: 0.9302 | Mem: 792.88MB | Speed: 1178.08 samples/s\n",
      "Epoch 806/1000 | Train Loss: 0.1198 Acc: 0.9600 | Val Loss: 0.1287 Acc: 0.9186 | Mem: 792.88MB | Speed: 1151.82 samples/s\n",
      "Epoch 807/1000 | Train Loss: 0.1049 Acc: 0.9659 | Val Loss: 0.1258 Acc: 0.9186 | Mem: 792.88MB | Speed: 1170.23 samples/s\n",
      "Epoch 808/1000 | Train Loss: 0.0899 Acc: 0.9706 | Val Loss: 0.1203 Acc: 0.9302 | Mem: 792.88MB | Speed: 1180.61 samples/s\n",
      "Epoch 809/1000 | Train Loss: 0.0848 Acc: 0.9647 | Val Loss: 0.1197 Acc: 0.9302 | Mem: 792.88MB | Speed: 1191.09 samples/s\n",
      "Epoch 810/1000 | Train Loss: 0.1360 Acc: 0.9518 | Val Loss: 0.1201 Acc: 0.9302 | Mem: 792.88MB | Speed: 1161.91 samples/s\n",
      "Epoch 811/1000 | Train Loss: 0.1146 Acc: 0.9635 | Val Loss: 0.1169 Acc: 0.9302 | Mem: 792.88MB | Speed: 1187.69 samples/s\n",
      "Epoch 812/1000 | Train Loss: 0.1177 Acc: 0.9600 | Val Loss: 0.1289 Acc: 0.9302 | Mem: 792.88MB | Speed: 1160.38 samples/s\n",
      "Epoch 813/1000 | Train Loss: 0.1230 Acc: 0.9459 | Val Loss: 0.1454 Acc: 0.9070 | Mem: 792.88MB | Speed: 1159.06 samples/s\n",
      "Epoch 814/1000 | Train Loss: 0.0907 Acc: 0.9647 | Val Loss: 0.1567 Acc: 0.9070 | Mem: 792.88MB | Speed: 1206.44 samples/s\n",
      "Epoch 815/1000 | Train Loss: 0.1230 Acc: 0.9565 | Val Loss: 0.1289 Acc: 0.9186 | Mem: 792.88MB | Speed: 1154.97 samples/s\n",
      "Epoch 816/1000 | Train Loss: 0.1017 Acc: 0.9635 | Val Loss: 0.1066 Acc: 0.9419 | Mem: 792.88MB | Speed: 1161.58 samples/s\n",
      "Epoch 817/1000 | Train Loss: 0.1477 Acc: 0.9506 | Val Loss: 0.1057 Acc: 0.9535 | Mem: 792.88MB | Speed: 1168.03 samples/s\n",
      "Epoch 818/1000 | Train Loss: 0.1329 Acc: 0.9494 | Val Loss: 0.1174 Acc: 0.9302 | Mem: 792.88MB | Speed: 1160.72 samples/s\n",
      "Epoch 819/1000 | Train Loss: 0.0995 Acc: 0.9706 | Val Loss: 0.1077 Acc: 0.9302 | Mem: 792.88MB | Speed: 1160.34 samples/s\n",
      "Epoch 820/1000 | Train Loss: 0.0904 Acc: 0.9635 | Val Loss: 0.1042 Acc: 0.9302 | Mem: 792.88MB | Speed: 1161.03 samples/s\n",
      "Epoch 821/1000 | Train Loss: 0.1272 Acc: 0.9482 | Val Loss: 0.1138 Acc: 0.9302 | Mem: 792.88MB | Speed: 1170.45 samples/s\n",
      "Epoch 822/1000 | Train Loss: 0.1278 Acc: 0.9482 | Val Loss: 0.1432 Acc: 0.9070 | Mem: 792.88MB | Speed: 1163.32 samples/s\n",
      "Epoch 823/1000 | Train Loss: 0.1520 Acc: 0.9506 | Val Loss: 0.1446 Acc: 0.9186 | Mem: 792.88MB | Speed: 1184.52 samples/s\n",
      "Epoch 824/1000 | Train Loss: 0.1275 Acc: 0.9482 | Val Loss: 0.1233 Acc: 0.9186 | Mem: 792.88MB | Speed: 1186.58 samples/s\n",
      "Epoch 825/1000 | Train Loss: 0.1204 Acc: 0.9482 | Val Loss: 0.1081 Acc: 0.9302 | Mem: 792.88MB | Speed: 1181.74 samples/s\n",
      "Epoch 826/1000 | Train Loss: 0.1039 Acc: 0.9659 | Val Loss: 0.1124 Acc: 0.9186 | Mem: 792.88MB | Speed: 1203.77 samples/s\n",
      "Epoch 827/1000 | Train Loss: 0.0994 Acc: 0.9624 | Val Loss: 0.1066 Acc: 0.9186 | Mem: 792.88MB | Speed: 1170.18 samples/s\n",
      "Epoch 828/1000 | Train Loss: 0.1273 Acc: 0.9553 | Val Loss: 0.1044 Acc: 0.9419 | Mem: 792.88MB | Speed: 1173.56 samples/s\n",
      "Epoch 829/1000 | Train Loss: 0.1249 Acc: 0.9529 | Val Loss: 0.1249 Acc: 0.9419 | Mem: 792.88MB | Speed: 1175.87 samples/s\n",
      "Epoch 830/1000 | Train Loss: 0.1184 Acc: 0.9506 | Val Loss: 0.1451 Acc: 0.9302 | Mem: 792.88MB | Speed: 1179.51 samples/s\n",
      "Epoch 831/1000 | Train Loss: 0.1193 Acc: 0.9659 | Val Loss: 0.1523 Acc: 0.9302 | Mem: 792.88MB | Speed: 1173.50 samples/s\n",
      "Epoch 832/1000 | Train Loss: 0.1399 Acc: 0.9518 | Val Loss: 0.1561 Acc: 0.9419 | Mem: 792.88MB | Speed: 1171.77 samples/s\n",
      "Epoch 833/1000 | Train Loss: 0.1144 Acc: 0.9529 | Val Loss: 0.1458 Acc: 0.9302 | Mem: 792.88MB | Speed: 1167.23 samples/s\n",
      "Epoch 834/1000 | Train Loss: 0.1161 Acc: 0.9600 | Val Loss: 0.1420 Acc: 0.9302 | Mem: 792.88MB | Speed: 1201.33 samples/s\n",
      "Epoch 835/1000 | Train Loss: 0.1208 Acc: 0.9541 | Val Loss: 0.1252 Acc: 0.9302 | Mem: 792.88MB | Speed: 1170.90 samples/s\n",
      "Epoch 836/1000 | Train Loss: 0.0935 Acc: 0.9612 | Val Loss: 0.1141 Acc: 0.9302 | Mem: 792.88MB | Speed: 1231.44 samples/s\n",
      "Epoch 837/1000 | Train Loss: 0.1086 Acc: 0.9600 | Val Loss: 0.1146 Acc: 0.9302 | Mem: 792.88MB | Speed: 1172.90 samples/s\n",
      "Epoch 838/1000 | Train Loss: 0.1247 Acc: 0.9482 | Val Loss: 0.1227 Acc: 0.9186 | Mem: 792.88MB | Speed: 1159.95 samples/s\n",
      "Epoch 839/1000 | Train Loss: 0.0876 Acc: 0.9647 | Val Loss: 0.1368 Acc: 0.9186 | Mem: 792.88MB | Speed: 1163.42 samples/s\n",
      "Epoch 840/1000 | Train Loss: 0.1142 Acc: 0.9612 | Val Loss: 0.1222 Acc: 0.9302 | Mem: 792.88MB | Speed: 1165.61 samples/s\n",
      "Epoch 841/1000 | Train Loss: 0.1141 Acc: 0.9600 | Val Loss: 0.1125 Acc: 0.9302 | Mem: 792.88MB | Speed: 1165.58 samples/s\n",
      "Epoch 842/1000 | Train Loss: 0.1097 Acc: 0.9588 | Val Loss: 0.1195 Acc: 0.9302 | Mem: 792.88MB | Speed: 1178.95 samples/s\n",
      "Epoch 843/1000 | Train Loss: 0.1024 Acc: 0.9671 | Val Loss: 0.1200 Acc: 0.9186 | Mem: 792.88MB | Speed: 1162.39 samples/s\n",
      "Epoch 844/1000 | Train Loss: 0.1129 Acc: 0.9612 | Val Loss: 0.1202 Acc: 0.9419 | Mem: 792.88MB | Speed: 1169.61 samples/s\n",
      "Epoch 845/1000 | Train Loss: 0.1082 Acc: 0.9635 | Val Loss: 0.1394 Acc: 0.9302 | Mem: 792.88MB | Speed: 1155.35 samples/s\n",
      "Epoch 846/1000 | Train Loss: 0.1269 Acc: 0.9565 | Val Loss: 0.1480 Acc: 0.9070 | Mem: 792.88MB | Speed: 1148.06 samples/s\n",
      "Epoch 847/1000 | Train Loss: 0.0996 Acc: 0.9588 | Val Loss: 0.1110 Acc: 0.9419 | Mem: 792.88MB | Speed: 1158.19 samples/s\n",
      "Epoch 848/1000 | Train Loss: 0.1063 Acc: 0.9600 | Val Loss: 0.0949 Acc: 0.9535 | Mem: 792.88MB | Speed: 1149.17 samples/s\n",
      "Epoch 849/1000 | Train Loss: 0.1309 Acc: 0.9494 | Val Loss: 0.0921 Acc: 0.9535 | Mem: 792.88MB | Speed: 1170.03 samples/s\n",
      "Epoch 850/1000 | Train Loss: 0.1133 Acc: 0.9635 | Val Loss: 0.1040 Acc: 0.9535 | Mem: 792.88MB | Speed: 1172.07 samples/s\n",
      "Epoch 851/1000 | Train Loss: 0.1103 Acc: 0.9635 | Val Loss: 0.1247 Acc: 0.9302 | Mem: 792.88MB | Speed: 1149.03 samples/s\n",
      "Epoch 852/1000 | Train Loss: 0.1103 Acc: 0.9600 | Val Loss: 0.1272 Acc: 0.9302 | Mem: 792.88MB | Speed: 1175.43 samples/s\n",
      "Epoch 853/1000 | Train Loss: 0.1191 Acc: 0.9518 | Val Loss: 0.1253 Acc: 0.9186 | Mem: 792.88MB | Speed: 1169.82 samples/s\n",
      "Epoch 854/1000 | Train Loss: 0.1048 Acc: 0.9635 | Val Loss: 0.1175 Acc: 0.9302 | Mem: 792.88MB | Speed: 1170.43 samples/s\n",
      "Epoch 855/1000 | Train Loss: 0.1016 Acc: 0.9600 | Val Loss: 0.1109 Acc: 0.9419 | Mem: 792.88MB | Speed: 1170.72 samples/s\n",
      "Epoch 856/1000 | Train Loss: 0.1251 Acc: 0.9576 | Val Loss: 0.0953 Acc: 0.9535 | Mem: 792.88MB | Speed: 1152.29 samples/s\n",
      "Epoch 857/1000 | Train Loss: 0.0989 Acc: 0.9624 | Val Loss: 0.0902 Acc: 0.9535 | Mem: 792.88MB | Speed: 1164.88 samples/s\n",
      "Epoch 858/1000 | Train Loss: 0.1259 Acc: 0.9529 | Val Loss: 0.0910 Acc: 0.9419 | Mem: 792.88MB | Speed: 1144.64 samples/s\n",
      "Epoch 859/1000 | Train Loss: 0.1097 Acc: 0.9529 | Val Loss: 0.0882 Acc: 0.9419 | Mem: 792.88MB | Speed: 1161.38 samples/s\n",
      "Epoch 860/1000 | Train Loss: 0.1079 Acc: 0.9635 | Val Loss: 0.0927 Acc: 0.9302 | Mem: 792.88MB | Speed: 1197.39 samples/s\n",
      "Epoch 861/1000 | Train Loss: 0.1106 Acc: 0.9565 | Val Loss: 0.1102 Acc: 0.9186 | Mem: 792.88MB | Speed: 1153.05 samples/s\n",
      "Epoch 862/1000 | Train Loss: 0.0840 Acc: 0.9741 | Val Loss: 0.1122 Acc: 0.9302 | Mem: 792.88MB | Speed: 1186.92 samples/s\n",
      "Epoch 863/1000 | Train Loss: 0.1128 Acc: 0.9588 | Val Loss: 0.1056 Acc: 0.9419 | Mem: 792.88MB | Speed: 1200.55 samples/s\n",
      "Epoch 864/1000 | Train Loss: 0.1367 Acc: 0.9541 | Val Loss: 0.1050 Acc: 0.9419 | Mem: 792.88MB | Speed: 1178.22 samples/s\n",
      "Epoch 865/1000 | Train Loss: 0.1201 Acc: 0.9541 | Val Loss: 0.1069 Acc: 0.9419 | Mem: 792.88MB | Speed: 1176.78 samples/s\n",
      "Epoch 866/1000 | Train Loss: 0.1008 Acc: 0.9624 | Val Loss: 0.1160 Acc: 0.9302 | Mem: 792.88MB | Speed: 1189.91 samples/s\n",
      "Epoch 867/1000 | Train Loss: 0.1091 Acc: 0.9588 | Val Loss: 0.1087 Acc: 0.9302 | Mem: 792.88MB | Speed: 1197.77 samples/s\n",
      "Epoch 868/1000 | Train Loss: 0.1050 Acc: 0.9576 | Val Loss: 0.1145 Acc: 0.9302 | Mem: 792.88MB | Speed: 1163.14 samples/s\n",
      "Epoch 869/1000 | Train Loss: 0.1664 Acc: 0.9435 | Val Loss: 0.1028 Acc: 0.9419 | Mem: 792.88MB | Speed: 1163.50 samples/s\n",
      "Epoch 870/1000 | Train Loss: 0.1003 Acc: 0.9600 | Val Loss: 0.0989 Acc: 0.9419 | Mem: 792.88MB | Speed: 1155.35 samples/s\n",
      "Epoch 871/1000 | Train Loss: 0.0982 Acc: 0.9553 | Val Loss: 0.1143 Acc: 0.9302 | Mem: 792.88MB | Speed: 1152.89 samples/s\n",
      "Epoch 872/1000 | Train Loss: 0.1102 Acc: 0.9659 | Val Loss: 0.1358 Acc: 0.8837 | Mem: 792.88MB | Speed: 1160.76 samples/s\n",
      "Epoch 873/1000 | Train Loss: 0.0932 Acc: 0.9694 | Val Loss: 0.1433 Acc: 0.9302 | Mem: 792.88MB | Speed: 1232.89 samples/s\n",
      "Epoch 874/1000 | Train Loss: 0.1037 Acc: 0.9565 | Val Loss: 0.1437 Acc: 0.9302 | Mem: 792.88MB | Speed: 1163.82 samples/s\n",
      "Epoch 875/1000 | Train Loss: 0.0877 Acc: 0.9718 | Val Loss: 0.1329 Acc: 0.9419 | Mem: 792.88MB | Speed: 1177.40 samples/s\n",
      "Epoch 876/1000 | Train Loss: 0.0953 Acc: 0.9659 | Val Loss: 0.1339 Acc: 0.9419 | Mem: 792.88MB | Speed: 1179.96 samples/s\n",
      "Epoch 877/1000 | Train Loss: 0.1220 Acc: 0.9612 | Val Loss: 0.1297 Acc: 0.9302 | Mem: 792.88MB | Speed: 1179.21 samples/s\n",
      "Epoch 878/1000 | Train Loss: 0.1173 Acc: 0.9576 | Val Loss: 0.1236 Acc: 0.9186 | Mem: 792.88MB | Speed: 1167.10 samples/s\n",
      "Epoch 879/1000 | Train Loss: 0.1122 Acc: 0.9612 | Val Loss: 0.1222 Acc: 0.9186 | Mem: 792.88MB | Speed: 1178.55 samples/s\n",
      "Epoch 880/1000 | Train Loss: 0.0831 Acc: 0.9682 | Val Loss: 0.1106 Acc: 0.9186 | Mem: 792.88MB | Speed: 1155.79 samples/s\n",
      "Epoch 881/1000 | Train Loss: 0.1042 Acc: 0.9682 | Val Loss: 0.0878 Acc: 0.9419 | Mem: 792.88MB | Speed: 1144.32 samples/s\n",
      "Epoch 882/1000 | Train Loss: 0.1177 Acc: 0.9553 | Val Loss: 0.0762 Acc: 0.9535 | Mem: 792.88MB | Speed: 1149.55 samples/s\n",
      "Epoch 883/1000 | Train Loss: 0.1188 Acc: 0.9518 | Val Loss: 0.0771 Acc: 0.9651 | Mem: 792.88MB | Speed: 1149.07 samples/s\n",
      "Epoch 884/1000 | Train Loss: 0.1296 Acc: 0.9565 | Val Loss: 0.0787 Acc: 0.9651 | Mem: 792.88MB | Speed: 1168.92 samples/s\n",
      "Epoch 885/1000 | Train Loss: 0.1305 Acc: 0.9529 | Val Loss: 0.0770 Acc: 0.9651 | Mem: 792.88MB | Speed: 1158.18 samples/s\n",
      "Epoch 886/1000 | Train Loss: 0.1482 Acc: 0.9494 | Val Loss: 0.0824 Acc: 0.9535 | Mem: 792.88MB | Speed: 1145.20 samples/s\n",
      "Epoch 887/1000 | Train Loss: 0.0960 Acc: 0.9659 | Val Loss: 0.0875 Acc: 0.9419 | Mem: 792.88MB | Speed: 1148.07 samples/s\n",
      "Epoch 888/1000 | Train Loss: 0.0996 Acc: 0.9671 | Val Loss: 0.0949 Acc: 0.9302 | Mem: 792.88MB | Speed: 1154.14 samples/s\n",
      "Epoch 889/1000 | Train Loss: 0.1464 Acc: 0.9400 | Val Loss: 0.0995 Acc: 0.9419 | Mem: 792.88MB | Speed: 1149.65 samples/s\n",
      "Epoch 890/1000 | Train Loss: 0.0848 Acc: 0.9694 | Val Loss: 0.1091 Acc: 0.9419 | Mem: 792.88MB | Speed: 1176.58 samples/s\n",
      "Epoch 891/1000 | Train Loss: 0.1068 Acc: 0.9612 | Val Loss: 0.1145 Acc: 0.9419 | Mem: 792.88MB | Speed: 1167.65 samples/s\n",
      "Epoch 892/1000 | Train Loss: 0.1264 Acc: 0.9541 | Val Loss: 0.1163 Acc: 0.9419 | Mem: 792.88MB | Speed: 1215.82 samples/s\n",
      "Epoch 893/1000 | Train Loss: 0.0890 Acc: 0.9694 | Val Loss: 0.1120 Acc: 0.9419 | Mem: 792.88MB | Speed: 1157.97 samples/s\n",
      "Epoch 894/1000 | Train Loss: 0.1115 Acc: 0.9624 | Val Loss: 0.1107 Acc: 0.9302 | Mem: 792.88MB | Speed: 1164.09 samples/s\n",
      "Epoch 895/1000 | Train Loss: 0.0930 Acc: 0.9588 | Val Loss: 0.1054 Acc: 0.9302 | Mem: 792.88MB | Speed: 1146.42 samples/s\n",
      "Epoch 896/1000 | Train Loss: 0.0924 Acc: 0.9671 | Val Loss: 0.0963 Acc: 0.9302 | Mem: 792.88MB | Speed: 1153.77 samples/s\n",
      "Epoch 897/1000 | Train Loss: 0.0898 Acc: 0.9612 | Val Loss: 0.1006 Acc: 0.9302 | Mem: 792.88MB | Speed: 1220.08 samples/s\n",
      "Epoch 898/1000 | Train Loss: 0.0829 Acc: 0.9776 | Val Loss: 0.1140 Acc: 0.9302 | Mem: 792.88MB | Speed: 1152.01 samples/s\n",
      "Epoch 899/1000 | Train Loss: 0.1405 Acc: 0.9494 | Val Loss: 0.1200 Acc: 0.9419 | Mem: 792.88MB | Speed: 1151.21 samples/s\n",
      "Epoch 900/1000 | Train Loss: 0.1063 Acc: 0.9682 | Val Loss: 0.1191 Acc: 0.9419 | Mem: 792.88MB | Speed: 1150.15 samples/s\n",
      "Epoch 901/1000 | Train Loss: 0.0900 Acc: 0.9659 | Val Loss: 0.1266 Acc: 0.9419 | Mem: 792.88MB | Speed: 1157.02 samples/s\n",
      "Epoch 902/1000 | Train Loss: 0.1173 Acc: 0.9576 | Val Loss: 0.1198 Acc: 0.9419 | Mem: 792.88MB | Speed: 1149.30 samples/s\n",
      "Epoch 903/1000 | Train Loss: 0.0990 Acc: 0.9635 | Val Loss: 0.1184 Acc: 0.9419 | Mem: 792.88MB | Speed: 1163.33 samples/s\n",
      "Epoch 904/1000 | Train Loss: 0.1339 Acc: 0.9541 | Val Loss: 0.1225 Acc: 0.9419 | Mem: 792.88MB | Speed: 1167.69 samples/s\n",
      "Epoch 905/1000 | Train Loss: 0.1385 Acc: 0.9518 | Val Loss: 0.1270 Acc: 0.9419 | Mem: 792.88MB | Speed: 1137.17 samples/s\n",
      "Epoch 906/1000 | Train Loss: 0.0863 Acc: 0.9600 | Val Loss: 0.1304 Acc: 0.9186 | Mem: 792.88MB | Speed: 1164.54 samples/s\n",
      "Epoch 907/1000 | Train Loss: 0.1287 Acc: 0.9518 | Val Loss: 0.1267 Acc: 0.9186 | Mem: 792.88MB | Speed: 1165.69 samples/s\n",
      "Epoch 908/1000 | Train Loss: 0.1155 Acc: 0.9541 | Val Loss: 0.1070 Acc: 0.9419 | Mem: 792.88MB | Speed: 1176.78 samples/s\n",
      "Epoch 909/1000 | Train Loss: 0.1467 Acc: 0.9435 | Val Loss: 0.1064 Acc: 0.9419 | Mem: 792.88MB | Speed: 1175.17 samples/s\n",
      "Epoch 910/1000 | Train Loss: 0.0909 Acc: 0.9624 | Val Loss: 0.1157 Acc: 0.9302 | Mem: 792.88MB | Speed: 1169.85 samples/s\n",
      "Epoch 911/1000 | Train Loss: 0.1353 Acc: 0.9459 | Val Loss: 0.1147 Acc: 0.9419 | Mem: 792.88MB | Speed: 1160.68 samples/s\n",
      "Epoch 912/1000 | Train Loss: 0.1130 Acc: 0.9576 | Val Loss: 0.1281 Acc: 0.9186 | Mem: 792.88MB | Speed: 1134.44 samples/s\n",
      "Epoch 913/1000 | Train Loss: 0.1072 Acc: 0.9624 | Val Loss: 0.1283 Acc: 0.9186 | Mem: 792.88MB | Speed: 1140.76 samples/s\n",
      "Epoch 914/1000 | Train Loss: 0.1172 Acc: 0.9600 | Val Loss: 0.1230 Acc: 0.9186 | Mem: 792.88MB | Speed: 1142.81 samples/s\n",
      "Epoch 915/1000 | Train Loss: 0.0986 Acc: 0.9671 | Val Loss: 0.1269 Acc: 0.9302 | Mem: 792.88MB | Speed: 1132.44 samples/s\n",
      "Epoch 916/1000 | Train Loss: 0.1018 Acc: 0.9541 | Val Loss: 0.1374 Acc: 0.9302 | Mem: 792.88MB | Speed: 1203.46 samples/s\n",
      "Epoch 917/1000 | Train Loss: 0.1067 Acc: 0.9624 | Val Loss: 0.1413 Acc: 0.9419 | Mem: 792.88MB | Speed: 1147.09 samples/s\n",
      "Epoch 918/1000 | Train Loss: 0.0875 Acc: 0.9612 | Val Loss: 0.1369 Acc: 0.9419 | Mem: 792.88MB | Speed: 1145.53 samples/s\n",
      "Epoch 919/1000 | Train Loss: 0.1137 Acc: 0.9612 | Val Loss: 0.1242 Acc: 0.9419 | Mem: 792.88MB | Speed: 1162.21 samples/s\n",
      "Epoch 920/1000 | Train Loss: 0.1109 Acc: 0.9600 | Val Loss: 0.1243 Acc: 0.9302 | Mem: 792.88MB | Speed: 1141.22 samples/s\n",
      "Epoch 921/1000 | Train Loss: 0.1016 Acc: 0.9659 | Val Loss: 0.1152 Acc: 0.9535 | Mem: 792.88MB | Speed: 1136.42 samples/s\n",
      "Epoch 922/1000 | Train Loss: 0.1201 Acc: 0.9565 | Val Loss: 0.1263 Acc: 0.9419 | Mem: 792.88MB | Speed: 1129.17 samples/s\n",
      "Epoch 923/1000 | Train Loss: 0.0998 Acc: 0.9671 | Val Loss: 0.1356 Acc: 0.9302 | Mem: 792.88MB | Speed: 1147.39 samples/s\n",
      "Epoch 924/1000 | Train Loss: 0.1104 Acc: 0.9588 | Val Loss: 0.1314 Acc: 0.9302 | Mem: 792.88MB | Speed: 1141.74 samples/s\n",
      "Epoch 925/1000 | Train Loss: 0.0941 Acc: 0.9659 | Val Loss: 0.1236 Acc: 0.9419 | Mem: 792.88MB | Speed: 1160.60 samples/s\n",
      "Epoch 926/1000 | Train Loss: 0.1031 Acc: 0.9612 | Val Loss: 0.1202 Acc: 0.9419 | Mem: 792.88MB | Speed: 1232.16 samples/s\n",
      "Epoch 927/1000 | Train Loss: 0.1122 Acc: 0.9529 | Val Loss: 0.1282 Acc: 0.9302 | Mem: 792.88MB | Speed: 1142.96 samples/s\n",
      "Epoch 928/1000 | Train Loss: 0.1248 Acc: 0.9494 | Val Loss: 0.1412 Acc: 0.9302 | Mem: 792.88MB | Speed: 1161.78 samples/s\n",
      "Epoch 929/1000 | Train Loss: 0.1129 Acc: 0.9518 | Val Loss: 0.1330 Acc: 0.9186 | Mem: 792.88MB | Speed: 1152.64 samples/s\n",
      "Epoch 930/1000 | Train Loss: 0.1250 Acc: 0.9541 | Val Loss: 0.1078 Acc: 0.9186 | Mem: 792.88MB | Speed: 1159.95 samples/s\n",
      "Epoch 931/1000 | Train Loss: 0.1127 Acc: 0.9565 | Val Loss: 0.1026 Acc: 0.9419 | Mem: 792.88MB | Speed: 1163.68 samples/s\n",
      "Epoch 932/1000 | Train Loss: 0.1011 Acc: 0.9647 | Val Loss: 0.1060 Acc: 0.9535 | Mem: 792.88MB | Speed: 1204.08 samples/s\n",
      "Epoch 933/1000 | Train Loss: 0.1066 Acc: 0.9588 | Val Loss: 0.1098 Acc: 0.9535 | Mem: 792.88MB | Speed: 1167.30 samples/s\n",
      "Epoch 934/1000 | Train Loss: 0.0831 Acc: 0.9741 | Val Loss: 0.1145 Acc: 0.9419 | Mem: 792.88MB | Speed: 1174.35 samples/s\n",
      "Epoch 935/1000 | Train Loss: 0.1142 Acc: 0.9553 | Val Loss: 0.1098 Acc: 0.9419 | Mem: 792.88MB | Speed: 1156.19 samples/s\n",
      "Epoch 936/1000 | Train Loss: 0.0971 Acc: 0.9576 | Val Loss: 0.1102 Acc: 0.9535 | Mem: 792.88MB | Speed: 1187.14 samples/s\n",
      "Epoch 937/1000 | Train Loss: 0.0940 Acc: 0.9600 | Val Loss: 0.1094 Acc: 0.9535 | Mem: 792.88MB | Speed: 1144.87 samples/s\n",
      "Epoch 938/1000 | Train Loss: 0.1111 Acc: 0.9588 | Val Loss: 0.1055 Acc: 0.9535 | Mem: 792.88MB | Speed: 1167.84 samples/s\n",
      "Epoch 939/1000 | Train Loss: 0.1247 Acc: 0.9529 | Val Loss: 0.1120 Acc: 0.9419 | Mem: 792.88MB | Speed: 1172.05 samples/s\n",
      "Epoch 940/1000 | Train Loss: 0.1044 Acc: 0.9612 | Val Loss: 0.1231 Acc: 0.9419 | Mem: 792.88MB | Speed: 1180.90 samples/s\n",
      "Epoch 941/1000 | Train Loss: 0.1001 Acc: 0.9565 | Val Loss: 0.1263 Acc: 0.9419 | Mem: 792.88MB | Speed: 1178.14 samples/s\n",
      "Epoch 942/1000 | Train Loss: 0.1143 Acc: 0.9600 | Val Loss: 0.1187 Acc: 0.9302 | Mem: 792.88MB | Speed: 1178.53 samples/s\n",
      "Epoch 943/1000 | Train Loss: 0.0977 Acc: 0.9647 | Val Loss: 0.1220 Acc: 0.9419 | Mem: 792.88MB | Speed: 1166.66 samples/s\n",
      "Epoch 944/1000 | Train Loss: 0.1099 Acc: 0.9576 | Val Loss: 0.1385 Acc: 0.9419 | Mem: 792.88MB | Speed: 1176.09 samples/s\n",
      "Epoch 945/1000 | Train Loss: 0.0925 Acc: 0.9612 | Val Loss: 0.1507 Acc: 0.9186 | Mem: 792.88MB | Speed: 1152.41 samples/s\n",
      "Epoch 946/1000 | Train Loss: 0.1159 Acc: 0.9588 | Val Loss: 0.1559 Acc: 0.9070 | Mem: 792.88MB | Speed: 1165.95 samples/s\n",
      "Epoch 947/1000 | Train Loss: 0.1202 Acc: 0.9482 | Val Loss: 0.1696 Acc: 0.8953 | Mem: 792.88MB | Speed: 1177.43 samples/s\n",
      "Epoch 948/1000 | Train Loss: 0.1135 Acc: 0.9612 | Val Loss: 0.1948 Acc: 0.8953 | Mem: 792.88MB | Speed: 1172.80 samples/s\n",
      "Epoch 949/1000 | Train Loss: 0.1059 Acc: 0.9553 | Val Loss: 0.1694 Acc: 0.8953 | Mem: 792.88MB | Speed: 1165.43 samples/s\n",
      "Epoch 950/1000 | Train Loss: 0.1031 Acc: 0.9682 | Val Loss: 0.1467 Acc: 0.9302 | Mem: 792.88MB | Speed: 1173.37 samples/s\n",
      "Epoch 951/1000 | Train Loss: 0.1040 Acc: 0.9600 | Val Loss: 0.1226 Acc: 0.9419 | Mem: 792.88MB | Speed: 1179.33 samples/s\n",
      "Epoch 952/1000 | Train Loss: 0.0939 Acc: 0.9588 | Val Loss: 0.1247 Acc: 0.9302 | Mem: 792.88MB | Speed: 1164.92 samples/s\n",
      "Epoch 953/1000 | Train Loss: 0.0984 Acc: 0.9659 | Val Loss: 0.1320 Acc: 0.9419 | Mem: 792.88MB | Speed: 1185.51 samples/s\n",
      "Epoch 954/1000 | Train Loss: 0.0922 Acc: 0.9682 | Val Loss: 0.1340 Acc: 0.9419 | Mem: 792.88MB | Speed: 1175.57 samples/s\n",
      "Epoch 955/1000 | Train Loss: 0.0963 Acc: 0.9659 | Val Loss: 0.1330 Acc: 0.9535 | Mem: 792.88MB | Speed: 1206.83 samples/s\n",
      "Epoch 956/1000 | Train Loss: 0.0989 Acc: 0.9635 | Val Loss: 0.1306 Acc: 0.9535 | Mem: 792.88MB | Speed: 1144.03 samples/s\n",
      "Epoch 957/1000 | Train Loss: 0.1035 Acc: 0.9576 | Val Loss: 0.1278 Acc: 0.9302 | Mem: 792.88MB | Speed: 1165.88 samples/s\n",
      "Epoch 958/1000 | Train Loss: 0.0980 Acc: 0.9647 | Val Loss: 0.1278 Acc: 0.9419 | Mem: 792.88MB | Speed: 1193.50 samples/s\n",
      "Epoch 959/1000 | Train Loss: 0.0962 Acc: 0.9706 | Val Loss: 0.1232 Acc: 0.9419 | Mem: 792.88MB | Speed: 1172.60 samples/s\n",
      "Epoch 960/1000 | Train Loss: 0.0814 Acc: 0.9706 | Val Loss: 0.1194 Acc: 0.9302 | Mem: 792.88MB | Speed: 1178.41 samples/s\n",
      "Epoch 961/1000 | Train Loss: 0.1089 Acc: 0.9576 | Val Loss: 0.1339 Acc: 0.9419 | Mem: 792.88MB | Speed: 1167.82 samples/s\n",
      "Epoch 962/1000 | Train Loss: 0.1082 Acc: 0.9553 | Val Loss: 0.1593 Acc: 0.9302 | Mem: 792.88MB | Speed: 1164.88 samples/s\n",
      "Epoch 963/1000 | Train Loss: 0.0797 Acc: 0.9753 | Val Loss: 0.1764 Acc: 0.9186 | Mem: 792.88MB | Speed: 1176.48 samples/s\n",
      "Epoch 964/1000 | Train Loss: 0.1316 Acc: 0.9494 | Val Loss: 0.1743 Acc: 0.9186 | Mem: 792.88MB | Speed: 1221.31 samples/s\n",
      "Epoch 965/1000 | Train Loss: 0.0845 Acc: 0.9612 | Val Loss: 0.1535 Acc: 0.9302 | Mem: 792.88MB | Speed: 1174.39 samples/s\n",
      "Epoch 966/1000 | Train Loss: 0.0984 Acc: 0.9682 | Val Loss: 0.1498 Acc: 0.9186 | Mem: 792.88MB | Speed: 1171.09 samples/s\n",
      "Epoch 967/1000 | Train Loss: 0.1064 Acc: 0.9588 | Val Loss: 0.1381 Acc: 0.9302 | Mem: 792.88MB | Speed: 1176.32 samples/s\n",
      "Epoch 968/1000 | Train Loss: 0.0726 Acc: 0.9741 | Val Loss: 0.1225 Acc: 0.9419 | Mem: 792.88MB | Speed: 1194.32 samples/s\n",
      "Epoch 969/1000 | Train Loss: 0.1105 Acc: 0.9600 | Val Loss: 0.1264 Acc: 0.9419 | Mem: 792.88MB | Speed: 1202.44 samples/s\n",
      "Epoch 970/1000 | Train Loss: 0.1122 Acc: 0.9541 | Val Loss: 0.1307 Acc: 0.9419 | Mem: 792.88MB | Speed: 1181.01 samples/s\n",
      "Epoch 971/1000 | Train Loss: 0.0932 Acc: 0.9659 | Val Loss: 0.1248 Acc: 0.9419 | Mem: 792.88MB | Speed: 1180.41 samples/s\n",
      "Epoch 972/1000 | Train Loss: 0.1002 Acc: 0.9576 | Val Loss: 0.1189 Acc: 0.9419 | Mem: 792.88MB | Speed: 1180.38 samples/s\n",
      "Epoch 973/1000 | Train Loss: 0.1168 Acc: 0.9612 | Val Loss: 0.1119 Acc: 0.9651 | Mem: 792.88MB | Speed: 1165.77 samples/s\n",
      "Epoch 974/1000 | Train Loss: 0.1052 Acc: 0.9635 | Val Loss: 0.1126 Acc: 0.9535 | Mem: 792.88MB | Speed: 1169.95 samples/s\n",
      "Epoch 975/1000 | Train Loss: 0.1010 Acc: 0.9588 | Val Loss: 0.1274 Acc: 0.9302 | Mem: 792.88MB | Speed: 1162.21 samples/s\n",
      "Epoch 976/1000 | Train Loss: 0.0891 Acc: 0.9671 | Val Loss: 0.1502 Acc: 0.9186 | Mem: 792.88MB | Speed: 1165.45 samples/s\n",
      "Epoch 977/1000 | Train Loss: 0.1109 Acc: 0.9529 | Val Loss: 0.1398 Acc: 0.9186 | Mem: 792.88MB | Speed: 1165.51 samples/s\n",
      "Epoch 978/1000 | Train Loss: 0.1002 Acc: 0.9612 | Val Loss: 0.1113 Acc: 0.9419 | Mem: 792.88MB | Speed: 1184.93 samples/s\n",
      "Epoch 979/1000 | Train Loss: 0.1215 Acc: 0.9576 | Val Loss: 0.0956 Acc: 0.9535 | Mem: 792.88MB | Speed: 1170.98 samples/s\n",
      "Epoch 980/1000 | Train Loss: 0.1081 Acc: 0.9588 | Val Loss: 0.1007 Acc: 0.9535 | Mem: 792.88MB | Speed: 1205.36 samples/s\n",
      "Epoch 981/1000 | Train Loss: 0.0891 Acc: 0.9694 | Val Loss: 0.1115 Acc: 0.9302 | Mem: 792.88MB | Speed: 1174.24 samples/s\n",
      "Epoch 982/1000 | Train Loss: 0.0908 Acc: 0.9694 | Val Loss: 0.1105 Acc: 0.9419 | Mem: 792.88MB | Speed: 1182.68 samples/s\n",
      "Epoch 983/1000 | Train Loss: 0.0881 Acc: 0.9682 | Val Loss: 0.1033 Acc: 0.9419 | Mem: 792.88MB | Speed: 1182.98 samples/s\n",
      "Epoch 984/1000 | Train Loss: 0.1053 Acc: 0.9624 | Val Loss: 0.1045 Acc: 0.9535 | Mem: 792.88MB | Speed: 1185.43 samples/s\n",
      "Epoch 985/1000 | Train Loss: 0.1038 Acc: 0.9635 | Val Loss: 0.1247 Acc: 0.9186 | Mem: 792.88MB | Speed: 1196.87 samples/s\n",
      "Epoch 986/1000 | Train Loss: 0.0936 Acc: 0.9612 | Val Loss: 0.1428 Acc: 0.9186 | Mem: 792.88MB | Speed: 1183.00 samples/s\n",
      "Epoch 987/1000 | Train Loss: 0.1016 Acc: 0.9624 | Val Loss: 0.1171 Acc: 0.9419 | Mem: 792.88MB | Speed: 1188.54 samples/s\n",
      "Epoch 988/1000 | Train Loss: 0.0982 Acc: 0.9682 | Val Loss: 0.1054 Acc: 0.9419 | Mem: 792.88MB | Speed: 1193.92 samples/s\n",
      "Epoch 989/1000 | Train Loss: 0.1297 Acc: 0.9518 | Val Loss: 0.1054 Acc: 0.9419 | Mem: 792.88MB | Speed: 1172.51 samples/s\n",
      "Epoch 990/1000 | Train Loss: 0.1068 Acc: 0.9647 | Val Loss: 0.1185 Acc: 0.9302 | Mem: 792.88MB | Speed: 1172.04 samples/s\n",
      "Epoch 991/1000 | Train Loss: 0.0869 Acc: 0.9729 | Val Loss: 0.1112 Acc: 0.9419 | Mem: 792.88MB | Speed: 1165.18 samples/s\n",
      "Epoch 992/1000 | Train Loss: 0.1111 Acc: 0.9600 | Val Loss: 0.1117 Acc: 0.9302 | Mem: 792.88MB | Speed: 1184.93 samples/s\n",
      "Epoch 993/1000 | Train Loss: 0.0837 Acc: 0.9671 | Val Loss: 0.1231 Acc: 0.9419 | Mem: 792.88MB | Speed: 1175.71 samples/s\n",
      "Epoch 994/1000 | Train Loss: 0.1141 Acc: 0.9612 | Val Loss: 0.1281 Acc: 0.9186 | Mem: 792.88MB | Speed: 1156.71 samples/s\n",
      "Epoch 995/1000 | Train Loss: 0.1012 Acc: 0.9612 | Val Loss: 0.1199 Acc: 0.9302 | Mem: 792.88MB | Speed: 1192.76 samples/s\n",
      "Epoch 996/1000 | Train Loss: 0.1011 Acc: 0.9588 | Val Loss: 0.1203 Acc: 0.9302 | Mem: 792.88MB | Speed: 1149.64 samples/s\n",
      "Epoch 997/1000 | Train Loss: 0.1303 Acc: 0.9529 | Val Loss: 0.1092 Acc: 0.9302 | Mem: 792.88MB | Speed: 1163.74 samples/s\n",
      "Epoch 998/1000 | Train Loss: 0.0973 Acc: 0.9647 | Val Loss: 0.1255 Acc: 0.9302 | Mem: 792.88MB | Speed: 1163.69 samples/s\n",
      "Epoch 999/1000 | Train Loss: 0.0749 Acc: 0.9729 | Val Loss: 0.1448 Acc: 0.9302 | Mem: 792.88MB | Speed: 1147.39 samples/s\n",
      "Epoch 1000/1000 | Train Loss: 0.1232 Acc: 0.9471 | Val Loss: 0.1373 Acc: 0.9302 | Mem: 792.88MB | Speed: 1144.53 samples/s\n",
      "Subject 1 final accuracy: 0.8646\n",
      "seed is 1196\n",
      "Subject 2\n",
      "Epoch 1/1000 | Train Loss: 1.5414 Acc: 0.2776 | Val Loss: 1.3103 Acc: 0.3488 | Mem: 793.02MB | Speed: 1195.86 samples/s\n",
      "Epoch 2/1000 | Train Loss: 1.5351 Acc: 0.2765 | Val Loss: 1.2614 Acc: 0.3488 | Mem: 793.02MB | Speed: 1213.86 samples/s\n",
      "Epoch 3/1000 | Train Loss: 1.4885 Acc: 0.3035 | Val Loss: 1.2308 Acc: 0.3721 | Mem: 793.02MB | Speed: 1201.94 samples/s\n",
      "Epoch 4/1000 | Train Loss: 1.3530 Acc: 0.3776 | Val Loss: 1.2168 Acc: 0.3837 | Mem: 793.02MB | Speed: 1218.98 samples/s\n",
      "Epoch 5/1000 | Train Loss: 1.3793 Acc: 0.3400 | Val Loss: 1.2097 Acc: 0.4302 | Mem: 793.02MB | Speed: 1202.21 samples/s\n",
      "Epoch 6/1000 | Train Loss: 1.3313 Acc: 0.3682 | Val Loss: 1.1882 Acc: 0.4419 | Mem: 793.02MB | Speed: 1213.66 samples/s\n",
      "Epoch 7/1000 | Train Loss: 1.3125 Acc: 0.3659 | Val Loss: 1.1634 Acc: 0.4651 | Mem: 793.02MB | Speed: 1211.23 samples/s\n",
      "Epoch 8/1000 | Train Loss: 1.3283 Acc: 0.3788 | Val Loss: 1.1418 Acc: 0.5116 | Mem: 793.02MB | Speed: 1222.40 samples/s\n",
      "Epoch 9/1000 | Train Loss: 1.2753 Acc: 0.4212 | Val Loss: 1.1212 Acc: 0.5349 | Mem: 793.02MB | Speed: 1241.07 samples/s\n",
      "Epoch 10/1000 | Train Loss: 1.2826 Acc: 0.3800 | Val Loss: 1.0975 Acc: 0.5349 | Mem: 793.02MB | Speed: 1216.08 samples/s\n",
      "Epoch 11/1000 | Train Loss: 1.2459 Acc: 0.4165 | Val Loss: 1.0819 Acc: 0.5465 | Mem: 793.02MB | Speed: 1237.54 samples/s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 429\u001b[0m\n\u001b[1;32m    426\u001b[0m RESULT_DIR \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCTNet_Mamba_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(time\u001b[38;5;241m.\u001b[39mtime())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    427\u001b[0m N_SUBJECT  \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m9\u001b[39m\n\u001b[0;32m--> 429\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRESULT_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDATA_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_SUBJECT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mCONFIG\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 405\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(result_dir, DATA_DIR, N_SUBJECT, **cfg)\u001b[0m\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSubject \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msub\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    404\u001b[0m     exp \u001b[38;5;241m=\u001b[39m ExP(sub, DATA_DIR, result_dir, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcfg)\n\u001b[0;32m--> 405\u001b[0m     accs\u001b[38;5;241m.\u001b[39mappend(\u001b[43mexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage accuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mmean(accs))\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m accs\n",
      "Cell \u001b[0;32mIn[1], line 312\u001b[0m, in \u001b[0;36mExP.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;66;03m# Backprop\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 312\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    315\u001b[0m \u001b[38;5;66;03m# Track metrics\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mamba_working/lib/python3.10/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mamba_working/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mamba_working/lib/python3.10/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "CTNet-Mamba: A Convolution-Mamba Network for EEG-Based Motor Imagery Classification\n",
    "\n",
    "author: zhaowei701@163.com\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.backends import cudnn\n",
    "from torchsummary import summary\n",
    "\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "from utils import load_data_evaluate, numberClassChannel, calMetrics\n",
    "\n",
    "# Try to import real MambaBlock (fallback if missing)\n",
    "try:\n",
    "    from mamba_ssm import Mamba\n",
    "except ImportError:\n",
    "    class Mamba(nn.Module):\n",
    "        def __init__(self, d_model, d_state=16, d_conv=4, expand=2):\n",
    "            super().__init__()\n",
    "            self.linear = nn.Linear(d_model, d_model)\n",
    "        def forward(self, x):\n",
    "            return self.linear(x)\n",
    "\n",
    "from zeta.nn import MambaBlock, FeedForward, MultiQueryAttention\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# PatchEmbeddingCNN\n",
    "# -----------------------------------------------------------------------------\n",
    "class PatchEmbeddingCNN(nn.Module):\n",
    "    def __init__(self, f1=16, kernel_size=64, D=2,\n",
    "                 pooling_size1=8, pooling_size2=8,\n",
    "                 dropout_rate=0.3, number_channel=22, emb_size=40):\n",
    "        super().__init__()\n",
    "        f2 = D * f1\n",
    "        self.cnn_module = nn.Sequential(\n",
    "            nn.Conv2d(1, f1, (1, kernel_size), padding='same', bias=False),\n",
    "            nn.BatchNorm2d(f1),\n",
    "            nn.Conv2d(f1, f2, (number_channel, 1), groups=f1, bias=False),\n",
    "            nn.BatchNorm2d(f2),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d((1, pooling_size1)),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Conv2d(f2, f2, (1, 16), padding='same', bias=False),\n",
    "            nn.BatchNorm2d(f2),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d((1, pooling_size2)),\n",
    "            nn.Dropout(dropout_rate),\n",
    "        )\n",
    "        self.projection = Rearrange('b e h w -> b (h w) e')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn_module(x)        # [B, f2, 1, seq]\n",
    "        return self.projection(x)     # [B, seq, f2]\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# LinearAttention\n",
    "# -----------------------------------------------------------------------------\n",
    "def exists(val): return val is not None\n",
    "\n",
    "class LinearAttention(nn.Module):\n",
    "    def __init__(self, dim, *, heads=4, dim_head=64, dropout=0.0):\n",
    "        super().__init__()\n",
    "        inner = heads * dim_head\n",
    "        self.heads, self.scale = heads, dim_head**-0.5\n",
    "        self.to_qkv = nn.Linear(dim, inner*3, bias=False)\n",
    "        self.to_out = nn.Sequential(nn.Linear(inner, dim), nn.Dropout(dropout))\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        h = self.heads\n",
    "        q,k,v = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        q,k,v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h=h), (q,k,v))\n",
    "        q, k = q*self.scale, k\n",
    "        q, k = q.softmax(dim=-1), k.softmax(dim=-2)\n",
    "        if exists(mask):\n",
    "            mask = rearrange(mask, 'b n -> (b h) n', h=h)\n",
    "            k = k.masked_fill(~mask.unsqueeze(-1), 0.)\n",
    "        ctx = torch.einsum('b n d, b n e -> b d e', q, k)\n",
    "        out = torch.einsum('b d e, b n d -> b n e', ctx, v)\n",
    "        out = rearrange(out, '(b h) n d -> b n (h d)', h=h)\n",
    "        return self.to_out(out)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# TransformerBlock\n",
    "# -----------------------------------------------------------------------------\n",
    "class MambaTransformerblock(nn.Module):\n",
    "    def __init__(self, dim, heads, dim_head, dropout=0.1, \n",
    "                 ff_mult=4, d_state=16, depth=4):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            self._create_layer(dim, heads, dim_head, dropout, ff_mult, d_state)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "    def _create_layer(self, dim, heads, dim_head, dropout, ff_mult, d_state):\n",
    "        return nn.ModuleDict({\n",
    "            # Mamba components\n",
    "            'mamba_norm': nn.LayerNorm(dim),\n",
    "            'mamba': MambaBlock(dim, d_state=d_state),\n",
    "            'mamba_dropout': nn.Dropout(dropout),\n",
    "            \n",
    "            # Linear attention components\n",
    "            'attn_norm': nn.LayerNorm(dim),\n",
    "            'linear_attn': LinearAttention(dim, heads=heads, dim_head=dim_head, dropout=dropout),\n",
    "            'attn_dropout': nn.Dropout(dropout),\n",
    "            \n",
    "            # FFN components\n",
    "            'ffn_norm': nn.LayerNorm(dim),\n",
    "            'feedforward': nn.Sequential(\n",
    "                nn.Linear(dim, ff_mult * dim),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(ff_mult * dim, dim)\n",
    "            ),\n",
    "            'ffn_dropout': nn.Dropout(dropout)\n",
    "        })\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        for layer in self.layers:\n",
    "            # (1) Mamba path\n",
    "            mamba_norm = layer['mamba_norm']\n",
    "            mamba_out = mamba_norm(x)\n",
    "            mamba_out = layer['mamba'](mamba_out)\n",
    "            x = x + layer['mamba_dropout'](mamba_out)\n",
    "            \n",
    "            # (2) Linear attention path\n",
    "            attn_norm = layer['attn_norm']\n",
    "            attn_out = attn_norm(x)\n",
    "            attn_out = layer['linear_attn'](attn_out)\n",
    "            x = x + layer['attn_dropout'](attn_out)\n",
    "            \n",
    "            # (3) FFN path\n",
    "            ffn_norm = layer['ffn_norm']\n",
    "            ffn_out = ffn_norm(x)\n",
    "            ffn_out = layer['feedforward'](ffn_out)\n",
    "            x = x + layer['ffn_dropout'](ffn_out)\n",
    "            \n",
    "        return self.norm(x)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# EEGMambaTransformer\n",
    "# -----------------------------------------------------------------------------\n",
    "class EEGMambaTransformer(nn.Module):\n",
    "    def __init__(self, emb_size=40, depth=6, heads=4,\n",
    "                 d_state=16, transformer_depth=1, mamba_depth=3,\n",
    "                 database_type='A', eeg1_f1=8, eeg1_kernel_size=64,\n",
    "                 eeg1_D=2, eeg1_pooling_size1=8, eeg1_pooling_size2=8,\n",
    "                 eeg1_dropout_rate=0.5, flatten_eeg1=15*16):\n",
    "        super().__init__()\n",
    "        self.number_class, self.number_channel = numberClassChannel(database_type)\n",
    "\n",
    "        self.cnn = PatchEmbeddingCNN(\n",
    "            f1=eeg1_f1, kernel_size=eeg1_kernel_size,\n",
    "            D=eeg1_D, pooling_size1=eeg1_pooling_size1,\n",
    "            pooling_size2=eeg1_pooling_size2,\n",
    "            dropout_rate=eeg1_dropout_rate,\n",
    "            number_channel=self.number_channel,\n",
    "            emb_size=emb_size\n",
    "        )\n",
    "\n",
    "        dim_head = emb_size // heads\n",
    "\n",
    "        self.mamba_transformer = MambaTransformerblock(\n",
    "            dim=emb_size, heads=heads,\n",
    "            dim_head=dim_head,\n",
    "            dropout=eeg1_dropout_rate, ff_mult=4,\n",
    "            d_state=d_state,\n",
    "            depth = depth,\n",
    "        )\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.classification = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(flatten_eeg1, self.number_class)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)                       # [B, seq, emb]\n",
    "        x = x * np.sqrt(x.shape[-1])\n",
    "        x = self.mamba_transformer(x)\n",
    "        return x, self.classification(self.flatten(x))\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Experiment wrapper\n",
    "# -----------------------------------------------------------------------------\n",
    "class ExP:\n",
    "    def __init__(self, nsub, data_dir, result_name,\n",
    "                 epochs=300, number_aug=3, number_seg=8,\n",
    "                 evaluate_mode='subject-dependent',\n",
    "                 heads=2, emb_size=16, depth=6,\n",
    "                 d_state=16, transformer_depth=1, mamba_depth=3,\n",
    "                 dataset_type='A', eeg1_f1=8, eeg1_kernel_size=64,\n",
    "                 eeg1_D=2, eeg1_pooling_size1=8, eeg1_pooling_size2=8,\n",
    "                 eeg1_dropout_rate=0.5, flatten_eeg1=15*16,\n",
    "                 validate_ratio=0.3, learning_rate=1e-3, batch_size=72):\n",
    "        self.nSub = nsub\n",
    "        self.dataset_type = dataset_type\n",
    "        self.data_dir = data_dir\n",
    "        self.result_name = result_name\n",
    "        self.n_epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.validate_ratio = validate_ratio\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "        self.model = EEGMambaTransformer(\n",
    "            emb_size=emb_size, depth=depth, heads=heads,\n",
    "            d_state=d_state,\n",
    "            transformer_depth=transformer_depth,\n",
    "            mamba_depth=mamba_depth,\n",
    "            database_type=dataset_type,\n",
    "            eeg1_f1=eeg1_f1, eeg1_kernel_size=eeg1_kernel_size,\n",
    "            eeg1_D=eeg1_D, eeg1_pooling_size1=eeg1_pooling_size1,\n",
    "            eeg1_pooling_size2=eeg1_pooling_size2,\n",
    "            eeg1_dropout_rate=eeg1_dropout_rate,\n",
    "            flatten_eeg1=flatten_eeg1\n",
    "        ).cuda()\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        os.makedirs(self.result_name, exist_ok=True)\n",
    "        self.model_filename = os.path.join(self.result_name, f\"model_{self.nSub}.pth\")\n",
    "\n",
    "    def interaug(self, timg, label):\n",
    "        aug_data, aug_label = [], []\n",
    "        recs = 3 * (self.batch_size // self.model.number_class)\n",
    "        segpts = 1000 // 8\n",
    "        for cls in range(self.model.number_class):\n",
    "            idx = np.where(label == cls + 1)\n",
    "            data, lbl = timg[idx], label[idx]\n",
    "            tmp = np.zeros((recs,1,self.model.number_channel,1000), dtype=np.float32)\n",
    "            for i in range(recs):\n",
    "                for j in range(8):\n",
    "                    ridx = random.randrange(data.shape[0])\n",
    "                    tmp[i,0,:,j*segpts:(j+1)*segpts] = data[ridx,0,:,j*segpts:(j+1)*segpts]\n",
    "            aug_data.append(tmp); aug_label.append(lbl[:recs])\n",
    "        aug_data = np.concatenate(aug_data).astype(np.float32)\n",
    "        aug_label = np.concatenate(aug_label)\n",
    "        perm = np.random.permutation(len(aug_data))\n",
    "        return (\n",
    "            torch.from_numpy(aug_data[perm]).cuda(),\n",
    "            torch.from_numpy((aug_label[perm]-1)).long().cuda()\n",
    "        )\n",
    "\n",
    "    def get_source_data(self):\n",
    "        tr_x, tr_y, te_x, te_y = load_data_evaluate(\n",
    "            self.data_dir, self.dataset_type, self.nSub,\n",
    "            mode_evaluate='subject-dependent'\n",
    "        )\n",
    "        tr_x = np.expand_dims(tr_x, axis=1).astype(np.float32)\n",
    "        te_x = np.expand_dims(te_x, axis=1).astype(np.float32)\n",
    "        tr_y = tr_y.reshape(-1)\n",
    "        te_y = te_y.reshape(-1)\n",
    "        m, s = tr_x.mean(), tr_x.std()\n",
    "        tr_x = (tr_x - m) / s\n",
    "        te_x = (te_x - m) / s\n",
    "        return tr_x, tr_y, te_x, te_y\n",
    "\n",
    "    def train(self):\n",
    "        tr_x, tr_y, te_x, te_y = self.get_source_data()\n",
    "    \n",
    "        # Create validation split\n",
    "        dataset_size = len(tr_x)\n",
    "        val_size = int(self.validate_ratio * dataset_size)\n",
    "        train_size = dataset_size - val_size\n",
    "        train_ds, val_ds = torch.utils.data.random_split(\n",
    "            torch.utils.data.TensorDataset(torch.from_numpy(tr_x), torch.from_numpy(tr_y-1)),\n",
    "            [train_size, val_size]\n",
    "        )\n",
    "    \n",
    "        test_ds = torch.utils.data.TensorDataset(torch.from_numpy(te_x), torch.from_numpy(te_y-1))\n",
    "        self.test_loader = torch.utils.data.DataLoader(test_ds, batch_size=self.batch_size)\n",
    "    \n",
    "        best_loss = float('inf')\n",
    "        for e in range(self.n_epochs):\n",
    "            # Training\n",
    "            self.model.train()\n",
    "            train_loader = torch.utils.data.DataLoader(train_ds, batch_size=self.batch_size, shuffle=True)\n",
    "            \n",
    "            # Initialize training metrics\n",
    "            epoch_train_loss = 0.0\n",
    "            train_correct = 0\n",
    "            total_samples = 0\n",
    "            \n",
    "            # Start timing and memory tracking\n",
    "            torch.cuda.synchronize()\n",
    "            start_time = time.time()\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            \n",
    "            for xb, yb in train_loader:\n",
    "                xb, yb = xb.cuda().float(), yb.cuda().long()\n",
    "                aug_x, aug_y = self.interaug(tr_x, tr_y)\n",
    "                xb = torch.cat([xb, aug_x])\n",
    "                yb = torch.cat([yb, aug_y])\n",
    "                \n",
    "                _, out = self.model(xb)\n",
    "                loss = self.criterion(out, yb)\n",
    "                \n",
    "                # Backprop\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                # Track metrics\n",
    "                epoch_train_loss += loss.item()\n",
    "                preds = out.argmax(dim=1)\n",
    "                train_correct += (preds == yb).sum().item()\n",
    "                total_samples += yb.size(0)\n",
    "    \n",
    "            # Calculate training metrics\n",
    "            train_loss = epoch_train_loss / len(train_loader)\n",
    "            train_acc = train_correct / total_samples\n",
    "            \n",
    "            # Calculate memory and speed\n",
    "            torch.cuda.synchronize()\n",
    "            elapsed = time.time() - start_time\n",
    "            mem_used = torch.cuda.max_memory_allocated() / (1024 ** 2)  # MB\n",
    "            speed = total_samples / elapsed if elapsed > 0 else 0\n",
    "    \n",
    "            # Validation\n",
    "            self.model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_correct = 0\n",
    "            with torch.no_grad():\n",
    "                val_loader = torch.utils.data.DataLoader(val_ds, batch_size=self.batch_size)\n",
    "                for xb, yb in val_loader:\n",
    "                    xb, yb = xb.cuda().float(), yb.cuda().long()\n",
    "                    _, out = self.model(xb)\n",
    "                    loss = self.criterion(out, yb)\n",
    "                    val_loss += loss.item()\n",
    "                    val_correct += (out.argmax(1) == yb).sum().item()\n",
    "    \n",
    "            val_loss = val_loss / len(val_loader)\n",
    "            val_acc = val_correct / len(val_ds)\n",
    "    \n",
    "            print(f\"Epoch {e+1}/{self.n_epochs} | \"\n",
    "                  f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | \"\n",
    "                  f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.4f} | \"\n",
    "                  f\"Mem: {mem_used:.2f}MB | Speed: {speed:.2f} samples/s\")\n",
    "    \n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                torch.save(self.model.state_dict(), self.model_filename)\n",
    "\n",
    "    # Rest of test evaluation...\n",
    "\n",
    "    # Rest of test evaluation remains the same...\n",
    "\n",
    "        self.model.load_state_dict(torch.load(self.model_filename))\n",
    "        self.model.eval()\n",
    "        preds, trues = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in self.test_loader:\n",
    "                xb = xb.cuda()\n",
    "                _, out = self.model(xb)\n",
    "                preds.append(out.argmax(1).cpu().numpy())\n",
    "                trues.append(yb.numpy())\n",
    "        preds = np.concatenate(preds); trues = np.concatenate(trues)\n",
    "        acc, *_ = calMetrics(trues, preds)\n",
    "        print(f\"Subject {self.nSub} final accuracy: {acc:.4f}\")\n",
    "        return acc\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "def main(result_dir, DATA_DIR, N_SUBJECT, **cfg):\n",
    "    os.makedirs(result_dir, exist_ok=True)\n",
    "\n",
    "    number_class, number_channel = numberClassChannel(cfg['dataset_type'])\n",
    "    model = EEGMambaTransformer(\n",
    "        emb_size=cfg['emb_size'], depth=cfg['depth'], heads=cfg['heads'],\n",
    "        d_state=cfg['d_state'],\n",
    "        transformer_depth=cfg['transformer_depth'],\n",
    "        mamba_depth=cfg['mamba_depth'],\n",
    "        database_type=cfg['dataset_type'],\n",
    "        eeg1_f1=cfg['eeg1_f1'], eeg1_kernel_size=cfg['eeg1_kernel_size'],\n",
    "        eeg1_D=cfg['eeg1_D'], eeg1_pooling_size1=cfg['eeg1_pooling_size1'],\n",
    "        eeg1_pooling_size2=cfg['eeg1_pooling_size2'],\n",
    "        eeg1_dropout_rate=cfg['eeg1_dropout_rate'],\n",
    "        flatten_eeg1=cfg['flatten_eeg1']\n",
    "    ).cuda()\n",
    "    summary(model, (1, number_channel, 1000))\n",
    "    print(time.asctime(time.localtime(time.time())))\n",
    "\n",
    "    accs = []\n",
    "    for sub in range(1, N_SUBJECT+1):\n",
    "        seed_n = np.random.randint(2024)\n",
    "        random.seed(seed_n); np.random.seed(seed_n)\n",
    "        torch.manual_seed(seed_n); torch.cuda.manual_seed(seed_n)\n",
    "        torch.cuda.manual_seed_all(seed_n)\n",
    "\n",
    "        print(f\"seed is {seed_n}\")\n",
    "        print(f\"Subject {sub}\")\n",
    "\n",
    "        exp = ExP(sub, DATA_DIR, result_dir, **cfg)\n",
    "        accs.append(exp.train())\n",
    "\n",
    "    print(\"Average accuracy:\", np.mean(accs))\n",
    "    return accs\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cudnn.benchmark = False\n",
    "    cudnn.deterministic = True\n",
    "\n",
    "    CONFIG = dict(\n",
    "        emb_size=16, depth=3, heads=2,\n",
    "        d_state=32, transformer_depth=6, mamba_depth=6,\n",
    "        dataset_type='A',\n",
    "        eeg1_f1=8, eeg1_kernel_size=64,\n",
    "        eeg1_D=2, eeg1_pooling_size1=8, eeg1_pooling_size2=8,\n",
    "        eeg1_dropout_rate=0.5, flatten_eeg1=15*16,\n",
    "        epochs=1000, number_aug=3, number_seg=8,\n",
    "        validate_ratio=0.3, learning_rate=1e-3, batch_size=72\n",
    "    )\n",
    "\n",
    "    DATA_DIR   = \"bci2a/\"\n",
    "    RESULT_DIR = f\"CTNet_Mamba_{int(time.time())}\"\n",
    "    N_SUBJECT  = 9\n",
    "\n",
    "    main(RESULT_DIR, DATA_DIR, N_SUBJECT, **CONFIG)\n",
    "    print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c0af06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops.layers.torch import Rearrange\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the CNN model\n",
    "class PatchEmbeddingCNN(nn.Module):\n",
    "    def __init__(self, f1=16, kernel_size=64, D=2, pooling_size1=8, pooling_size2=8, dropout_rate=0.3, number_channel=22, emb_size=40):\n",
    "        super().__init__()\n",
    "        f2 = D * f1\n",
    "        self.cnn_module = nn.Sequential(\n",
    "            # Temporal convolution (kernel size = 64)\n",
    "            nn.Conv2d(1, f1, (1, kernel_size), (1, 1), padding='same', bias=False), # [batch, 1, 22, 1000]\n",
    "            nn.BatchNorm2d(f1),\n",
    "            # Channel depth-wise convolution\n",
    "            nn.Conv2d(f1, f2, (number_channel, 1), (1, 1), groups=f1, padding='valid', bias=False), \n",
    "            nn.BatchNorm2d(f2),\n",
    "            nn.ELU(),\n",
    "            # Average pooling to obtain 'patches' along the time dimension (as in ViT)\n",
    "            nn.AvgPool2d((1, pooling_size1)),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            # Additional spatial convolution\n",
    "            nn.Conv2d(f2, f2, (1, 16), padding='same', bias=False), \n",
    "            nn.BatchNorm2d(f2),\n",
    "            nn.ELU(),\n",
    "            # Final pooling\n",
    "            nn.AvgPool2d((1, pooling_size2)),\n",
    "            nn.Dropout(dropout_rate),\n",
    "        )\n",
    "\n",
    "        # Projection to reshape the tensor to match the desired output format\n",
    "        self.projection = nn.Sequential(\n",
    "            Rearrange('b e h w -> b (h w) e'),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        b, _, _, _ = x.shape\n",
    "        x = self.cnn_module(x)  # Pass through CNN layers\n",
    "        x = self.projection(x)   # Flatten and rearrange to (batch, seq_len, emb_size)\n",
    "        return x\n",
    "\n",
    "# Create a dummy input tensor (example: 288 samples, 1 channel, 22 features, 1000 time steps)\n",
    "input_data = torch.randn(1, 1, 22, 1000)  # Random example data\n",
    "\n",
    "# Initialize the CNN model\n",
    "cnn_model = PatchEmbeddingCNN(f1=8, kernel_size=64, D=2, pooling_size1=8, pooling_size2=8, dropout_rate=0.3, number_channel=22, emb_size=40)\n",
    "\n",
    "# Pass the input data through the CNN model\n",
    "output_data = cnn_model(input_data)\n",
    "\n",
    "# Print the shape of the output data\n",
    "print(f\"Input data shape: {input_data.shape}\")\n",
    "print(f\"Output data shape after CNN: {output_data.shape}\")\n",
    "print(output_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1f72b7-6945-48be-9251-3d4b1233790a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.max_memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5873853d-4e62-4886-9f2f-137de970b476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from mamba_transformer import MambaTransformer\n",
    "# Create an instance of the MambaTransformer model\n",
    "model = MambaTransformer(\n",
    "    num_tokens=100,  # Number of tokens in the input sequence\n",
    "    dim=512,  # Dimension of the model\n",
    "    heads=8,  # Number of attention heads\n",
    "    depth=4,  # Number of transformer layers\n",
    "    dim_head=64,  # Dimension of each attention head\n",
    "    d_state=512,  # Dimension of the state\n",
    "    dropout=0.1,  # Dropout rate\n",
    "    ff_mult=4,  # Multiplier for the feed-forward layer dimension\n",
    "    return_embeddings=False,  # Whether to return the embeddings,\n",
    "    transformer_depth=2,  # Number of transformer blocks\n",
    "    mamba_depth=10,  # Number of Mamba blocks,\n",
    "    use_linear_attn=True,  # Whether to use linear attention\n",
    ")\n",
    "\n",
    "# Pass the input tensor through the model and print the output shape\n",
    "output_data = output_data.long()  # Convert the tensor to Long type\n",
    "\n",
    "out = model(output_data)\n",
    "\n",
    "print(out.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
