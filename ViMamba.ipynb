{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "754d462e-f4e8-4b4f-93ab-e7961b13cf9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smartview/miniconda3/envs/vim_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embedding: torch.Size([1, 16, 256])\n",
      "Cls tokens: torch.Size([1, 1, 256])\n",
      "torch.Size([1, 16, 256])\n",
      "Conv1d: tensor([[[0.5589, 0.5844, 0.5376,  ..., 0.6169, 0.6532, 0.6044],\n",
      "         [0.6766, 0.6771, 0.6502,  ..., 0.6393, 0.7033, 0.6912],\n",
      "         [0.5860, 0.6117, 0.6059,  ..., 0.7396, 0.5281, 0.5852],\n",
      "         ...,\n",
      "         [0.8196, 0.9678, 0.9183,  ..., 0.7591, 1.0497, 0.8515],\n",
      "         [0.6529, 0.7233, 0.7075,  ..., 0.6982, 0.7302, 0.7037],\n",
      "         [0.6631, 0.6507, 0.7012,  ..., 0.7229, 0.6045, 0.6367]]],\n",
      "       device='cuda:0')\n",
      "Conv1d: tensor([[[0.7498, 0.8072, 0.7791,  ..., 0.6276, 0.8242, 0.7291],\n",
      "         [0.8065, 0.7854, 0.8366,  ..., 0.8757, 0.9061, 0.7766],\n",
      "         [0.7791, 0.7467, 0.7335,  ..., 0.7866, 0.6657, 0.7630],\n",
      "         ...,\n",
      "         [0.6508, 0.6218, 0.7448,  ..., 0.7284, 0.6653, 0.7428],\n",
      "         [0.8520, 0.7383, 0.8200,  ..., 0.8150, 0.7878, 0.9361],\n",
      "         [0.7867, 0.6099, 0.7925,  ..., 0.6493, 0.6867, 0.7017]]],\n",
      "       device='cuda:0')\n",
      "Layer: torch.Size([1, 16, 256])\n",
      "Conv1d: tensor([[[0.5954, 0.6895, 0.5760,  ..., 0.5459, 0.6627, 0.6699],\n",
      "         [0.5987, 0.5139, 0.5957,  ..., 0.5295, 0.5569, 0.5143],\n",
      "         [0.5165, 0.6550, 0.4636,  ..., 0.4702, 0.5244, 0.5519],\n",
      "         ...,\n",
      "         [0.7214, 0.7841, 0.6375,  ..., 0.6759, 0.7360, 0.6992],\n",
      "         [0.6705, 0.7236, 0.7083,  ..., 0.6039, 0.6794, 0.5536],\n",
      "         [0.6383, 0.6714, 0.7168,  ..., 0.6948, 0.7803, 0.8128]]],\n",
      "       device='cuda:0')\n",
      "Conv1d: tensor([[[0.8051, 0.7471, 0.7391,  ..., 0.6857, 0.6967, 0.7649],\n",
      "         [0.5114, 0.5032, 0.4597,  ..., 0.5569, 0.4983, 0.4744],\n",
      "         [0.5994, 0.6098, 0.6730,  ..., 0.7220, 0.5465, 0.5068],\n",
      "         ...,\n",
      "         [0.8443, 0.7221, 0.9188,  ..., 0.8696, 0.9212, 0.7446],\n",
      "         [0.7385, 0.7145, 0.7841,  ..., 0.8046, 0.7451, 0.8221],\n",
      "         [0.7382, 0.7579, 0.7716,  ..., 0.7597, 0.8102, 0.7797]]],\n",
      "       device='cuda:0')\n",
      "Layer: torch.Size([1, 16, 256])\n",
      "Conv1d: tensor([[[0.5824, 0.5870, 0.5802,  ..., 0.5770, 0.5130, 0.6113],\n",
      "         [0.8252, 0.7942, 0.9000,  ..., 0.9107, 0.9769, 0.8582],\n",
      "         [0.4451, 0.3747, 0.5803,  ..., 0.4916, 0.3965, 0.4874],\n",
      "         ...,\n",
      "         [0.6641, 0.7783, 0.6760,  ..., 0.7150, 0.6887, 0.7358],\n",
      "         [0.4688, 0.3424, 0.4122,  ..., 0.4134, 0.4176, 0.4242],\n",
      "         [0.8889, 0.8302, 0.8015,  ..., 0.9186, 0.7790, 0.8057]]],\n",
      "       device='cuda:0')\n",
      "Conv1d: tensor([[[0.5887, 0.5677, 0.5284,  ..., 0.6662, 0.5906, 0.6717],\n",
      "         [0.7905, 0.9013, 0.7384,  ..., 0.8094, 0.8677, 0.9443],\n",
      "         [0.7723, 0.9790, 0.9069,  ..., 0.9048, 0.8296, 0.8320],\n",
      "         ...,\n",
      "         [0.6651, 0.6936, 0.7304,  ..., 0.7062, 0.7568, 0.6592],\n",
      "         [0.8445, 0.8398, 0.6614,  ..., 0.8229, 0.7504, 0.7628],\n",
      "         [0.6667, 0.6530, 0.8027,  ..., 0.6675, 0.7756, 0.6769]]],\n",
      "       device='cuda:0')\n",
      "Layer: torch.Size([1, 16, 256])\n",
      "Conv1d: tensor([[[0.4864, 0.4355, 0.5105,  ..., 0.4679, 0.4646, 0.4633],\n",
      "         [1.0184, 0.9757, 0.9182,  ..., 0.9910, 1.0370, 0.9375],\n",
      "         [0.5309, 0.5953, 0.5946,  ..., 0.5460, 0.5804, 0.5872],\n",
      "         ...,\n",
      "         [0.5670, 0.5735, 0.5303,  ..., 0.5575, 0.5802, 0.5353],\n",
      "         [0.9099, 0.8840, 0.9574,  ..., 0.9065, 0.9218, 0.7927],\n",
      "         [0.7752, 0.7362, 0.7713,  ..., 0.7426, 0.6882, 0.7527]]],\n",
      "       device='cuda:0')\n",
      "Conv1d: tensor([[[0.5665, 0.6550, 0.5186,  ..., 0.5411, 0.5934, 0.5504],\n",
      "         [0.7526, 0.8291, 0.7506,  ..., 0.6362, 0.7677, 0.6393],\n",
      "         [0.5201, 0.3650, 0.5124,  ..., 0.5062, 0.4352, 0.5345],\n",
      "         ...,\n",
      "         [0.9311, 1.0148, 0.9056,  ..., 0.8434, 1.0106, 0.8208],\n",
      "         [0.4981, 0.7752, 0.5036,  ..., 0.6102, 0.5727, 0.5411],\n",
      "         [0.7779, 0.7674, 0.8483,  ..., 0.8906, 0.7899, 0.7381]]],\n",
      "       device='cuda:0')\n",
      "Layer: torch.Size([1, 16, 256])\n",
      "Conv1d: tensor([[[0.5174, 0.5493, 0.5415,  ..., 0.5483, 0.5055, 0.5191],\n",
      "         [0.9409, 0.9064, 0.8660,  ..., 0.8605, 0.9267, 0.8525],\n",
      "         [0.9831, 0.9483, 0.9830,  ..., 0.9390, 0.9444, 1.0368],\n",
      "         ...,\n",
      "         [1.0979, 0.9449, 1.1126,  ..., 0.9999, 1.0932, 1.0764],\n",
      "         [1.1720, 0.9689, 1.1813,  ..., 1.1000, 0.9934, 1.0805],\n",
      "         [0.8008, 0.7944, 0.7547,  ..., 0.7002, 0.6934, 0.6613]]],\n",
      "       device='cuda:0')\n",
      "Conv1d: tensor([[[0.4408, 0.3180, 0.4361,  ..., 0.4612, 0.3955, 0.4249],\n",
      "         [0.7867, 0.9644, 0.8002,  ..., 0.8518, 0.8374, 0.8349],\n",
      "         [0.7116, 0.8058, 0.5936,  ..., 0.5969, 0.6541, 0.8511],\n",
      "         ...,\n",
      "         [0.8795, 0.9601, 0.9574,  ..., 0.9183, 1.0369, 0.9155],\n",
      "         [0.6981, 0.8197, 0.6113,  ..., 0.6435, 0.8925, 0.7002],\n",
      "         [0.7316, 0.7525, 0.7677,  ..., 0.8033, 0.8958, 0.7296]]],\n",
      "       device='cuda:0')\n",
      "Layer: torch.Size([1, 16, 256])\n",
      "Conv1d: tensor([[[0.6562, 0.5732, 0.6941,  ..., 0.7068, 0.7043, 0.5509],\n",
      "         [0.5837, 0.6491, 0.4795,  ..., 0.5030, 0.5842, 0.7175],\n",
      "         [0.7590, 0.7053, 0.7232,  ..., 0.7369, 0.7543, 0.7015],\n",
      "         ...,\n",
      "         [0.4376, 0.4187, 0.3353,  ..., 0.4620, 0.4384, 0.5171],\n",
      "         [0.6962, 0.6003, 0.6390,  ..., 0.7460, 0.5690, 0.7037],\n",
      "         [0.7163, 0.6743, 0.7626,  ..., 0.7129, 0.6593, 0.6442]]],\n",
      "       device='cuda:0')\n",
      "Conv1d: tensor([[[0.5562, 0.6136, 0.5906,  ..., 0.5652, 0.5512, 0.5518],\n",
      "         [0.8309, 0.9204, 1.0114,  ..., 0.8634, 0.9881, 0.6656],\n",
      "         [0.6848, 0.6546, 0.7103,  ..., 0.7301, 0.7119, 0.6262],\n",
      "         ...,\n",
      "         [0.6779, 0.6410, 0.5968,  ..., 0.6530, 0.6766, 0.5514],\n",
      "         [1.1896, 1.0906, 1.0730,  ..., 0.9912, 1.2889, 1.1097],\n",
      "         [0.8534, 0.7982, 1.0099,  ..., 0.9277, 0.8803, 0.8431]]],\n",
      "       device='cuda:0')\n",
      "Layer: torch.Size([1, 16, 256])\n",
      "Wed Apr 30 07:21:30 2025\n",
      "seed is 509\n",
      "Subject 1\n",
      "Patch embedding: torch.Size([288, 16, 256])\n",
      "Cls tokens: torch.Size([288, 1, 256])\n",
      "torch.Size([288, 16, 256])\n",
      "Conv1d: tensor([[[0.5185, 0.9797, 0.5080,  ..., 0.7757, 0.8247, 0.4739],\n",
      "         [0.4818, 0.6891, 0.5452,  ..., 0.8391, 0.5002, 0.7244],\n",
      "         [0.8000, 0.8385, 1.0233,  ..., 0.8559, 0.9606, 0.5590],\n",
      "         ...,\n",
      "         [0.7561, 0.7755, 0.9814,  ..., 0.4690, 1.0547, 1.0399],\n",
      "         [0.4560, 0.8757, 1.0951,  ..., 0.9364, 1.0137, 0.6824],\n",
      "         [0.8698, 0.5519, 0.7500,  ..., 0.5854, 0.5616, 0.5941]],\n",
      "\n",
      "        [[0.6274, 0.7184, 0.5665,  ..., 0.7606, 0.6984, 0.9155],\n",
      "         [0.7705, 0.5744, 0.5643,  ..., 0.6674, 0.5229, 0.7386],\n",
      "         [0.7696, 0.6765, 0.9815,  ..., 0.6057, 0.6207, 0.6146],\n",
      "         ...,\n",
      "         [0.8509, 0.7648, 1.0355,  ..., 0.6566, 0.9070, 0.5308],\n",
      "         [0.9252, 0.7812, 0.9193,  ..., 0.7099, 0.5839, 0.6055],\n",
      "         [0.6014, 0.8445, 0.4612,  ..., 0.7399, 0.6149, 0.4488]],\n",
      "\n",
      "        [[0.7002, 0.7459, 0.8613,  ..., 0.9123, 0.8967, 0.7396],\n",
      "         [0.5671, 0.6651, 1.0453,  ..., 0.6606, 1.0124, 0.6542],\n",
      "         [0.6831, 0.8345, 0.6108,  ..., 0.6065, 0.6885, 0.5842],\n",
      "         ...,\n",
      "         [0.5233, 0.5991, 0.7280,  ..., 0.5176, 0.8243, 0.5937],\n",
      "         [0.7179, 1.0244, 0.9482,  ..., 0.8862, 0.9518, 0.6260],\n",
      "         [0.8763, 0.8328, 0.4037,  ..., 0.6735, 0.4752, 0.6675]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.5226, 0.6892, 0.8850,  ..., 0.7138, 0.8198, 0.9311],\n",
      "         [0.5563, 0.7045, 0.9082,  ..., 0.6040, 0.7981, 0.5451],\n",
      "         [0.6273, 0.4844, 0.4345,  ..., 0.6247, 0.7881, 0.5302],\n",
      "         ...,\n",
      "         [1.0914, 0.6111, 0.5601,  ..., 0.6779, 0.4533, 0.5541],\n",
      "         [0.5577, 0.5384, 0.5821,  ..., 0.6221, 0.8593, 0.7575],\n",
      "         [0.7536, 0.7398, 0.5693,  ..., 0.7636, 0.7061, 0.5857]],\n",
      "\n",
      "        [[0.6715, 0.5885, 0.9537,  ..., 0.4480, 0.5954, 0.7561],\n",
      "         [0.7375, 0.8050, 0.8302,  ..., 0.5905, 0.8998, 0.6663],\n",
      "         [0.5149, 1.0789, 0.7200,  ..., 0.9894, 0.7775, 0.9908],\n",
      "         ...,\n",
      "         [0.5598, 0.7382, 0.6514,  ..., 0.7966, 0.9427, 0.6961],\n",
      "         [0.4679, 1.0432, 0.9255,  ..., 0.9736, 0.8601, 0.9306],\n",
      "         [0.7761, 0.5890, 0.3593,  ..., 0.8267, 0.4482, 0.5651]],\n",
      "\n",
      "        [[0.7628, 0.7493, 0.6853,  ..., 0.6942, 0.6444, 0.8063],\n",
      "         [0.5930, 0.7051, 0.7629,  ..., 0.8876, 0.7149, 0.6121],\n",
      "         [0.7252, 0.5847, 0.4878,  ..., 0.5758, 0.4092, 0.6217],\n",
      "         ...,\n",
      "         [0.5109, 0.6693, 0.7252,  ..., 0.7410, 0.7087, 0.4593],\n",
      "         [0.5863, 0.6394, 0.5524,  ..., 0.8010, 0.6132, 0.8518],\n",
      "         [0.4960, 0.5981, 0.7926,  ..., 0.4212, 0.6840, 0.8247]]],\n",
      "       device='cuda:0', grad_fn=<SoftplusBackward0>)\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.12 GiB. GPU 0 has a total capacty of 14.57 GiB of which 566.00 MiB is free. Process 180055 has 9.21 GiB memory in use. Including non-PyTorch memory, this process has 4.80 GiB memory in use. Of the allocated memory 4.63 GiB is allocated by PyTorch, and 41.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 444\u001b[0m\n\u001b[1;32m    441\u001b[0m RESULT_DIR \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCTNet_Mamba_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(time\u001b[38;5;241m.\u001b[39mtime())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    442\u001b[0m N_SUBJECT  \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m9\u001b[39m\n\u001b[0;32m--> 444\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRESULT_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDATA_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_SUBJECT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mCONFIG\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 420\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(result_dir, DATA_DIR, N_SUBJECT, **cfg)\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSubject \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msub\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    419\u001b[0m     exp \u001b[38;5;241m=\u001b[39m ExP(sub, DATA_DIR, result_dir, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcfg)\n\u001b[0;32m--> 420\u001b[0m     accs\u001b[38;5;241m.\u001b[39mappend(\u001b[43mexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage accuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mmean(accs))\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m accs\n",
      "Cell \u001b[0;32mIn[1], line 321\u001b[0m, in \u001b[0;36mExP.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    318\u001b[0m xb \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([xb, aug_x])\n\u001b[1;32m    319\u001b[0m yb \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([yb, aug_y])\n\u001b[0;32m--> 321\u001b[0m _, out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(out, yb)\n\u001b[1;32m    324\u001b[0m \u001b[38;5;66;03m# Backprop\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/vim_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vim_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 209\u001b[0m, in \u001b[0;36mEEGMambaTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    206\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39minterpolate(x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m), size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m32\u001b[39m), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m'\u001b[39m, align_corners\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)  \u001b[38;5;66;03m# [B, 1, 32, 32]\u001b[39;00m\n\u001b[1;32m    207\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [B, 3, 32, 32]\u001b[39;00m\n\u001b[0;32m--> 209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vim_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vim_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/vim_env/lib/python3.10/site-packages/vision_mamba/model.py:264\u001b[0m, in \u001b[0;36mVim.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# Forward pass with the layers\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 264\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# Latent\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/vim_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vim_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/vim_env/lib/python3.10/site-packages/vision_mamba/model.py:100\u001b[0m, in \u001b[0;36mVisionEncoderMambaBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     97\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj(x)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# forward con1d\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_direction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_conv1d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# backward conv1d\u001b[39;00m\n\u001b[1;32m    107\u001b[0m x2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_direction(\n\u001b[1;32m    108\u001b[0m     x,\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackward_conv1d,\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssm,\n\u001b[1;32m    111\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/vim_env/lib/python3.10/site-packages/vision_mamba/model.py:133\u001b[0m, in \u001b[0;36mVisionEncoderMambaBlock.process_direction\u001b[0;34m(self, x, conv1d, ssm)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConv1d: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    132\u001b[0m x \u001b[38;5;241m=\u001b[39m rearrange(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb d s -> b s d\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 133\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mssm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/vim_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vim_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/vim_env/lib/python3.10/site-packages/zeta/nn/modules/ssm.py:147\u001b[0m, in \u001b[0;36mSSM.forward\u001b[0;34m(self, x, pscan)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# Assuming selective_scan and selective_scan_seq are defined functions\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pscan:\n\u001b[0;32m--> 147\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mselective_scan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    149\u001b[0m     y \u001b[38;5;241m=\u001b[39m selective_scan_seq(x, delta, A, B, C, D)\n",
      "File \u001b[0;32m~/miniconda3/envs/vim_env/lib/python3.10/site-packages/zeta/nn/modules/ssm.py:31\u001b[0m, in \u001b[0;36mselective_scan\u001b[0;34m(x, delta, A, B, C, D)\u001b[0m\n\u001b[1;32m     27\u001b[0m deltaB \u001b[38;5;241m=\u001b[39m delta\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m B\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# (B, L, ED, N)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m BX \u001b[38;5;241m=\u001b[39m deltaB \u001b[38;5;241m*\u001b[39m x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (B, L, ED, N)\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m hs \u001b[38;5;241m=\u001b[39m \u001b[43mpscan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeltaA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m y \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     34\u001b[0m     hs \u001b[38;5;241m@\u001b[39m C\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     35\u001b[0m )\u001b[38;5;241m.\u001b[39msqueeze()  \u001b[38;5;66;03m# (B, L, ED, N) @ (B, L, N, 1) -> (B, L, ED, 1)\u001b[39;00m\n\u001b[1;32m     37\u001b[0m y \u001b[38;5;241m=\u001b[39m y \u001b[38;5;241m+\u001b[39m D \u001b[38;5;241m*\u001b[39m x\n",
      "File \u001b[0;32m~/miniconda3/envs/vim_env/lib/python3.10/site-packages/torch/autograd/function.py:539\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    538\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_context \u001b[38;5;241m==\u001b[39m _SingleLevelFunction\u001b[38;5;241m.\u001b[39msetup_context:\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    543\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    544\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    545\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    546\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    547\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/vim_env/lib/python3.10/site-packages/zeta/nn/modules/p_scan.py:89\u001b[0m, in \u001b[0;36mPScan.forward\u001b[0;34m(ctx, A_in, X_in)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# clone tensor (in-place ops)\u001b[39;00m\n\u001b[1;32m     88\u001b[0m A \u001b[38;5;241m=\u001b[39m A_in\u001b[38;5;241m.\u001b[39mclone()  \u001b[38;5;66;03m# (B, L, D, N)\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mX_in\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (B, L, D, N)\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# prepare tensors\u001b[39;00m\n\u001b[1;32m     92\u001b[0m A \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (B, D, L, N)\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.12 GiB. GPU 0 has a total capacty of 14.57 GiB of which 566.00 MiB is free. Process 180055 has 9.21 GiB memory in use. Including non-PyTorch memory, this process has 4.80 GiB memory in use. Of the allocated memory 4.63 GiB is allocated by PyTorch, and 41.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "CTNet-Mamba: A Convolution-Mamba Network for EEG-Based Motor Imagery Classification\n",
    "\n",
    "author: zhaowei701@163.com\n",
    "\"\"\"\n",
    "import math\n",
    "\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from barebones_hymba.barebones_hymba_block import HymbaBlock\n",
    "import flash_attn\n",
    "\n",
    "import numpy as np\n",
    "from zeta.nn import MultiQueryAttention\n",
    "from vision_mamba import Vim\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.backends import cudnn\n",
    "from torchsummary import summary\n",
    "\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "from utils import load_data_evaluate, numberClassChannel, calMetrics\n",
    "\n",
    "# Try to import real MambaBlock (fallback if missing)\n",
    "try:\n",
    "    from mamba_ssm import Mamba\n",
    "except ImportError:\n",
    "    class Mamba(nn.Module):\n",
    "        def __init__(self, d_model, d_state=16, d_conv=4, expand=2):\n",
    "            super().__init__()\n",
    "            self.linear = nn.Linear(d_model, d_model)\n",
    "        def forward(self, x):\n",
    "            return self.linear(x)\n",
    "\n",
    "from zeta.nn import MambaBlock, FeedForward, MultiQueryAttention\n",
    "from torchinfo import summary\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# PatchEmbeddingCNN\n",
    "# -----------------------------------------------------------------------------\n",
    "class PatchEmbeddingCNN(nn.Module):\n",
    "    def __init__(self, f1=16, kernel_size=64, D=2,\n",
    "                 pooling_size1=8, pooling_size2=8,\n",
    "                 dropout_rate=0.3, number_channel=22, emb_size=40):\n",
    "        super().__init__()\n",
    "        f2 = D * f1\n",
    "        self.cnn_module = nn.Sequential(\n",
    "            nn.Conv2d(1, f1, (1, kernel_size), padding='same', bias=False),\n",
    "            nn.BatchNorm2d(f1),\n",
    "            nn.Conv2d(f1, f2, (number_channel, 1), groups=f1, bias=False),\n",
    "            nn.BatchNorm2d(f2),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d((1, pooling_size1)),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Conv2d(f2, f2, (1, 16), padding='same', bias=False),\n",
    "            nn.BatchNorm2d(f2),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d((1, pooling_size2)),\n",
    "            nn.Dropout(dropout_rate),\n",
    "        )\n",
    "        self.projection = Rearrange('b e h w -> b (h w) e')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn_module(x)        # [B, f2, 1, seq]\n",
    "        return self.projection(x)     # [B, seq, f2]\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# LinearAttention\n",
    "# -----------------------------------------------------------------------------\n",
    "def exists(val): return val is not None\n",
    "\n",
    "class LinearAttention(nn.Module):\n",
    "    def __init__(self, dim, *, heads=4, dim_head=64, dropout=0.0):\n",
    "        super().__init__()\n",
    "        inner = heads * dim_head\n",
    "        self.heads, self.scale = heads, dim_head**-0.5\n",
    "        self.to_qkv = nn.Linear(dim, inner*3, bias=False)\n",
    "        self.to_out = nn.Sequential(nn.Linear(inner, dim), nn.Dropout(dropout))\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        h = self.heads\n",
    "        q,k,v = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        q,k,v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h=h), (q,k,v))\n",
    "        q, k = q*self.scale, k\n",
    "        q, k = q.softmax(dim=-1), k.softmax(dim=-2)\n",
    "        if exists(mask):\n",
    "            mask = rearrange(mask, 'b n -> (b h) n', h=h)\n",
    "            k = k.masked_fill(~mask.unsqueeze(-1), 0.)\n",
    "        ctx = torch.einsum('b n d, b n e -> b d e', q, k)\n",
    "        out = torch.einsum('b d e, b n d -> b n e', ctx, v)\n",
    "        out = rearrange(out, '(b h) n d -> b n (h d)', h=h)\n",
    "        return self.to_out(out)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# TransformerBlock\n",
    "# -----------------------------------------------------------------------------\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MambaTransformerblock(nn.Module):\n",
    "    def __init__(self, dim, heads, dim_head, dropout=0.1, \n",
    "                 ff_mult=4, d_state=16, depth=4):        \n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            self._create_layer(dim, heads, ff_mult, d_state, dropout)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "    def _create_layer(self, dim, heads, ff_mult, d_state, dropout):\n",
    "        return nn.ModuleDict({\n",
    "            # 1) Mamba\n",
    "            'mamba_norm':   nn.LayerNorm(dim),\n",
    "            'mamba':        MambaBlock(dim, d_state=d_state),\n",
    "            'mamba_dropout':nn.Dropout(dropout),\n",
    "\n",
    "            # 2) PyTorch MultiheadAttention\n",
    "            'attn_norm':    nn.LayerNorm(dim),\n",
    "            'attn':         nn.MultiheadAttention(embed_dim=dim,\n",
    "                                                  num_heads=heads,\n",
    "                                                  dropout=dropout,\n",
    "                                                  batch_first=True),\n",
    "            'attn_dropout': nn.Dropout(dropout),\n",
    "\n",
    "            # 3) FFN\n",
    "            'ffn_norm':     nn.LayerNorm(dim),\n",
    "            'feedforward':  nn.Sequential(\n",
    "                                 nn.Linear(dim, ff_mult * dim),\n",
    "                                 nn.GELU(),\n",
    "                                 nn.Dropout(dropout),\n",
    "                                 nn.Linear(ff_mult * dim, dim)\n",
    "                             ),\n",
    "            'ffn_dropout':  nn.Dropout(dropout),\n",
    "        })\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, L, dim]\n",
    "        for layer in self.layers:\n",
    "            # (1) Mamba sublayer\n",
    "            m = layer['mamba_norm'](x)\n",
    "            x = x + layer['mamba_dropout'](layer['mamba'](m))\n",
    "\n",
    "            # (2) Multi-head self-attention sublayer\n",
    "            a = layer['attn_norm'](x)\n",
    "            # because of the monkey-patch, this returns a Tensor directly\n",
    "            attn_out = layer['attn'](a, a, a, need_weights=False)\n",
    "            x = x + layer['attn_dropout'](attn_out)\n",
    "\n",
    "            # (3) Feed-forward sublayer\n",
    "            f = layer['ffn_norm'](x)\n",
    "            x = x + layer['ffn_dropout'](layer['feedforward'](f))\n",
    "\n",
    "        return self.norm(x)\n",
    "\n",
    "\n",
    "\n",
    "class SummaryWrapper(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # [B, C, T] ➜ [B, 1, C, T]\n",
    "        _, out = self.model(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# EEGMambaTransformer\n",
    "# -----------------------------------------------------------------------------\n",
    "class EEGMambaTransformer(nn.Module):\n",
    "    def __init__(self, num_classes=4, image_size=(32, 32)):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.number_class, self.number_channel = numberClassChannel('A')\n",
    "\n",
    "        self.model = Vim(\n",
    "            dim=256,\n",
    "            dt_rank=32,\n",
    "            dim_inner=256,\n",
    "            d_state=256,\n",
    "            num_classes=num_classes,\n",
    "            image_size=image_size,  # 2D image size\n",
    "            patch_size=8,\n",
    "            channels=3,\n",
    "            dropout=0.1,\n",
    "            depth=6,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, 1, channels, time] -> need to reshape to [B, 3, H, W]\n",
    "        B = x.size(0)\n",
    "        x = x.squeeze(1)  # [B, channels, time]\n",
    "        \n",
    "        # Reshape to (B, 3, H, W)\n",
    "        # Here, you can map EEG channels and time into a fake 2D layout\n",
    "        # For simplicity, let's reshape (22,1000) into (32,32), fill zeros if needed\n",
    "        x = torch.nn.functional.interpolate(x.unsqueeze(1), size=(32, 32), mode='bilinear', align_corners=False)  # [B, 1, 32, 32]\n",
    "        x = x.repeat(1, 3, 1, 1)  # [B, 3, 32, 32]\n",
    "\n",
    "        return None, self.model(x)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Experiment wrapper\n",
    "# -----------------------------------------------------------------------------\n",
    "class ExP:\n",
    "    def __init__(self, nsub, data_dir, result_name,\n",
    "                 epochs=300, number_aug=3, number_seg=8,\n",
    "                 evaluate_mode='subject-dependent',\n",
    "                 heads=2, emb_size=16, depth=6,\n",
    "                 d_state=16, transformer_depth=1, mamba_depth=3,\n",
    "                 dataset_type='A', eeg1_f1=8, eeg1_kernel_size=64,\n",
    "                 eeg1_D=2, eeg1_pooling_size1=8, eeg1_pooling_size2=8,\n",
    "                 eeg1_dropout_rate=0.5, flatten_eeg1=15*16,\n",
    "                 validate_ratio=0.3, learning_rate=1e-3, batch_size=72,\n",
    "                 early_stopping_patience=100 ):\n",
    "        self.nSub = nsub\n",
    "        self.dataset_type = dataset_type\n",
    "        self.data_dir = data_dir\n",
    "        self.result_name = result_name\n",
    "        self.n_epochs = epochs\n",
    "        self.patience = early_stopping_patience\n",
    "        self.no_improve = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.batch_size = batch_size\n",
    "        self.validate_ratio = validate_ratio\n",
    "        self.number_class, self.number_channel = numberClassChannel('A')\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "        self.model = EEGMambaTransformer(\n",
    "            num_classes=self.number_class,\n",
    "            image_size=(32, 32)\n",
    "        ).cuda()\n",
    "\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        os.makedirs(self.result_name, exist_ok=True)\n",
    "        self.model_filename = os.path.join(self.result_name, f\"model_{self.nSub}.pth\")\n",
    "\n",
    "    def interaug(self, timg, label):\n",
    "        aug_data, aug_label = [], []\n",
    "        recs = 3 * (self.batch_size // self.model.number_class)\n",
    "        segpts = 1000 // 8\n",
    "        for cls in range(self.model.number_class):\n",
    "            idx = np.where(label == cls + 1)\n",
    "            data, lbl = timg[idx], label[idx]\n",
    "            tmp = np.zeros((recs,1,self.model.number_channel,1000), dtype=np.float32)\n",
    "            for i in range(recs):\n",
    "                for j in range(8):\n",
    "                    ridx = random.randrange(data.shape[0])\n",
    "                    tmp[i,0,:,j*segpts:(j+1)*segpts] = data[ridx,0,:,j*segpts:(j+1)*segpts]\n",
    "            aug_data.append(tmp); aug_label.append(lbl[:recs])\n",
    "        aug_data = np.concatenate(aug_data).astype(np.float32)\n",
    "        aug_label = np.concatenate(aug_label)\n",
    "        perm = np.random.permutation(len(aug_data))\n",
    "        return (\n",
    "            torch.from_numpy(aug_data[perm]).cuda(),\n",
    "            torch.from_numpy((aug_label[perm]-1)).long().cuda()\n",
    "        )\n",
    "\n",
    "    def get_source_data(self):\n",
    "        tr_x, tr_y, te_x, te_y = load_data_evaluate(\n",
    "            self.data_dir, self.dataset_type, self.nSub,\n",
    "            mode_evaluate='subject-dependent'\n",
    "        )\n",
    "        tr_x = np.expand_dims(tr_x, axis=1).astype(np.float32)\n",
    "        te_x = np.expand_dims(te_x, axis=1).astype(np.float32)\n",
    "        tr_y = tr_y.reshape(-1)\n",
    "        te_y = te_y.reshape(-1)\n",
    "        m, s = tr_x.mean(), tr_x.std()\n",
    "        tr_x = (tr_x - m) / s\n",
    "        te_x = (te_x - m) / s\n",
    "        return tr_x, tr_y, te_x, te_y\n",
    "\n",
    "    def train(self):\n",
    "        tr_x, tr_y, te_x, te_y = self.get_source_data()\n",
    "    \n",
    "        # Create validation split\n",
    "        dataset_size = len(tr_x)\n",
    "        val_size = int(self.validate_ratio * dataset_size)\n",
    "        train_size = dataset_size - val_size\n",
    "        train_ds, val_ds = torch.utils.data.random_split(\n",
    "            torch.utils.data.TensorDataset(torch.from_numpy(tr_x), torch.from_numpy(tr_y-1)),\n",
    "            [train_size, val_size]\n",
    "        )\n",
    "    \n",
    "        test_ds = torch.utils.data.TensorDataset(torch.from_numpy(te_x), torch.from_numpy(te_y-1))\n",
    "        self.test_loader = torch.utils.data.DataLoader(test_ds, batch_size=self.batch_size)\n",
    "    \n",
    "        best_loss = float('inf')\n",
    "        for e in range(self.n_epochs):\n",
    "            # Training\n",
    "            self.model.train()\n",
    "            train_loader = torch.utils.data.DataLoader(train_ds, batch_size=self.batch_size, shuffle=True)\n",
    "            \n",
    "            # Initialize training metrics\n",
    "            epoch_train_loss = 0.0\n",
    "            train_correct = 0\n",
    "            total_samples = 0\n",
    "            \n",
    "            # Start timing and memory tracking\n",
    "            torch.cuda.synchronize()\n",
    "            start_time = time.time()\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            \n",
    "            for xb, yb in train_loader:\n",
    "                xb, yb = xb.cuda().float(), yb.cuda().long()\n",
    "                aug_x, aug_y = self.interaug(tr_x, tr_y)\n",
    "                xb = torch.cat([xb, aug_x])\n",
    "                yb = torch.cat([yb, aug_y])\n",
    "                \n",
    "                _, out = self.model(xb)\n",
    "                loss = self.criterion(out, yb)\n",
    "                \n",
    "                # Backprop\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                # Track metrics\n",
    "                epoch_train_loss += loss.item()\n",
    "                preds = out.argmax(dim=1)\n",
    "                train_correct += (preds == yb).sum().item()\n",
    "                total_samples += yb.size(0)\n",
    "    \n",
    "            # Calculate training metrics\n",
    "            train_loss = epoch_train_loss / len(train_loader)\n",
    "            train_acc = train_correct / total_samples\n",
    "            \n",
    "            # Calculate memory and speed\n",
    "            torch.cuda.synchronize()\n",
    "            elapsed = time.time() - start_time\n",
    "            mem_used = torch.cuda.max_memory_allocated() / (1024 ** 2)  # MB\n",
    "            speed = total_samples / elapsed if elapsed > 0 else 0\n",
    "    \n",
    "            # Validation\n",
    "            self.model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_correct = 0\n",
    "            with torch.no_grad():\n",
    "                val_loader = torch.utils.data.DataLoader(val_ds, batch_size=self.batch_size)\n",
    "                for xb, yb in val_loader:\n",
    "                    xb, yb = xb.cuda().float(), yb.cuda().long()\n",
    "                    _, out = self.model(xb)\n",
    "                    loss = self.criterion(out, yb)\n",
    "                    val_loss += loss.item()\n",
    "                    val_correct += (out.argmax(1) == yb).sum().item()\n",
    "    \n",
    "            val_loss = val_loss / len(val_loader)\n",
    "            val_acc = val_correct / len(val_ds)\n",
    "    \n",
    "            print(f\"Epoch {e+1}/{self.n_epochs} | \"\n",
    "                  f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | \"\n",
    "                  f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.4f} | \"\n",
    "                  f\"Mem: {mem_used:.2f}MB | Speed: {speed:.2f} samples/s\")\n",
    "    \n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                torch.save(self.model.state_dict(), self.model_filename)\n",
    "            else:\n",
    "                self.no_improve += 1\n",
    "                if self.no_improve >= self.patience:\n",
    "                    print(f\"Stopping early at epoch {e+1} (no improvement in {self.patience} epochs).\")\n",
    "                    break\n",
    "\n",
    "    # Rest of test evaluation...\n",
    "\n",
    "    # Rest of test evaluation remains the same...\n",
    "\n",
    "        self.model.load_state_dict(torch.load(self.model_filename))\n",
    "        self.model.eval()\n",
    "        preds, trues = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in self.test_loader:\n",
    "                xb = xb.cuda()\n",
    "                _, out = self.model(xb)\n",
    "                preds.append(out.argmax(1).cpu().numpy())\n",
    "                trues.append(yb.numpy())\n",
    "        preds = np.concatenate(preds); trues = np.concatenate(trues)\n",
    "        acc, *_ = calMetrics(trues, preds)\n",
    "        print(f\"Subject {self.nSub} final accuracy: {acc:.4f}\")\n",
    "        return acc\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "def main(result_dir, DATA_DIR, N_SUBJECT, **cfg):\n",
    "    os.makedirs(result_dir, exist_ok=True)\n",
    "\n",
    "    number_class, number_channel = numberClassChannel(cfg['dataset_type'])\n",
    "    sModel = EEGMambaTransformer(\n",
    "        num_classes=number_class,\n",
    "        image_size=(32, 32)\n",
    "    ).cuda()\n",
    "\n",
    "\n",
    "    summary(sModel, (1, number_channel, 1000))\n",
    "\n",
    "\n",
    "    print(time.asctime(time.localtime(time.time())))\n",
    "\n",
    "    accs = []\n",
    "    for sub in range(1, N_SUBJECT+1):\n",
    "        seed_n = np.random.randint(2024)\n",
    "        random.seed(seed_n); np.random.seed(seed_n)\n",
    "        torch.manual_seed(seed_n); torch.cuda.manual_seed(seed_n)\n",
    "        torch.cuda.manual_seed_all(seed_n)\n",
    "\n",
    "        print(f\"seed is {seed_n}\")\n",
    "        print(f\"Subject {sub}\")\n",
    "\n",
    "        exp = ExP(sub, DATA_DIR, result_dir, **cfg)\n",
    "        accs.append(exp.train())\n",
    "\n",
    "    print(\"Average accuracy:\", np.mean(accs))\n",
    "    return accs\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cudnn.benchmark = False\n",
    "    cudnn.deterministic = True\n",
    "\n",
    "    CONFIG = dict(\n",
    "        emb_size=128, depth=1, heads=4,\n",
    "        d_state=32, transformer_depth=1, mamba_depth=2,\n",
    "        dataset_type='A',\n",
    "        eeg1_f1=8, eeg1_kernel_size=64,\n",
    "        eeg1_D=16, eeg1_pooling_size1=8, eeg1_pooling_size2=8,\n",
    "        eeg1_dropout_rate=0.5, flatten_eeg1=15*128,\n",
    "        epochs=1000, number_aug=3, number_seg=8,\n",
    "        validate_ratio=0.3, learning_rate=1e-3, batch_size=72\n",
    "    )\n",
    "\n",
    "    DATA_DIR   = \"bci2a/\"\n",
    "    RESULT_DIR = f\"CTNet_Mamba_{int(time.time())}\"\n",
    "    N_SUBJECT  = 9\n",
    "\n",
    "    main(RESULT_DIR, DATA_DIR, N_SUBJECT, **CONFIG)\n",
    "    print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4294462-2e9e-4037-af14-aa22381c2691",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40fc1e1-fc45-4c39-9b17-f0c1fa330348",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smartview/miniconda3/envs/vim_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "CTNet-Mamba: A Convolution-Mamba Network for EEG-Based Motor Imagery Classification\n",
    "\n",
    "author: zhaowei701@163.com\n",
    "\"\"\"\n",
    "import math\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from barebones_hymba.barebones_hymba_block import HymbaBlock\n",
    "import flash_attn\n",
    "\n",
    "import numpy as np\n",
    "from zeta.nn import MultiQueryAttention\n",
    "from vision_mamba import Vim\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.backends import cudnn\n",
    "from torchsummary import summary\n",
    "\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils import load_data_evaluate, numberClassChannel, calMetrics\n",
    "\n",
    "# Try to import real MambaBlock (fallback if missing)\n",
    "try:\n",
    "    from mamba_ssm import Mamba\n",
    "except ImportError:\n",
    "    class Mamba(nn.Module):\n",
    "        def __init__(self, d_model, d_state=16, d_conv=4, expand=2):\n",
    "            super().__init__()\n",
    "            self.linear = nn.Linear(d_model, d_model)\n",
    "        def forward(self, x):\n",
    "            return self.linear(x)\n",
    "\n",
    "from zeta.nn import MambaBlock, FeedForward, MultiQueryAttention\n",
    "from torchinfo import summary\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "# Log filename with timestamp\n",
    "log_filename = f\"vimambaprinted.log\"\n",
    "\n",
    "# Redirect stdout (print, summary, etc.)\n",
    "sys.stdout = open(log_filename, \"w\")\n",
    "sys.stderr = sys.stdout  # Optional: log errors too\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# PatchEmbeddingCNN\n",
    "# -----------------------------------------------------------------------------\n",
    "class ConvFeatureExtractor(nn.Module):\n",
    "    def __init__(self, in_channels=22, f1=16, D=2, kernel_size=64, pool1=8, pool2=8, dropout=0.3):\n",
    "        super().__init__()\n",
    "        f2 = D * f1\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, f1, (1, kernel_size), padding='same', bias=False),\n",
    "            nn.BatchNorm2d(f1),\n",
    "            nn.Conv2d(f1, f2, (in_channels, 1), groups=f1, bias=False),\n",
    "            nn.BatchNorm2d(f2),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d((1, pool1)),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv2d(f2, f2, (1, 16), padding='same', bias=False),\n",
    "            nn.BatchNorm2d(f2),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d((1, pool2)),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.features(x)  # [B, f2, 1, time]\n",
    "\n",
    "def sliding_projected_windows(x, cnn_model, num_windows=5):\n",
    "    B, C, T = x.shape\n",
    "    x = x.unsqueeze(1)  # [B, 1, C, T]\n",
    "    feats = cnn_model(x)  # [B, f2, 1, T']\n",
    "    feats = feats.squeeze(2)  # [B, f2, T']\n",
    "    T_proj = feats.size(-1)\n",
    "    step = (T_proj - 32) // (num_windows - 1)\n",
    "    windows = []\n",
    "    for i in range(num_windows):\n",
    "        start = i * step\n",
    "        end = start + 32\n",
    "        w = feats[:, :, start:end]  # [B, f2, 32]\n",
    "        w = w.unsqueeze(1)  # [B, 1, f2, 32]\n",
    "        w = F.interpolate(w, size=(32, 32), mode='bilinear', align_corners=False)  # [B, 1, 32, 32]\n",
    "        windows.append(w.repeat(1, 3, 1, 1))  # [B, 3, 32, 32]\n",
    "    return windows\n",
    "\n",
    "class EEGMambaTransformer(nn.Module):\n",
    "    def __init__(self, num_classes=4, image_size=(32, 32), num_windows=5):\n",
    "        super().__init__()\n",
    "        self.num_windows = num_windows\n",
    "        self.cnn = ConvFeatureExtractor()\n",
    "        self.number_class, self.number_channel = numberClassChannel('A')\n",
    "\n",
    "\n",
    "        self.vim = Vim(\n",
    "            dim=128, dt_rank=16, dim_inner=128, d_state=64,\n",
    "            num_classes=num_classes, image_size=image_size,\n",
    "            patch_size=8, channels=3, dropout=0.1, depth=3\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.squeeze(1)  # [B, C, T]\n",
    "        windows = sliding_projected_windows(x, self.cnn, self.num_windows)\n",
    "        logits = [self.vim(patch) for patch in windows]\n",
    "        out = torch.stack(logits).mean(dim=0)\n",
    "        return None, out\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Experiment wrapper\n",
    "# -----------------------------------------------------------------------------\n",
    "class ExP:\n",
    "    def __init__(self, nsub, data_dir, result_name,\n",
    "                 epochs=300, number_aug=3, number_seg=8,\n",
    "                 evaluate_mode='subject-dependent',\n",
    "                 heads=2, emb_size=16, depth=6,\n",
    "                 d_state=16, transformer_depth=1, mamba_depth=3,\n",
    "                 dataset_type='A', eeg1_f1=8, eeg1_kernel_size=64,\n",
    "                 eeg1_D=2, eeg1_pooling_size1=8, eeg1_pooling_size2=8,\n",
    "                 eeg1_dropout_rate=0.5, flatten_eeg1=15*16,\n",
    "                 validate_ratio=0.3, learning_rate=1e-3, batch_size=72,\n",
    "                 early_stopping_patience=100 ):\n",
    "        self.nSub = nsub\n",
    "        self.dataset_type = dataset_type\n",
    "        self.data_dir = data_dir\n",
    "        self.result_name = result_name\n",
    "        self.n_epochs = epochs\n",
    "        self.patience = early_stopping_patience\n",
    "        self.no_improve = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.batch_size = batch_size\n",
    "        self.validate_ratio = validate_ratio\n",
    "        self.number_class, self.number_channel = numberClassChannel('A')\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "        self.model = EEGMambaTransformer(\n",
    "            num_classes=self.number_class,\n",
    "            image_size=(32, 32)\n",
    "        ).cuda()\n",
    "\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        os.makedirs(self.result_name, exist_ok=True)\n",
    "        self.model_filename = os.path.join(self.result_name, f\"model_{self.nSub}.pth\")\n",
    "\n",
    "    def interaug(self, timg, label):\n",
    "        aug_data, aug_label = [], []\n",
    "        recs = 3 * (self.batch_size // self.model.number_class)\n",
    "        segpts = 1000 // 8\n",
    "        for cls in range(self.model.number_class):\n",
    "            idx = np.where(label == cls + 1)\n",
    "            data, lbl = timg[idx], label[idx]\n",
    "            tmp = np.zeros((recs,1,self.model.number_channel,1000), dtype=np.float32)\n",
    "            for i in range(recs):\n",
    "                for j in range(8):\n",
    "                    ridx = random.randrange(data.shape[0])\n",
    "                    tmp[i,0,:,j*segpts:(j+1)*segpts] = data[ridx,0,:,j*segpts:(j+1)*segpts]\n",
    "            aug_data.append(tmp); aug_label.append(lbl[:recs])\n",
    "        aug_data = np.concatenate(aug_data).astype(np.float32)\n",
    "        aug_label = np.concatenate(aug_label)\n",
    "        perm = np.random.permutation(len(aug_data))\n",
    "        return (\n",
    "            torch.from_numpy(aug_data[perm]).cuda(),\n",
    "            torch.from_numpy((aug_label[perm]-1)).long().cuda()\n",
    "        )\n",
    "\n",
    "    def get_source_data(self):\n",
    "        tr_x, tr_y, te_x, te_y = load_data_evaluate(\n",
    "            self.data_dir, self.dataset_type, self.nSub,\n",
    "            mode_evaluate='subject-dependent'\n",
    "        )\n",
    "        tr_x = np.expand_dims(tr_x, axis=1).astype(np.float32)\n",
    "        te_x = np.expand_dims(te_x, axis=1).astype(np.float32)\n",
    "        tr_y = tr_y.reshape(-1)\n",
    "        te_y = te_y.reshape(-1)\n",
    "        m, s = tr_x.mean(), tr_x.std()\n",
    "        tr_x = (tr_x - m) / s\n",
    "        te_x = (te_x - m) / s\n",
    "        return tr_x, tr_y, te_x, te_y\n",
    "\n",
    "    def train(self):\n",
    "        tr_x, tr_y, te_x, te_y = self.get_source_data()\n",
    "    \n",
    "        # Create validation split\n",
    "        dataset_size = len(tr_x)\n",
    "        val_size = int(self.validate_ratio * dataset_size)\n",
    "        train_size = dataset_size - val_size\n",
    "        train_ds, val_ds = torch.utils.data.random_split(\n",
    "            torch.utils.data.TensorDataset(torch.from_numpy(tr_x), torch.from_numpy(tr_y-1)),\n",
    "            [train_size, val_size]\n",
    "        )\n",
    "    \n",
    "        test_ds = torch.utils.data.TensorDataset(torch.from_numpy(te_x), torch.from_numpy(te_y-1))\n",
    "        self.test_loader = torch.utils.data.DataLoader(test_ds, batch_size=self.batch_size)\n",
    "    \n",
    "        best_loss = float('inf')\n",
    "        for e in range(self.n_epochs):\n",
    "            # Training\n",
    "            self.model.train()\n",
    "            train_loader = torch.utils.data.DataLoader(train_ds, batch_size=self.batch_size, shuffle=True)\n",
    "            \n",
    "            # Initialize training metrics\n",
    "            epoch_train_loss = 0.0\n",
    "            train_correct = 0\n",
    "            total_samples = 0\n",
    "            \n",
    "            # Start timing and memory tracking\n",
    "            torch.cuda.synchronize()\n",
    "            start_time = time.time()\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            \n",
    "            for xb, yb in train_loader:\n",
    "                xb, yb = xb.cuda().float(), yb.cuda().long()\n",
    "                aug_x, aug_y = self.interaug(tr_x, tr_y)\n",
    "                xb = torch.cat([xb, aug_x])\n",
    "                yb = torch.cat([yb, aug_y])\n",
    "                \n",
    "                _, out = self.model(xb)\n",
    "                loss = self.criterion(out, yb)\n",
    "                \n",
    "                # Backprop\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                # Track metrics\n",
    "                epoch_train_loss += loss.item()\n",
    "                preds = out.argmax(dim=1)\n",
    "                train_correct += (preds == yb).sum().item()\n",
    "                total_samples += yb.size(0)\n",
    "    \n",
    "            # Calculate training metrics\n",
    "            train_loss = epoch_train_loss / len(train_loader)\n",
    "            train_acc = train_correct / total_samples\n",
    "            \n",
    "            # Calculate memory and speed\n",
    "            torch.cuda.synchronize()\n",
    "            elapsed = time.time() - start_time\n",
    "            mem_used = torch.cuda.max_memory_allocated() / (1024 ** 2)  # MB\n",
    "            speed = total_samples / elapsed if elapsed > 0 else 0\n",
    "    \n",
    "            # Validation\n",
    "            self.model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_correct = 0\n",
    "            with torch.no_grad():\n",
    "                val_loader = torch.utils.data.DataLoader(val_ds, batch_size=self.batch_size)\n",
    "                for xb, yb in val_loader:\n",
    "                    xb, yb = xb.cuda().float(), yb.cuda().long()\n",
    "                    _, out = self.model(xb)\n",
    "                    loss = self.criterion(out, yb)\n",
    "                    val_loss += loss.item()\n",
    "                    val_correct += (out.argmax(1) == yb).sum().item()\n",
    "    \n",
    "            val_loss = val_loss / len(val_loader)\n",
    "            val_acc = val_correct / len(val_ds)\n",
    "    \n",
    "            print(f\"Epoch {e+1}/{self.n_epochs} | \"\n",
    "                  f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | \"\n",
    "                  f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.4f} | \"\n",
    "                  f\"Mem: {mem_used:.2f}MB | Speed: {speed:.2f} samples/s\")\n",
    "    \n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                torch.save(self.model.state_dict(), self.model_filename)\n",
    "            else:\n",
    "                self.no_improve += 1\n",
    "                if self.no_improve >= self.patience:\n",
    "                    print(f\"Stopping early at epoch {e+1} (no improvement in {self.patience} epochs).\")\n",
    "                    break\n",
    "\n",
    "    # Rest of test evaluation...\n",
    "\n",
    "    # Rest of test evaluation remains the same...\n",
    "\n",
    "        self.model.load_state_dict(torch.load(self.model_filename))\n",
    "        self.model.eval()\n",
    "        preds, trues = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in self.test_loader:\n",
    "                xb = xb.cuda()\n",
    "                _, out = self.model(xb)\n",
    "                preds.append(out.argmax(1).cpu().numpy())\n",
    "                trues.append(yb.numpy())\n",
    "        preds = np.concatenate(preds); trues = np.concatenate(trues)\n",
    "        acc, *_ = calMetrics(trues, preds)\n",
    "        print(f\"Subject {self.nSub} final accuracy: {acc:.4f}\")\n",
    "        return acc\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "def main(result_dir, DATA_DIR, N_SUBJECT, **cfg):\n",
    "    os.makedirs(result_dir, exist_ok=True)\n",
    "\n",
    "    dataset_type = 'A'\n",
    "    number_class, number_channel = numberClassChannel(dataset_type)\n",
    "    sModel = EEGMambaTransformer(\n",
    "        num_classes=number_class,\n",
    "        image_size=(32, 32)\n",
    "    ).cuda()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(time.asctime(time.localtime(time.time())))\n",
    "\n",
    "    accs = []\n",
    "    for sub in range(1, N_SUBJECT+1):\n",
    "        seed_n = np.random.randint(2024)\n",
    "        random.seed(seed_n); np.random.seed(seed_n)\n",
    "        torch.manual_seed(seed_n); torch.cuda.manual_seed(seed_n)\n",
    "        torch.cuda.manual_seed_all(seed_n)\n",
    "\n",
    "        print(f\"seed is {seed_n}\")\n",
    "        print(f\"Subject {sub}\")\n",
    "\n",
    "        exp = ExP(sub, DATA_DIR, result_dir, **cfg)\n",
    "        accs.append(exp.train())\n",
    "\n",
    "    print(\"Average accuracy:\", np.mean(accs))\n",
    "    return accs\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cudnn.benchmark = False\n",
    "    cudnn.deterministic = True\n",
    "\n",
    "    CONFIG = dict(\n",
    "        emb_size=128, depth=1, heads=4,\n",
    "        d_state=32, transformer_depth=1, mamba_depth=2,\n",
    "        dataset_type='A',\n",
    "        eeg1_f1=8, eeg1_kernel_size=64,\n",
    "        eeg1_D=16, eeg1_pooling_size1=8, eeg1_pooling_size2=8,\n",
    "        eeg1_dropout_rate=0.5, flatten_eeg1=15*128,\n",
    "        epochs=1000, number_aug=3, number_seg=8,\n",
    "        validate_ratio=0.3, learning_rate=1e-3, batch_size=36\n",
    "    )\n",
    "\n",
    "    DATA_DIR   = \"bci2a/\"\n",
    "    RESULT_DIR = f\"CTNet_Mamba_{int(time.time())}\"\n",
    "    N_SUBJECT  = 9\n",
    "\n",
    "    main(RESULT_DIR, DATA_DIR, N_SUBJECT, **CONFIG)\n",
    "    print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "910c51a4-2e27-448a-a596-85700028d7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Apr 30 08:02:42 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:8B:00.0 Off |                    0 |\n",
      "| N/A   51C    P8              9W /   70W |       4MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A       732      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49512b2c-62ab-4950-b91f-75296f6d7652",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
